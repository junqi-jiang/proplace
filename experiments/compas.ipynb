{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Apps\\anaconda\\envs\\proplace\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n",
      "[WARNING] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      " [lazy_loader.py _load]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "import pandas as pd\n",
    "from proplace.clfutils import HiddenPrints\n",
    "import proplace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# load dataset utils\n",
    "dataset = proplace.InnDataSet(\"compas\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Epoch 0: train loss: 0.704266369342804\n",
      "Epoch 0: train loss: 0.6788407564163208\n",
      "Epoch 0: train loss: 0.6602728366851807\n",
      "Epoch 0: train loss: 0.6463878154754639\n",
      "Epoch 0: train loss: 0.6351909041404724\n",
      "Epoch 0: train loss: 0.5963695645332336\n",
      "Epoch 0: train loss: 0.5887550711631775\n",
      "Epoch 0: train loss: 0.6307805776596069\n",
      "Epoch 0: train loss: 0.53557288646698\n",
      "Epoch 0: train loss: 0.6204761266708374\n",
      "Epoch 0: train loss: 0.5046147108078003\n",
      "Epoch 0: train loss: 0.5312041640281677\n",
      "Epoch 0: train loss: 0.5049447417259216\n",
      "Epoch 0: train loss: 0.49821600317955017\n",
      "Epoch 0: train loss: 0.4659635126590729\n",
      "Epoch 0: train loss: 0.2855537533760071\n",
      "Epoch 0: train loss: 0.559812068939209\n",
      "Epoch 0: train loss: 0.43528714776039124\n",
      "Epoch 0: train loss: 0.6615065336227417\n",
      "Epoch 0: train loss: 0.3767836391925812\n",
      "Epoch 0: train loss: 0.3594193458557129\n",
      "Epoch 0: train loss: 0.6426411867141724\n",
      "Epoch 0: train loss: 0.4638318419456482\n",
      "Epoch 0: train loss: 0.29755380749702454\n",
      "Epoch 0: train loss: 0.41523265838623047\n",
      "Epoch 0: train loss: 0.2998034656047821\n",
      "Epoch 0: train loss: 0.5169100761413574\n",
      "Epoch 0: train loss: 0.5397275686264038\n",
      "Epoch 0: train loss: 0.5525503158569336\n",
      "Epoch 0: train loss: 0.40845203399658203\n",
      "Epoch 0: train loss: 0.5387842655181885\n",
      "Epoch 0: train loss: 0.40179598331451416\n",
      "Epoch 0: train loss: 0.3039369285106659\n",
      "Epoch 0: train loss: 0.2926522493362427\n",
      "Epoch 0: train loss: 0.33685562014579773\n",
      "Epoch 0: train loss: 0.35986098647117615\n",
      "Epoch 0: train loss: 0.38460639119148254\n",
      "Epoch 0: train loss: 0.3214374780654907\n",
      "Epoch 0: train loss: 0.30725282430648804\n",
      "Epoch 0: train loss: 0.33161333203315735\n",
      "Epoch 0: train loss: 0.3958968222141266\n",
      "Epoch 0: train loss: 0.38008803129196167\n",
      "Epoch 0: train loss: 0.42542406916618347\n",
      "Epoch 0: train loss: 0.4949567914009094\n",
      "Epoch 0: train loss: 0.4604753851890564\n",
      "Epoch 0: train loss: 0.2855861783027649\n",
      "Epoch 0: train loss: 0.32667815685272217\n",
      "Epoch 0: train loss: 0.4210566580295563\n",
      "Epoch 0: train loss: 0.548435389995575\n",
      "Epoch 0: train loss: 0.3576991856098175\n",
      "Epoch 0: train loss: 0.3633660078048706\n",
      "Epoch 0: train loss: 0.8291239738464355\n",
      "Epoch 0: train loss: 0.26884135603904724\n",
      "Epoch 0: train loss: 0.5782736539840698\n",
      "Epoch 0: train loss: 0.4119100868701935\n",
      "Epoch 0: train loss: 0.39700302481651306\n",
      "Epoch 0: train loss: 0.44851386547088623\n",
      "Epoch 0: train loss: 0.5339949727058411\n",
      "Epoch 0: train loss: 0.35966581106185913\n",
      "Epoch 0: train loss: 0.3791535198688507\n",
      "Epoch 0: train loss: 0.4540339708328247\n",
      "Epoch 0: train loss: 0.4400176703929901\n",
      "Epoch 0: train loss: 0.4875311851501465\n",
      "Epoch 0: train loss: 0.41306206583976746\n",
      "Epoch 0: train loss: 0.6196861863136292\n",
      "Epoch 0: train loss: 0.45017674565315247\n",
      "Epoch 0: train loss: 0.3841932415962219\n",
      "Epoch 0: train loss: 0.39628463983535767\n",
      "Epoch 0: train loss: 0.4879545569419861\n",
      "Epoch 0: train loss: 0.3471423387527466\n",
      "Epoch 0: train loss: 0.3044157326221466\n",
      "Epoch 0: train loss: 0.3425053656101227\n",
      "Epoch 0: train loss: 0.4100834131240845\n",
      "Epoch 0: train loss: 0.3259411156177521\n",
      "Epoch 0: train loss: 0.2841852009296417\n",
      "Epoch 0: train loss: 0.46395382285118103\n",
      "Epoch 0: train loss: 0.5365607142448425\n",
      "Epoch 0: train loss: 0.6828761696815491\n",
      "Epoch 1: train loss: 0.34508216381073\n",
      "Epoch 1: train loss: 0.2257959395647049\n",
      "Epoch 1: train loss: 0.42351004481315613\n",
      "Epoch 1: train loss: 0.3624516725540161\n",
      "Epoch 1: train loss: 0.49949386715888977\n",
      "Epoch 1: train loss: 0.3816087245941162\n",
      "Epoch 1: train loss: 0.4369608759880066\n",
      "Epoch 1: train loss: 0.4166424572467804\n",
      "Epoch 1: train loss: 0.591520369052887\n",
      "Epoch 1: train loss: 0.8294327259063721\n",
      "Epoch 1: train loss: 0.4813846945762634\n",
      "Epoch 1: train loss: 0.343181848526001\n",
      "Epoch 1: train loss: 0.40531280636787415\n",
      "Epoch 1: train loss: 0.43616586923599243\n",
      "Epoch 1: train loss: 0.31057217717170715\n",
      "Epoch 1: train loss: 0.32043325901031494\n",
      "Epoch 1: train loss: 0.335014671087265\n",
      "Epoch 1: train loss: 0.35510897636413574\n",
      "Epoch 1: train loss: 0.3876403570175171\n",
      "Epoch 1: train loss: 0.30214256048202515\n",
      "Epoch 1: train loss: 0.3703458905220032\n",
      "Epoch 1: train loss: 0.4527053236961365\n",
      "Epoch 1: train loss: 0.33004462718963623\n",
      "Epoch 1: train loss: 0.4025125503540039\n",
      "Epoch 1: train loss: 0.3801965117454529\n",
      "Epoch 1: train loss: 0.3568248152732849\n",
      "Epoch 1: train loss: 0.26670947670936584\n",
      "Epoch 1: train loss: 0.2852139472961426\n",
      "Epoch 1: train loss: 0.7189237475395203\n",
      "Epoch 1: train loss: 0.5227009654045105\n",
      "Epoch 1: train loss: 0.2847563922405243\n",
      "Epoch 1: train loss: 0.061882857233285904\n",
      "Epoch 1: train loss: 0.36233025789260864\n",
      "Epoch 1: train loss: 0.5172744393348694\n",
      "Epoch 1: train loss: 0.3011576533317566\n",
      "Epoch 1: train loss: 0.5143961906433105\n",
      "Epoch 1: train loss: 0.4827617108821869\n",
      "Epoch 1: train loss: 0.27107733488082886\n",
      "Epoch 1: train loss: 0.5404848456382751\n",
      "Epoch 1: train loss: 0.3219618797302246\n",
      "Epoch 1: train loss: 0.4137173593044281\n",
      "Epoch 1: train loss: 0.37710967659950256\n",
      "Epoch 1: train loss: 0.30629831552505493\n",
      "Epoch 1: train loss: 0.41230958700180054\n",
      "Epoch 1: train loss: 0.29268744587898254\n",
      "Epoch 1: train loss: 0.4503699243068695\n",
      "Epoch 1: train loss: 0.4842986464500427\n",
      "Epoch 1: train loss: 0.31686121225357056\n",
      "Epoch 1: train loss: 0.3877890110015869\n",
      "Epoch 1: train loss: 0.49858424067497253\n",
      "Epoch 1: train loss: 0.19035321474075317\n",
      "Epoch 1: train loss: 0.38744428753852844\n",
      "Epoch 1: train loss: 0.47903257608413696\n",
      "Epoch 1: train loss: 0.43301576375961304\n",
      "Epoch 1: train loss: 0.2638399302959442\n",
      "Epoch 1: train loss: 0.2698225677013397\n",
      "Epoch 1: train loss: 0.2840166687965393\n",
      "Epoch 1: train loss: 0.49998340010643005\n",
      "Epoch 1: train loss: 0.38976606726646423\n",
      "Epoch 1: train loss: 0.5888174772262573\n",
      "Epoch 1: train loss: 0.45898035168647766\n",
      "Epoch 1: train loss: 0.42586180567741394\n",
      "Epoch 1: train loss: 0.39023351669311523\n",
      "Epoch 1: train loss: 0.20841506123542786\n",
      "Epoch 1: train loss: 0.313941091299057\n",
      "Epoch 1: train loss: 0.5510286688804626\n",
      "Epoch 1: train loss: 0.3841283917427063\n",
      "Epoch 1: train loss: 0.4143398404121399\n",
      "Epoch 1: train loss: 0.24784201383590698\n",
      "Epoch 1: train loss: 0.35587969422340393\n",
      "Epoch 1: train loss: 0.3545147478580475\n",
      "Epoch 1: train loss: 0.3716048300266266\n",
      "Epoch 1: train loss: 0.5543501377105713\n",
      "Epoch 1: train loss: 0.5069857835769653\n",
      "Epoch 1: train loss: 0.5136384963989258\n",
      "Epoch 1: train loss: 0.4203149080276489\n",
      "Epoch 1: train loss: 0.4427779018878937\n",
      "Epoch 1: train loss: 0.41990983486175537\n",
      "Epoch 2: train loss: 0.2629512846469879\n",
      "Epoch 2: train loss: 0.5089548826217651\n",
      "Epoch 2: train loss: 0.43687328696250916\n",
      "Epoch 2: train loss: 0.5450446009635925\n",
      "Epoch 2: train loss: 0.37365883588790894\n",
      "Epoch 2: train loss: 0.46384334564208984\n",
      "Epoch 2: train loss: 0.301264226436615\n",
      "Epoch 2: train loss: 0.4034339487552643\n",
      "Epoch 2: train loss: 0.40845221281051636\n",
      "Epoch 2: train loss: 0.3535051941871643\n",
      "Epoch 2: train loss: 0.5165534615516663\n",
      "Epoch 2: train loss: 0.42351049184799194\n",
      "Epoch 2: train loss: 0.36114323139190674\n",
      "Epoch 2: train loss: 0.2607787847518921\n",
      "Epoch 2: train loss: 0.36493125557899475\n",
      "Epoch 2: train loss: 0.3987045884132385\n",
      "Epoch 2: train loss: 0.595022976398468\n",
      "Epoch 2: train loss: 0.36136844754219055\n",
      "Epoch 2: train loss: 0.2619231939315796\n",
      "Epoch 2: train loss: 0.3308359980583191\n",
      "Epoch 2: train loss: 0.21967913210391998\n",
      "Epoch 2: train loss: 0.3959493339061737\n",
      "Epoch 2: train loss: 0.3751554787158966\n",
      "Epoch 2: train loss: 0.5350521802902222\n",
      "Epoch 2: train loss: 0.22012487053871155\n",
      "Epoch 2: train loss: 0.34502309560775757\n",
      "Epoch 2: train loss: 0.3326827585697174\n",
      "Epoch 2: train loss: 0.49832361936569214\n",
      "Epoch 2: train loss: 0.3713682293891907\n",
      "Epoch 2: train loss: 0.4438486099243164\n",
      "Epoch 2: train loss: 0.3916188180446625\n",
      "Epoch 2: train loss: 0.493627667427063\n",
      "Epoch 2: train loss: 0.18314875662326813\n",
      "Epoch 2: train loss: 0.4136236011981964\n",
      "Epoch 2: train loss: 0.49462801218032837\n",
      "Epoch 2: train loss: 0.27785524725914\n",
      "Epoch 2: train loss: 0.31640395522117615\n",
      "Epoch 2: train loss: 0.23070505261421204\n",
      "Epoch 2: train loss: 0.3847331702709198\n",
      "Epoch 2: train loss: 0.3353636860847473\n",
      "Epoch 2: train loss: 0.32326921820640564\n",
      "Epoch 2: train loss: 0.2819449305534363\n",
      "Epoch 2: train loss: 0.29478907585144043\n",
      "Epoch 2: train loss: 0.43457069993019104\n",
      "Epoch 2: train loss: 0.6610288619995117\n",
      "Epoch 2: train loss: 0.42637550830841064\n",
      "Epoch 2: train loss: 0.4379274845123291\n",
      "Epoch 2: train loss: 0.27253565192222595\n",
      "Epoch 2: train loss: 0.44639483094215393\n",
      "Epoch 2: train loss: 0.4218467175960541\n",
      "Epoch 2: train loss: 0.4543765187263489\n",
      "Epoch 2: train loss: 0.414887398481369\n",
      "Epoch 2: train loss: 0.31120389699935913\n",
      "Epoch 2: train loss: 0.4088943898677826\n",
      "Epoch 2: train loss: 0.4342692792415619\n",
      "Epoch 2: train loss: 0.5217796564102173\n",
      "Epoch 2: train loss: 0.4716625511646271\n",
      "Epoch 2: train loss: 0.36948585510253906\n",
      "Epoch 2: train loss: 0.43301019072532654\n",
      "Epoch 2: train loss: 0.515886127948761\n",
      "Epoch 2: train loss: 0.3494834005832672\n",
      "Epoch 2: train loss: 0.3288325071334839\n",
      "Epoch 2: train loss: 0.3393682837486267\n",
      "Epoch 2: train loss: 0.35533878207206726\n",
      "Epoch 2: train loss: 0.2245108187198639\n",
      "Epoch 2: train loss: 0.36104777455329895\n",
      "Epoch 2: train loss: 0.46675142645835876\n",
      "Epoch 2: train loss: 0.33843085169792175\n",
      "Epoch 2: train loss: 0.42566797137260437\n",
      "Epoch 2: train loss: 0.5338674187660217\n",
      "Epoch 2: train loss: 0.2079053819179535\n",
      "Epoch 2: train loss: 0.20402131974697113\n",
      "Epoch 2: train loss: 0.4235842823982239\n",
      "Epoch 2: train loss: 0.2361631989479065\n",
      "Epoch 2: train loss: 0.2775145471096039\n",
      "Epoch 2: train loss: 0.265595406293869\n",
      "Epoch 2: train loss: 0.26835429668426514\n",
      "Epoch 2: train loss: 0.13178999722003937\n",
      "Epoch 3: train loss: 0.5372018218040466\n",
      "Epoch 3: train loss: 0.37792712450027466\n",
      "Epoch 3: train loss: 0.4784446358680725\n",
      "Epoch 3: train loss: 0.45292842388153076\n",
      "Epoch 3: train loss: 0.3148999512195587\n",
      "Epoch 3: train loss: 0.35785043239593506\n",
      "Epoch 3: train loss: 0.3412545323371887\n",
      "Epoch 3: train loss: 0.35143712162971497\n",
      "Epoch 3: train loss: 0.2589280605316162\n",
      "Epoch 3: train loss: 0.3299260139465332\n",
      "Epoch 3: train loss: 0.3714970350265503\n",
      "Epoch 3: train loss: 0.42146503925323486\n",
      "Epoch 3: train loss: 0.5421220064163208\n",
      "Epoch 3: train loss: 0.352260947227478\n",
      "Epoch 3: train loss: 0.5446802377700806\n",
      "Epoch 3: train loss: 0.22241151332855225\n",
      "Epoch 3: train loss: 0.26760178804397583\n",
      "Epoch 3: train loss: 0.4439617395401001\n",
      "Epoch 3: train loss: 0.5292064547538757\n",
      "Epoch 3: train loss: 0.4895825684070587\n",
      "Epoch 3: train loss: 0.44574257731437683\n",
      "Epoch 3: train loss: 0.42844218015670776\n",
      "Epoch 3: train loss: 0.3279600441455841\n",
      "Epoch 3: train loss: 0.3537353277206421\n",
      "Epoch 3: train loss: 0.40394818782806396\n",
      "Epoch 3: train loss: 0.4065678119659424\n",
      "Epoch 3: train loss: 0.3722440302371979\n",
      "Epoch 3: train loss: 0.36443471908569336\n",
      "Epoch 3: train loss: 0.34209883213043213\n",
      "Epoch 3: train loss: 0.3235970437526703\n",
      "Epoch 3: train loss: 0.3142751455307007\n",
      "Epoch 3: train loss: 0.3355507254600525\n",
      "Epoch 3: train loss: 0.5009132623672485\n",
      "Epoch 3: train loss: 0.22463861107826233\n",
      "Epoch 3: train loss: 0.1886957287788391\n",
      "Epoch 3: train loss: 0.3256651759147644\n",
      "Epoch 3: train loss: 0.5173522233963013\n",
      "Epoch 3: train loss: 0.20607462525367737\n",
      "Epoch 3: train loss: 0.291239470243454\n",
      "Epoch 3: train loss: 0.26215216517448425\n",
      "Epoch 3: train loss: 0.3118727207183838\n",
      "Epoch 3: train loss: 0.3077762722969055\n",
      "Epoch 3: train loss: 0.5055333375930786\n",
      "Epoch 3: train loss: 0.4161755442619324\n",
      "Epoch 3: train loss: 0.48618975281715393\n",
      "Epoch 3: train loss: 0.33037400245666504\n",
      "Epoch 3: train loss: 0.30699342489242554\n",
      "Epoch 3: train loss: 0.40448713302612305\n",
      "Epoch 3: train loss: 0.36251547932624817\n",
      "Epoch 3: train loss: 0.2749500870704651\n",
      "Epoch 3: train loss: 0.5129001140594482\n",
      "Epoch 3: train loss: 0.3164486885070801\n",
      "Epoch 3: train loss: 0.3385779857635498\n",
      "Epoch 3: train loss: 0.44592660665512085\n",
      "Epoch 3: train loss: 0.5294641256332397\n",
      "Epoch 3: train loss: 0.3941332697868347\n",
      "Epoch 3: train loss: 0.3643801510334015\n",
      "Epoch 3: train loss: 0.3382694721221924\n",
      "Epoch 3: train loss: 0.2204625904560089\n",
      "Epoch 3: train loss: 0.3380683660507202\n",
      "Epoch 3: train loss: 0.3567953407764435\n",
      "Epoch 3: train loss: 0.33133038878440857\n",
      "Epoch 3: train loss: 0.30770745873451233\n",
      "Epoch 3: train loss: 0.3637387156486511\n",
      "Epoch 3: train loss: 0.25935429334640503\n",
      "Epoch 3: train loss: 0.3373506963253021\n",
      "Epoch 3: train loss: 0.4956675171852112\n",
      "Epoch 3: train loss: 0.32675492763519287\n",
      "Epoch 3: train loss: 0.5450827479362488\n",
      "Epoch 3: train loss: 0.3628065884113312\n",
      "Epoch 3: train loss: 0.4086960554122925\n",
      "Epoch 3: train loss: 0.3563089966773987\n",
      "Epoch 3: train loss: 0.3590850830078125\n",
      "Epoch 3: train loss: 0.3357321619987488\n",
      "Epoch 3: train loss: 0.37118402123451233\n",
      "Epoch 3: train loss: 0.28954192996025085\n",
      "Epoch 3: train loss: 0.22765156626701355\n",
      "Epoch 3: train loss: 0.725491464138031\n",
      "Epoch 4: train loss: 0.3168255388736725\n",
      "Epoch 4: train loss: 0.2695242464542389\n",
      "Epoch 4: train loss: 0.3066577613353729\n",
      "Epoch 4: train loss: 0.34406915307044983\n",
      "Epoch 4: train loss: 0.435562402009964\n",
      "Epoch 4: train loss: 0.2631396949291229\n",
      "Epoch 4: train loss: 0.3046373426914215\n",
      "Epoch 4: train loss: 0.3793378174304962\n",
      "Epoch 4: train loss: 0.2774978578090668\n",
      "Epoch 4: train loss: 0.3957892954349518\n",
      "Epoch 4: train loss: 0.24419504404067993\n",
      "Epoch 4: train loss: 0.20006217062473297\n",
      "Epoch 4: train loss: 0.21269726753234863\n",
      "Epoch 4: train loss: 0.50008624792099\n",
      "Epoch 4: train loss: 0.5096474885940552\n",
      "Epoch 4: train loss: 0.3084714114665985\n",
      "Epoch 4: train loss: 0.34252098202705383\n",
      "Epoch 4: train loss: 0.3209060728549957\n",
      "Epoch 4: train loss: 0.3153703212738037\n",
      "Epoch 4: train loss: 0.33402425050735474\n",
      "Epoch 4: train loss: 0.21297717094421387\n",
      "Epoch 4: train loss: 0.34060996770858765\n",
      "Epoch 4: train loss: 0.2954489588737488\n",
      "Epoch 4: train loss: 0.424448162317276\n",
      "Epoch 4: train loss: 0.42102178931236267\n",
      "Epoch 4: train loss: 0.35025912523269653\n",
      "Epoch 4: train loss: 0.2553383409976959\n",
      "Epoch 4: train loss: 0.20576857030391693\n",
      "Epoch 4: train loss: 0.5309587717056274\n",
      "Epoch 4: train loss: 0.43645238876342773\n",
      "Epoch 4: train loss: 0.48001307249069214\n",
      "Epoch 4: train loss: 0.4296885132789612\n",
      "Epoch 4: train loss: 0.26788130402565\n",
      "Epoch 4: train loss: 0.266509085893631\n",
      "Epoch 4: train loss: 0.7182170152664185\n",
      "Epoch 4: train loss: 0.43259862065315247\n",
      "Epoch 4: train loss: 0.2973821461200714\n",
      "Epoch 4: train loss: 0.34136712551116943\n",
      "Epoch 4: train loss: 0.3411341607570648\n",
      "Epoch 4: train loss: 0.3938664495944977\n",
      "Epoch 4: train loss: 0.2532079517841339\n",
      "Epoch 4: train loss: 0.5111343264579773\n",
      "Epoch 4: train loss: 0.27608707547187805\n",
      "Epoch 4: train loss: 0.42598527669906616\n",
      "Epoch 4: train loss: 0.5048091411590576\n",
      "Epoch 4: train loss: 0.30447015166282654\n",
      "Epoch 4: train loss: 0.18879269063472748\n",
      "Epoch 4: train loss: 0.24618549644947052\n",
      "Epoch 4: train loss: 0.1561814397573471\n",
      "Epoch 4: train loss: 0.401420921087265\n",
      "Epoch 4: train loss: 0.29693955183029175\n",
      "Epoch 4: train loss: 0.3282036781311035\n",
      "Epoch 4: train loss: 0.524482250213623\n",
      "Epoch 4: train loss: 0.44022178649902344\n",
      "Epoch 4: train loss: 0.37007489800453186\n",
      "Epoch 4: train loss: 0.49302610754966736\n",
      "Epoch 4: train loss: 0.4213297367095947\n",
      "Epoch 4: train loss: 0.5322880744934082\n",
      "Epoch 4: train loss: 0.4794868230819702\n",
      "Epoch 4: train loss: 0.561208963394165\n",
      "Epoch 4: train loss: 0.3413110673427582\n",
      "Epoch 4: train loss: 0.380319207906723\n",
      "Epoch 4: train loss: 0.35466885566711426\n",
      "Epoch 4: train loss: 0.4373272955417633\n",
      "Epoch 4: train loss: 0.3424076735973358\n",
      "Epoch 4: train loss: 0.36342310905456543\n",
      "Epoch 4: train loss: 0.3768513798713684\n",
      "Epoch 4: train loss: 0.4338780641555786\n",
      "Epoch 4: train loss: 0.49069973826408386\n",
      "Epoch 4: train loss: 0.5469170212745667\n",
      "Epoch 4: train loss: 0.5109666585922241\n",
      "Epoch 4: train loss: 0.16648982465267181\n",
      "Epoch 4: train loss: 0.3927021920681\n",
      "Epoch 4: train loss: 0.20083042979240417\n",
      "Epoch 4: train loss: 0.314836710691452\n",
      "Epoch 4: train loss: 0.28829777240753174\n",
      "Epoch 4: train loss: 0.3690762519836426\n",
      "Epoch 4: train loss: 0.2021438479423523\n",
      "Epoch 5: train loss: 0.5933797955513\n",
      "Epoch 5: train loss: 0.3842369019985199\n",
      "Epoch 5: train loss: 0.4108668267726898\n",
      "Epoch 5: train loss: 0.20002219080924988\n",
      "Epoch 5: train loss: 0.23295125365257263\n",
      "Epoch 5: train loss: 0.5078381896018982\n",
      "Epoch 5: train loss: 0.2488221526145935\n",
      "Epoch 5: train loss: 0.31406518816947937\n",
      "Epoch 5: train loss: 0.33383214473724365\n",
      "Epoch 5: train loss: 0.21419209241867065\n",
      "Epoch 5: train loss: 0.2516307532787323\n",
      "Epoch 5: train loss: 0.2588440477848053\n",
      "Epoch 5: train loss: 0.3430712819099426\n",
      "Epoch 5: train loss: 0.4019392132759094\n",
      "Epoch 5: train loss: 0.3783378601074219\n",
      "Epoch 5: train loss: 0.29457390308380127\n",
      "Epoch 5: train loss: 0.39465194940567017\n",
      "Epoch 5: train loss: 0.4666595757007599\n",
      "Epoch 5: train loss: 0.3108942210674286\n",
      "Epoch 5: train loss: 0.23424531519412994\n",
      "Epoch 5: train loss: 0.2997948229312897\n",
      "Epoch 5: train loss: 0.3143513798713684\n",
      "Epoch 5: train loss: 0.4026012718677521\n",
      "Epoch 5: train loss: 0.44804465770721436\n",
      "Epoch 5: train loss: 0.32204365730285645\n",
      "Epoch 5: train loss: 0.3931141793727875\n",
      "Epoch 5: train loss: 0.2707630693912506\n",
      "Epoch 5: train loss: 0.1885639876127243\n",
      "Epoch 5: train loss: 0.5230598449707031\n",
      "Epoch 5: train loss: 0.3588517904281616\n",
      "Epoch 5: train loss: 0.3665826618671417\n",
      "Epoch 5: train loss: 0.4673205316066742\n",
      "Epoch 5: train loss: 0.22786945104599\n",
      "Epoch 5: train loss: 0.32399654388427734\n",
      "Epoch 5: train loss: 0.3248221278190613\n",
      "Epoch 5: train loss: 0.39756712317466736\n",
      "Epoch 5: train loss: 0.4548853933811188\n",
      "Epoch 5: train loss: 0.3497501015663147\n",
      "Epoch 5: train loss: 0.45502912998199463\n",
      "Epoch 5: train loss: 0.23227067291736603\n",
      "Epoch 5: train loss: 0.2909330725669861\n",
      "Epoch 5: train loss: 0.47759267687797546\n",
      "Epoch 5: train loss: 0.40806785225868225\n",
      "Epoch 5: train loss: 0.4978829622268677\n",
      "Epoch 5: train loss: 0.39729252457618713\n",
      "Epoch 5: train loss: 0.37474679946899414\n",
      "Epoch 5: train loss: 0.34272220730781555\n",
      "Epoch 5: train loss: 0.3632940649986267\n",
      "Epoch 5: train loss: 0.2657308280467987\n",
      "Epoch 5: train loss: 0.3917052745819092\n",
      "Epoch 5: train loss: 0.3557800352573395\n",
      "Epoch 5: train loss: 0.26103201508522034\n",
      "Epoch 5: train loss: 0.5247770547866821\n",
      "Epoch 5: train loss: 0.22663119435310364\n",
      "Epoch 5: train loss: 0.3299575448036194\n",
      "Epoch 5: train loss: 0.5437423586845398\n",
      "Epoch 5: train loss: 0.38596078753471375\n",
      "Epoch 5: train loss: 0.48829036951065063\n",
      "Epoch 5: train loss: 0.4666753113269806\n",
      "Epoch 5: train loss: 0.2897520959377289\n",
      "Epoch 5: train loss: 0.3716776371002197\n",
      "Epoch 5: train loss: 0.25064775347709656\n",
      "Epoch 5: train loss: 0.6741265654563904\n",
      "Epoch 5: train loss: 0.27477213740348816\n",
      "Epoch 5: train loss: 0.34404465556144714\n",
      "Epoch 5: train loss: 0.34426555037498474\n",
      "Epoch 5: train loss: 0.4716052711009979\n",
      "Epoch 5: train loss: 0.3216239809989929\n",
      "Epoch 5: train loss: 0.3486810326576233\n",
      "Epoch 5: train loss: 0.4558332860469818\n",
      "Epoch 5: train loss: 0.4149783253669739\n",
      "Epoch 5: train loss: 0.1833190768957138\n",
      "Epoch 5: train loss: 0.5276541709899902\n",
      "Epoch 5: train loss: 0.34561434388160706\n",
      "Epoch 5: train loss: 0.37315109372138977\n",
      "Epoch 5: train loss: 0.32040300965309143\n",
      "Epoch 5: train loss: 0.46871766448020935\n",
      "Epoch 5: train loss: 0.18951040506362915\n",
      "Epoch 6: train loss: 0.24464151263237\n",
      "Epoch 6: train loss: 0.28737330436706543\n",
      "Epoch 6: train loss: 0.46672165393829346\n",
      "Epoch 6: train loss: 0.3547498881816864\n",
      "Epoch 6: train loss: 0.370125949382782\n",
      "Epoch 6: train loss: 0.28132203221321106\n",
      "Epoch 6: train loss: 0.2605821490287781\n",
      "Epoch 6: train loss: 0.45067885518074036\n",
      "Epoch 6: train loss: 0.40529578924179077\n",
      "Epoch 6: train loss: 0.3674387037754059\n",
      "Epoch 6: train loss: 0.3503153920173645\n",
      "Epoch 6: train loss: 0.4377695322036743\n",
      "Epoch 6: train loss: 0.5403733849525452\n",
      "Epoch 6: train loss: 0.3401968479156494\n",
      "Epoch 6: train loss: 0.37309297919273376\n",
      "Epoch 6: train loss: 0.4004662036895752\n",
      "Epoch 6: train loss: 0.1683531105518341\n",
      "Epoch 6: train loss: 0.5942513942718506\n",
      "Epoch 6: train loss: 0.49842289090156555\n",
      "Epoch 6: train loss: 0.31090083718299866\n",
      "Epoch 6: train loss: 0.2811655104160309\n",
      "Epoch 6: train loss: 0.41722628474235535\n",
      "Epoch 6: train loss: 0.30055347084999084\n",
      "Epoch 6: train loss: 0.2026556134223938\n",
      "Epoch 6: train loss: 0.5633814930915833\n",
      "Epoch 6: train loss: 0.3186403512954712\n",
      "Epoch 6: train loss: 0.25168377161026\n",
      "Epoch 6: train loss: 0.4866178333759308\n",
      "Epoch 6: train loss: 0.38754406571388245\n",
      "Epoch 6: train loss: 0.2793899178504944\n",
      "Epoch 6: train loss: 0.35194820165634155\n",
      "Epoch 6: train loss: 0.21914049983024597\n",
      "Epoch 6: train loss: 0.29561659693717957\n",
      "Epoch 6: train loss: 0.4136773347854614\n",
      "Epoch 6: train loss: 0.31050413846969604\n",
      "Epoch 6: train loss: 0.3867693543434143\n",
      "Epoch 6: train loss: 0.37723928689956665\n",
      "Epoch 6: train loss: 0.4669400155544281\n",
      "Epoch 6: train loss: 0.5110081434249878\n",
      "Epoch 6: train loss: 0.24067477881908417\n",
      "Epoch 6: train loss: 0.26394355297088623\n",
      "Epoch 6: train loss: 0.23613616824150085\n",
      "Epoch 6: train loss: 0.23897017538547516\n",
      "Epoch 6: train loss: 0.23154306411743164\n",
      "Epoch 6: train loss: 0.5057036876678467\n",
      "Epoch 6: train loss: 0.31186333298683167\n",
      "Epoch 6: train loss: 0.36314913630485535\n",
      "Epoch 6: train loss: 0.3163485825061798\n",
      "Epoch 6: train loss: 0.37991735339164734\n",
      "Epoch 6: train loss: 0.3776456117630005\n",
      "Epoch 6: train loss: 0.3148714303970337\n",
      "Epoch 6: train loss: 0.36940672993659973\n",
      "Epoch 6: train loss: 0.4233505129814148\n",
      "Epoch 6: train loss: 0.26680848002433777\n",
      "Epoch 6: train loss: 0.32215920090675354\n",
      "Epoch 6: train loss: 0.32894450426101685\n",
      "Epoch 6: train loss: 0.3655364513397217\n",
      "Epoch 6: train loss: 0.3525334894657135\n",
      "Epoch 6: train loss: 0.34311243891716003\n",
      "Epoch 6: train loss: 0.36896494030952454\n",
      "Epoch 6: train loss: 0.3476153314113617\n",
      "Epoch 6: train loss: 0.2708997428417206\n",
      "Epoch 6: train loss: 0.372036337852478\n",
      "Epoch 6: train loss: 0.29526597261428833\n",
      "Epoch 6: train loss: 0.30961155891418457\n",
      "Epoch 6: train loss: 0.5362116694450378\n",
      "Epoch 6: train loss: 0.31163114309310913\n",
      "Epoch 6: train loss: 0.3432144224643707\n",
      "Epoch 6: train loss: 0.40327689051628113\n",
      "Epoch 6: train loss: 0.3098735511302948\n",
      "Epoch 6: train loss: 0.377754807472229\n",
      "Epoch 6: train loss: 0.6084452271461487\n",
      "Epoch 6: train loss: 0.387715220451355\n",
      "Epoch 6: train loss: 0.540683388710022\n",
      "Epoch 6: train loss: 0.44654661417007446\n",
      "Epoch 6: train loss: 0.3692578077316284\n",
      "Epoch 6: train loss: 0.385394811630249\n",
      "Epoch 6: train loss: 0.4428154230117798\n",
      "Epoch 7: train loss: 0.516840934753418\n",
      "Epoch 7: train loss: 0.3011516034603119\n",
      "Epoch 7: train loss: 0.49673160910606384\n",
      "Epoch 7: train loss: 0.3960247337818146\n",
      "Epoch 7: train loss: 0.22196990251541138\n",
      "Epoch 7: train loss: 0.26461461186408997\n",
      "Epoch 7: train loss: 0.3134220838546753\n",
      "Epoch 7: train loss: 0.45877113938331604\n",
      "Epoch 7: train loss: 0.2790065407752991\n",
      "Epoch 7: train loss: 0.3151949644088745\n",
      "Epoch 7: train loss: 0.5168882012367249\n",
      "Epoch 7: train loss: 0.4326815903186798\n",
      "Epoch 7: train loss: 0.2507905960083008\n",
      "Epoch 7: train loss: 0.3917480409145355\n",
      "Epoch 7: train loss: 0.16109614074230194\n",
      "Epoch 7: train loss: 0.2904319167137146\n",
      "Epoch 7: train loss: 0.5329791903495789\n",
      "Epoch 7: train loss: 0.2824494242668152\n",
      "Epoch 7: train loss: 0.4129577577114105\n",
      "Epoch 7: train loss: 0.4392889440059662\n",
      "Epoch 7: train loss: 0.2901691794395447\n",
      "Epoch 7: train loss: 0.29977652430534363\n",
      "Epoch 7: train loss: 0.38218486309051514\n",
      "Epoch 7: train loss: 0.312516450881958\n",
      "Epoch 7: train loss: 0.5082964301109314\n",
      "Epoch 7: train loss: 0.35655295848846436\n",
      "Epoch 7: train loss: 0.3407845199108124\n",
      "Epoch 7: train loss: 0.40650197863578796\n",
      "Epoch 7: train loss: 0.31283271312713623\n",
      "Epoch 7: train loss: 0.34100520610809326\n",
      "Epoch 7: train loss: 0.46203726530075073\n",
      "Epoch 7: train loss: 0.29773640632629395\n",
      "Epoch 7: train loss: 0.22524023056030273\n",
      "Epoch 7: train loss: 0.27244383096694946\n",
      "Epoch 7: train loss: 0.3231005072593689\n",
      "Epoch 7: train loss: 0.31208303570747375\n",
      "Epoch 7: train loss: 0.34551990032196045\n",
      "Epoch 7: train loss: 0.4198886752128601\n",
      "Epoch 7: train loss: 0.297980934381485\n",
      "Epoch 7: train loss: 0.44037139415740967\n",
      "Epoch 7: train loss: 0.5260255932807922\n",
      "Epoch 7: train loss: 0.4359733760356903\n",
      "Epoch 7: train loss: 0.38379472494125366\n",
      "Epoch 7: train loss: 0.44882169365882874\n",
      "Epoch 7: train loss: 0.2785915434360504\n",
      "Epoch 7: train loss: 0.42842715978622437\n",
      "Epoch 7: train loss: 0.42176809906959534\n",
      "Epoch 7: train loss: 0.30883145332336426\n",
      "Epoch 7: train loss: 0.3947812020778656\n",
      "Epoch 7: train loss: 0.37582820653915405\n",
      "Epoch 7: train loss: 0.3020919859409332\n",
      "Epoch 7: train loss: 0.4444279074668884\n",
      "Epoch 7: train loss: 0.38814371824264526\n",
      "Epoch 7: train loss: 0.3245614767074585\n",
      "Epoch 7: train loss: 0.35388025641441345\n",
      "Epoch 7: train loss: 0.38482925295829773\n",
      "Epoch 7: train loss: 0.5133306384086609\n",
      "Epoch 7: train loss: 0.4805416464805603\n",
      "Epoch 7: train loss: 0.39144814014434814\n",
      "Epoch 7: train loss: 0.3219115138053894\n",
      "Epoch 7: train loss: 0.47956404089927673\n",
      "Epoch 7: train loss: 0.23814162611961365\n",
      "Epoch 7: train loss: 0.35226088762283325\n",
      "Epoch 7: train loss: 0.26256540417671204\n",
      "Epoch 7: train loss: 0.6142944097518921\n",
      "Epoch 7: train loss: 0.2865530252456665\n",
      "Epoch 7: train loss: 0.3065556585788727\n",
      "Epoch 7: train loss: 0.3568454682826996\n",
      "Epoch 7: train loss: 0.4235750138759613\n",
      "Epoch 7: train loss: 0.41847825050354004\n",
      "Epoch 7: train loss: 0.27477768063545227\n",
      "Epoch 7: train loss: 0.4361231327056885\n",
      "Epoch 7: train loss: 0.28094881772994995\n",
      "Epoch 7: train loss: 0.28832104802131653\n",
      "Epoch 7: train loss: 0.2649844288825989\n",
      "Epoch 7: train loss: 0.33293595910072327\n",
      "Epoch 7: train loss: 0.4876141846179962\n",
      "Epoch 7: train loss: 0.341208815574646\n",
      "Epoch 8: train loss: 0.5690990090370178\n",
      "Epoch 8: train loss: 0.1743951439857483\n",
      "Epoch 8: train loss: 0.31295549869537354\n",
      "Epoch 8: train loss: 0.41083797812461853\n",
      "Epoch 8: train loss: 0.39105379581451416\n",
      "Epoch 8: train loss: 0.2752830684185028\n",
      "Epoch 8: train loss: 0.26755183935165405\n",
      "Epoch 8: train loss: 0.41394880414009094\n",
      "Epoch 8: train loss: 0.4139256775379181\n",
      "Epoch 8: train loss: 0.24989299476146698\n",
      "Epoch 8: train loss: 0.3722812533378601\n",
      "Epoch 8: train loss: 0.5215842723846436\n",
      "Epoch 8: train loss: 0.26148220896720886\n",
      "Epoch 8: train loss: 0.28940990567207336\n",
      "Epoch 8: train loss: 0.3361753821372986\n",
      "Epoch 8: train loss: 0.30180904269218445\n",
      "Epoch 8: train loss: 0.32088151574134827\n",
      "Epoch 8: train loss: 0.3793039321899414\n",
      "Epoch 8: train loss: 0.5637385249137878\n",
      "Epoch 8: train loss: 0.3459989130496979\n",
      "Epoch 8: train loss: 0.4644799530506134\n",
      "Epoch 8: train loss: 0.28937485814094543\n",
      "Epoch 8: train loss: 0.3237397372722626\n",
      "Epoch 8: train loss: 0.2973916232585907\n",
      "Epoch 8: train loss: 0.37067675590515137\n",
      "Epoch 8: train loss: 0.39383363723754883\n",
      "Epoch 8: train loss: 0.24775557219982147\n",
      "Epoch 8: train loss: 0.271899938583374\n",
      "Epoch 8: train loss: 0.2415599524974823\n",
      "Epoch 8: train loss: 0.346958726644516\n",
      "Epoch 8: train loss: 0.4578166604042053\n",
      "Epoch 8: train loss: 0.3930378556251526\n",
      "Epoch 8: train loss: 0.25864630937576294\n",
      "Epoch 8: train loss: 0.3709401786327362\n",
      "Epoch 8: train loss: 0.293127179145813\n",
      "Epoch 8: train loss: 0.4327249825000763\n",
      "Epoch 8: train loss: 0.3655543029308319\n",
      "Epoch 8: train loss: 0.4746713638305664\n",
      "Epoch 8: train loss: 0.23413236439228058\n",
      "Epoch 8: train loss: 0.31755781173706055\n",
      "Epoch 8: train loss: 0.4742697477340698\n",
      "Epoch 8: train loss: 0.4225207567214966\n",
      "Epoch 8: train loss: 0.1721499264240265\n",
      "Epoch 8: train loss: 0.47507357597351074\n",
      "Epoch 8: train loss: 0.4737778306007385\n",
      "Epoch 8: train loss: 0.2923118472099304\n",
      "Epoch 8: train loss: 0.5649166107177734\n",
      "Epoch 8: train loss: 0.23263786733150482\n",
      "Epoch 8: train loss: 0.4101056456565857\n",
      "Epoch 8: train loss: 0.4384790360927582\n",
      "Epoch 8: train loss: 0.505221426486969\n",
      "Epoch 8: train loss: 0.48200249671936035\n",
      "Epoch 8: train loss: 0.30205196142196655\n",
      "Epoch 8: train loss: 0.3123629689216614\n",
      "Epoch 8: train loss: 0.4387420117855072\n",
      "Epoch 8: train loss: 0.37577760219573975\n",
      "Epoch 8: train loss: 0.3126010596752167\n",
      "Epoch 8: train loss: 0.4743580222129822\n",
      "Epoch 8: train loss: 0.2897796034812927\n",
      "Epoch 8: train loss: 0.40189918875694275\n",
      "Epoch 8: train loss: 0.3151732087135315\n",
      "Epoch 8: train loss: 0.31025341153144836\n",
      "Epoch 8: train loss: 0.33318886160850525\n",
      "Epoch 8: train loss: 0.36214104294776917\n",
      "Epoch 8: train loss: 0.42881712317466736\n",
      "Epoch 8: train loss: 0.46467792987823486\n",
      "Epoch 8: train loss: 0.2877042591571808\n",
      "Epoch 8: train loss: 0.68116295337677\n",
      "Epoch 8: train loss: 0.24713443219661713\n",
      "Epoch 8: train loss: 0.29746514558792114\n",
      "Epoch 8: train loss: 0.2881632149219513\n",
      "Epoch 8: train loss: 0.36093875765800476\n",
      "Epoch 8: train loss: 0.247593492269516\n",
      "Epoch 8: train loss: 0.3527173399925232\n",
      "Epoch 8: train loss: 0.3563990294933319\n",
      "Epoch 8: train loss: 0.31918078660964966\n",
      "Epoch 8: train loss: 0.361905962228775\n",
      "Epoch 8: train loss: 0.09917362034320831\n",
      "Epoch 9: train loss: 0.3383628726005554\n",
      "Epoch 9: train loss: 0.26713111996650696\n",
      "Epoch 9: train loss: 0.2921012341976166\n",
      "Epoch 9: train loss: 0.47261565923690796\n",
      "Epoch 9: train loss: 0.23742149770259857\n",
      "Epoch 9: train loss: 0.3758373260498047\n",
      "Epoch 9: train loss: 0.45239514112472534\n",
      "Epoch 9: train loss: 0.3197692036628723\n",
      "Epoch 9: train loss: 0.31323885917663574\n",
      "Epoch 9: train loss: 0.3658057749271393\n",
      "Epoch 9: train loss: 0.35365498065948486\n",
      "Epoch 9: train loss: 0.34320515394210815\n",
      "Epoch 9: train loss: 0.4812603294849396\n",
      "Epoch 9: train loss: 0.2189377397298813\n",
      "Epoch 9: train loss: 0.3422669470310211\n",
      "Epoch 9: train loss: 0.31285473704338074\n",
      "Epoch 9: train loss: 0.4082995653152466\n",
      "Epoch 9: train loss: 0.4798627197742462\n",
      "Epoch 9: train loss: 0.3650979697704315\n",
      "Epoch 9: train loss: 0.24001888930797577\n",
      "Epoch 9: train loss: 0.3157992959022522\n",
      "Epoch 9: train loss: 0.32840222120285034\n",
      "Epoch 9: train loss: 0.4302714467048645\n",
      "Epoch 9: train loss: 0.2988192141056061\n",
      "Epoch 9: train loss: 0.4430916905403137\n",
      "Epoch 9: train loss: 0.5620685815811157\n",
      "Epoch 9: train loss: 0.19197247922420502\n",
      "Epoch 9: train loss: 0.31439608335494995\n",
      "Epoch 9: train loss: 0.4354929029941559\n",
      "Epoch 9: train loss: 0.3722955286502838\n",
      "Epoch 9: train loss: 0.2958519756793976\n",
      "Epoch 9: train loss: 0.3340558111667633\n",
      "Epoch 9: train loss: 0.478397011756897\n",
      "Epoch 9: train loss: 0.44456419348716736\n",
      "Epoch 9: train loss: 0.41021353006362915\n",
      "Epoch 9: train loss: 0.522179901599884\n",
      "Epoch 9: train loss: 0.2696535587310791\n",
      "Epoch 9: train loss: 0.420911580324173\n",
      "Epoch 9: train loss: 0.33145812153816223\n",
      "Epoch 9: train loss: 0.4809983968734741\n",
      "Epoch 9: train loss: 0.47874781489372253\n",
      "Epoch 9: train loss: 0.5412055253982544\n",
      "Epoch 9: train loss: 0.31620991230010986\n",
      "Epoch 9: train loss: 0.2775205671787262\n",
      "Epoch 9: train loss: 0.3763286769390106\n",
      "Epoch 9: train loss: 0.36177465319633484\n",
      "Epoch 9: train loss: 0.3797241151332855\n",
      "Epoch 9: train loss: 0.3919203281402588\n",
      "Epoch 9: train loss: 0.43767258524894714\n",
      "Epoch 9: train loss: 0.42464980483055115\n",
      "Epoch 9: train loss: 0.22243496775627136\n",
      "Epoch 9: train loss: 0.25309523940086365\n",
      "Epoch 9: train loss: 0.5688233971595764\n",
      "Epoch 9: train loss: 0.3923795521259308\n",
      "Epoch 9: train loss: 0.3188146948814392\n",
      "Epoch 9: train loss: 0.1585918664932251\n",
      "Epoch 9: train loss: 0.31654682755470276\n",
      "Epoch 9: train loss: 0.432627409696579\n",
      "Epoch 9: train loss: 0.24700960516929626\n",
      "Epoch 9: train loss: 0.2724062502384186\n",
      "Epoch 9: train loss: 0.38800784945487976\n",
      "Epoch 9: train loss: 0.5983855128288269\n",
      "Epoch 9: train loss: 0.41745537519454956\n",
      "Epoch 9: train loss: 0.4214448928833008\n",
      "Epoch 9: train loss: 0.4107041358947754\n",
      "Epoch 9: train loss: 0.30083227157592773\n",
      "Epoch 9: train loss: 0.44924354553222656\n",
      "Epoch 9: train loss: 0.35312414169311523\n",
      "Epoch 9: train loss: 0.2774961590766907\n",
      "Epoch 9: train loss: 0.40389230847358704\n",
      "Epoch 9: train loss: 0.2845548093318939\n",
      "Epoch 9: train loss: 0.27428820729255676\n",
      "Epoch 9: train loss: 0.26884621381759644\n",
      "Epoch 9: train loss: 0.3347109854221344\n",
      "Epoch 9: train loss: 0.24964193999767303\n",
      "Epoch 9: train loss: 0.39344915747642517\n",
      "Epoch 9: train loss: 0.3320467472076416\n",
      "Epoch 9: train loss: 0.30405575037002563\n",
      "Epoch 10: train loss: 0.44178417325019836\n",
      "Epoch 10: train loss: 0.2252470701932907\n",
      "Epoch 10: train loss: 0.2869434952735901\n",
      "Epoch 10: train loss: 0.2613312602043152\n",
      "Epoch 10: train loss: 0.3078690469264984\n",
      "Epoch 10: train loss: 0.32025381922721863\n",
      "Epoch 10: train loss: 0.18260878324508667\n",
      "Epoch 10: train loss: 0.3613114655017853\n",
      "Epoch 10: train loss: 0.34002992510795593\n",
      "Epoch 10: train loss: 0.5055364370346069\n",
      "Epoch 10: train loss: 0.4272707998752594\n",
      "Epoch 10: train loss: 0.5074230432510376\n",
      "Epoch 10: train loss: 0.30019140243530273\n",
      "Epoch 10: train loss: 0.37515631318092346\n",
      "Epoch 10: train loss: 0.4753261208534241\n",
      "Epoch 10: train loss: 0.48113471269607544\n",
      "Epoch 10: train loss: 0.4453171491622925\n",
      "Epoch 10: train loss: 0.4513597786426544\n",
      "Epoch 10: train loss: 0.29200685024261475\n",
      "Epoch 10: train loss: 0.37448886036872864\n",
      "Epoch 10: train loss: 0.49069109559059143\n",
      "Epoch 10: train loss: 0.3721243143081665\n",
      "Epoch 10: train loss: 0.4430698752403259\n",
      "Epoch 10: train loss: 0.3058961033821106\n",
      "Epoch 10: train loss: 0.35591450333595276\n",
      "Epoch 10: train loss: 0.47199875116348267\n",
      "Epoch 10: train loss: 0.28123027086257935\n",
      "Epoch 10: train loss: 0.3329441249370575\n",
      "Epoch 10: train loss: 0.326474666595459\n",
      "Epoch 10: train loss: 0.3829144835472107\n",
      "Epoch 10: train loss: 0.46423664689064026\n",
      "Epoch 10: train loss: 0.3605544865131378\n",
      "Epoch 10: train loss: 0.37597692012786865\n",
      "Epoch 10: train loss: 0.352077841758728\n",
      "Epoch 10: train loss: 0.3325021266937256\n",
      "Epoch 10: train loss: 0.4478347897529602\n",
      "Epoch 10: train loss: 0.19229626655578613\n",
      "Epoch 10: train loss: 0.4238572418689728\n",
      "Epoch 10: train loss: 0.26258084177970886\n",
      "Epoch 10: train loss: 0.19107721745967865\n",
      "Epoch 10: train loss: 0.27948638796806335\n",
      "Epoch 10: train loss: 0.36495018005371094\n",
      "Epoch 10: train loss: 0.46637532114982605\n",
      "Epoch 10: train loss: 0.3997804820537567\n",
      "Epoch 10: train loss: 0.6679157018661499\n",
      "Epoch 10: train loss: 0.3147377371788025\n",
      "Epoch 10: train loss: 0.5024627447128296\n",
      "Epoch 10: train loss: 0.27082449197769165\n",
      "Epoch 10: train loss: 0.24000854790210724\n",
      "Epoch 10: train loss: 0.42020413279533386\n",
      "Epoch 10: train loss: 0.4347972273826599\n",
      "Epoch 10: train loss: 0.37543556094169617\n",
      "Epoch 10: train loss: 0.260968416929245\n",
      "Epoch 10: train loss: 0.3962085545063019\n",
      "Epoch 10: train loss: 0.3166044056415558\n",
      "Epoch 10: train loss: 0.3730783462524414\n",
      "Epoch 10: train loss: 0.46280437707901\n",
      "Epoch 10: train loss: 0.3205229938030243\n",
      "Epoch 10: train loss: 0.3464759886264801\n",
      "Epoch 10: train loss: 0.26200178265571594\n",
      "Epoch 10: train loss: 0.32423773407936096\n",
      "Epoch 10: train loss: 0.28699856996536255\n",
      "Epoch 10: train loss: 0.36878320574760437\n",
      "Epoch 10: train loss: 0.4077140688896179\n",
      "Epoch 10: train loss: 0.27904587984085083\n",
      "Epoch 10: train loss: 0.32495400309562683\n",
      "Epoch 10: train loss: 0.40190693736076355\n",
      "Epoch 10: train loss: 0.48003920912742615\n",
      "Epoch 10: train loss: 0.18490003049373627\n",
      "Epoch 10: train loss: 0.2939135730266571\n",
      "Epoch 10: train loss: 0.355566143989563\n",
      "Epoch 10: train loss: 0.22980937361717224\n",
      "Epoch 10: train loss: 0.32322344183921814\n",
      "Epoch 10: train loss: 0.2873689830303192\n",
      "Epoch 10: train loss: 0.25938618183135986\n",
      "Epoch 10: train loss: 0.5276319980621338\n",
      "Epoch 10: train loss: 0.2946643531322479\n",
      "Epoch 10: train loss: 0.5648761987686157\n",
      "Epoch 11: train loss: 0.320511132478714\n",
      "Epoch 11: train loss: 0.3621300458908081\n",
      "Epoch 11: train loss: 0.2508905231952667\n",
      "Epoch 11: train loss: 0.34246617555618286\n",
      "Epoch 11: train loss: 0.3148081302642822\n",
      "Epoch 11: train loss: 0.3950793445110321\n",
      "Epoch 11: train loss: 0.4981798827648163\n",
      "Epoch 11: train loss: 0.5216357111930847\n",
      "Epoch 11: train loss: 0.28202852606773376\n",
      "Epoch 11: train loss: 0.21078263223171234\n",
      "Epoch 11: train loss: 0.3135254681110382\n",
      "Epoch 11: train loss: 0.26111119985580444\n",
      "Epoch 11: train loss: 0.5010283589363098\n",
      "Epoch 11: train loss: 0.21000629663467407\n",
      "Epoch 11: train loss: 0.32158198952674866\n",
      "Epoch 11: train loss: 0.20702944695949554\n",
      "Epoch 11: train loss: 0.287433385848999\n",
      "Epoch 11: train loss: 0.3269668519496918\n",
      "Epoch 11: train loss: 0.23832625150680542\n",
      "Epoch 11: train loss: 0.39707106351852417\n",
      "Epoch 11: train loss: 0.5119507908821106\n",
      "Epoch 11: train loss: 0.3377574384212494\n",
      "Epoch 11: train loss: 0.3296780288219452\n",
      "Epoch 11: train loss: 0.5537286996841431\n",
      "Epoch 11: train loss: 0.34640252590179443\n",
      "Epoch 11: train loss: 0.2866719663143158\n",
      "Epoch 11: train loss: 0.3338644802570343\n",
      "Epoch 11: train loss: 0.486127644777298\n",
      "Epoch 11: train loss: 0.7565291523933411\n",
      "Epoch 11: train loss: 0.4993690550327301\n",
      "Epoch 11: train loss: 0.5027239322662354\n",
      "Epoch 11: train loss: 0.4442487955093384\n",
      "Epoch 11: train loss: 0.35947468876838684\n",
      "Epoch 11: train loss: 0.43200719356536865\n",
      "Epoch 11: train loss: 0.3984180688858032\n",
      "Epoch 11: train loss: 0.3314498960971832\n",
      "Epoch 11: train loss: 0.41250279545783997\n",
      "Epoch 11: train loss: 0.2984558939933777\n",
      "Epoch 11: train loss: 0.3006839454174042\n",
      "Epoch 11: train loss: 0.23013868927955627\n",
      "Epoch 11: train loss: 0.30998602509498596\n",
      "Epoch 11: train loss: 0.6225876212120056\n",
      "Epoch 11: train loss: 0.30094802379608154\n",
      "Epoch 11: train loss: 0.35582318902015686\n",
      "Epoch 11: train loss: 0.2355530858039856\n",
      "Epoch 11: train loss: 0.3510523736476898\n",
      "Epoch 11: train loss: 0.4171797037124634\n",
      "Epoch 11: train loss: 0.48716840147972107\n",
      "Epoch 11: train loss: 0.3476329743862152\n",
      "Epoch 11: train loss: 0.27785956859588623\n",
      "Epoch 11: train loss: 0.32367849349975586\n",
      "Epoch 11: train loss: 0.4407678246498108\n",
      "Epoch 11: train loss: 0.32051098346710205\n",
      "Epoch 11: train loss: 0.211995929479599\n",
      "Epoch 11: train loss: 0.2725241780281067\n",
      "Epoch 11: train loss: 0.34209877252578735\n",
      "Epoch 11: train loss: 0.6090363264083862\n",
      "Epoch 11: train loss: 0.2613602876663208\n",
      "Epoch 11: train loss: 0.2942239046096802\n",
      "Epoch 11: train loss: 0.37988218665122986\n",
      "Epoch 11: train loss: 0.26556164026260376\n",
      "Epoch 11: train loss: 0.22550180554389954\n",
      "Epoch 11: train loss: 0.4204299747943878\n",
      "Epoch 11: train loss: 0.2490142285823822\n",
      "Epoch 11: train loss: 0.3611222505569458\n",
      "Epoch 11: train loss: 0.3668385148048401\n",
      "Epoch 11: train loss: 0.3157658576965332\n",
      "Epoch 11: train loss: 0.37166398763656616\n",
      "Epoch 11: train loss: 0.3124527037143707\n",
      "Epoch 11: train loss: 0.3965800106525421\n",
      "Epoch 11: train loss: 0.38946691155433655\n",
      "Epoch 11: train loss: 0.3121181130409241\n",
      "Epoch 11: train loss: 0.37807145714759827\n",
      "Epoch 11: train loss: 0.2934010326862335\n",
      "Epoch 11: train loss: 0.41081684827804565\n",
      "Epoch 11: train loss: 0.397993266582489\n",
      "Epoch 11: train loss: 0.3055903911590576\n",
      "Epoch 11: train loss: 0.24289767444133759\n",
      "Epoch 12: train loss: 0.2370321899652481\n",
      "Epoch 12: train loss: 0.2981409430503845\n",
      "Epoch 12: train loss: 0.2875344753265381\n",
      "Epoch 12: train loss: 0.3659992516040802\n",
      "Epoch 12: train loss: 0.5141932368278503\n",
      "Epoch 12: train loss: 0.2007373422384262\n",
      "Epoch 12: train loss: 0.4720362722873688\n",
      "Epoch 12: train loss: 0.4717879295349121\n",
      "Epoch 12: train loss: 0.3677266836166382\n",
      "Epoch 12: train loss: 0.18338896334171295\n",
      "Epoch 12: train loss: 0.23457816243171692\n",
      "Epoch 12: train loss: 0.26860886812210083\n",
      "Epoch 12: train loss: 0.28358232975006104\n",
      "Epoch 12: train loss: 0.4980766177177429\n",
      "Epoch 12: train loss: 0.3407350182533264\n",
      "Epoch 12: train loss: 0.2886398434638977\n",
      "Epoch 12: train loss: 0.4203297793865204\n",
      "Epoch 12: train loss: 0.3491426408290863\n",
      "Epoch 12: train loss: 0.3974016308784485\n",
      "Epoch 12: train loss: 0.3072807788848877\n",
      "Epoch 12: train loss: 0.38926970958709717\n",
      "Epoch 12: train loss: 0.40984344482421875\n",
      "Epoch 12: train loss: 0.309878408908844\n",
      "Epoch 12: train loss: 0.39504149556159973\n",
      "Epoch 12: train loss: 0.31634387373924255\n",
      "Epoch 12: train loss: 0.4770872890949249\n",
      "Epoch 12: train loss: 0.3208371102809906\n",
      "Epoch 12: train loss: 0.3360969126224518\n",
      "Epoch 12: train loss: 0.5551881194114685\n",
      "Epoch 12: train loss: 0.4658980667591095\n",
      "Epoch 12: train loss: 0.2275337278842926\n",
      "Epoch 12: train loss: 0.3516803979873657\n",
      "Epoch 12: train loss: 0.38888105750083923\n",
      "Epoch 12: train loss: 0.2835383117198944\n",
      "Epoch 12: train loss: 0.4517252743244171\n",
      "Epoch 12: train loss: 0.19952692091464996\n",
      "Epoch 12: train loss: 0.5197495222091675\n",
      "Epoch 12: train loss: 0.34844839572906494\n",
      "Epoch 12: train loss: 0.357044517993927\n",
      "Epoch 12: train loss: 0.2512744069099426\n",
      "Epoch 12: train loss: 0.27018991112709045\n",
      "Epoch 12: train loss: 0.3329421877861023\n",
      "Epoch 12: train loss: 0.3336014747619629\n",
      "Epoch 12: train loss: 0.5283486843109131\n",
      "Epoch 12: train loss: 0.21635393798351288\n",
      "Epoch 12: train loss: 0.27243465185165405\n",
      "Epoch 12: train loss: 0.3019433617591858\n",
      "Epoch 12: train loss: 0.5707058310508728\n",
      "Epoch 12: train loss: 0.2490328699350357\n",
      "Epoch 12: train loss: 0.4719424247741699\n",
      "Epoch 12: train loss: 0.35006043314933777\n",
      "Epoch 12: train loss: 0.3446126878261566\n",
      "Epoch 12: train loss: 0.5994507670402527\n",
      "Epoch 12: train loss: 0.3530883193016052\n",
      "Epoch 12: train loss: 0.317935049533844\n",
      "Epoch 12: train loss: 0.33176174759864807\n",
      "Epoch 12: train loss: 0.37056633830070496\n",
      "Epoch 12: train loss: 0.41455644369125366\n",
      "Epoch 12: train loss: 0.40900567173957825\n",
      "Epoch 12: train loss: 0.2696686387062073\n",
      "Epoch 12: train loss: 0.4462725818157196\n",
      "Epoch 12: train loss: 0.3765937387943268\n",
      "Epoch 12: train loss: 0.34676623344421387\n",
      "Epoch 12: train loss: 0.23316629230976105\n",
      "Epoch 12: train loss: 0.5612908005714417\n",
      "Epoch 12: train loss: 0.3250372111797333\n",
      "Epoch 12: train loss: 0.2667955756187439\n",
      "Epoch 12: train loss: 0.3612831234931946\n",
      "Epoch 12: train loss: 0.43024057149887085\n",
      "Epoch 12: train loss: 0.29161760210990906\n",
      "Epoch 12: train loss: 0.45055869221687317\n",
      "Epoch 12: train loss: 0.3369923233985901\n",
      "Epoch 12: train loss: 0.3536202311515808\n",
      "Epoch 12: train loss: 0.3440593481063843\n",
      "Epoch 12: train loss: 0.46961507201194763\n",
      "Epoch 12: train loss: 0.2991980016231537\n",
      "Epoch 12: train loss: 0.3308188021183014\n",
      "Epoch 12: train loss: 0.11361584067344666\n",
      "Epoch 13: train loss: 0.3027912974357605\n",
      "Epoch 13: train loss: 0.4400425851345062\n",
      "Epoch 13: train loss: 0.3003162741661072\n",
      "Epoch 13: train loss: 0.2430211901664734\n",
      "Epoch 13: train loss: 0.34372398257255554\n",
      "Epoch 13: train loss: 0.37750959396362305\n",
      "Epoch 13: train loss: 0.33206769824028015\n",
      "Epoch 13: train loss: 0.5506693720817566\n",
      "Epoch 13: train loss: 0.35401955246925354\n",
      "Epoch 13: train loss: 0.3387413024902344\n",
      "Epoch 13: train loss: 0.3796010911464691\n",
      "Epoch 13: train loss: 0.40232640504837036\n",
      "Epoch 13: train loss: 0.2919938862323761\n",
      "Epoch 13: train loss: 0.25868409872055054\n",
      "Epoch 13: train loss: 0.33411383628845215\n",
      "Epoch 13: train loss: 0.3388272225856781\n",
      "Epoch 13: train loss: 0.5689406394958496\n",
      "Epoch 13: train loss: 0.35911086201667786\n",
      "Epoch 13: train loss: 0.2814588248729706\n",
      "Epoch 13: train loss: 0.24473837018013\n",
      "Epoch 13: train loss: 0.24682924151420593\n",
      "Epoch 13: train loss: 0.3967868685722351\n",
      "Epoch 13: train loss: 0.4677075445652008\n",
      "Epoch 13: train loss: 0.41919124126434326\n",
      "Epoch 13: train loss: 0.267004132270813\n",
      "Epoch 13: train loss: 0.3232342302799225\n",
      "Epoch 13: train loss: 0.49914979934692383\n",
      "Epoch 13: train loss: 0.38650062680244446\n",
      "Epoch 13: train loss: 0.38196104764938354\n",
      "Epoch 13: train loss: 0.40285545587539673\n",
      "Epoch 13: train loss: 0.419166624546051\n",
      "Epoch 13: train loss: 0.4438580870628357\n",
      "Epoch 13: train loss: 0.3946674168109894\n",
      "Epoch 13: train loss: 0.41619008779525757\n",
      "Epoch 13: train loss: 0.3463640511035919\n",
      "Epoch 13: train loss: 0.33607417345046997\n",
      "Epoch 13: train loss: 0.4088713526725769\n",
      "Epoch 13: train loss: 0.33043909072875977\n",
      "Epoch 13: train loss: 0.2895005941390991\n",
      "Epoch 13: train loss: 0.25885292887687683\n",
      "Epoch 13: train loss: 0.25564125180244446\n",
      "Epoch 13: train loss: 0.2967826724052429\n",
      "Epoch 13: train loss: 0.4357609152793884\n",
      "Epoch 13: train loss: 0.3730261027812958\n",
      "Epoch 13: train loss: 0.5044763088226318\n",
      "Epoch 13: train loss: 0.4777815043926239\n",
      "Epoch 13: train loss: 0.2981753647327423\n",
      "Epoch 13: train loss: 0.27800315618515015\n",
      "Epoch 13: train loss: 0.31967368721961975\n",
      "Epoch 13: train loss: 0.3733929693698883\n",
      "Epoch 13: train loss: 0.3093293309211731\n",
      "Epoch 13: train loss: 0.23679153621196747\n",
      "Epoch 13: train loss: 0.385190486907959\n",
      "Epoch 13: train loss: 0.16964669525623322\n",
      "Epoch 13: train loss: 0.3772227168083191\n",
      "Epoch 13: train loss: 0.39646443724632263\n",
      "Epoch 13: train loss: 0.6560518145561218\n",
      "Epoch 13: train loss: 0.3704809546470642\n",
      "Epoch 13: train loss: 0.19805869460105896\n",
      "Epoch 13: train loss: 0.25145360827445984\n",
      "Epoch 13: train loss: 0.3406629264354706\n",
      "Epoch 13: train loss: 0.5966787934303284\n",
      "Epoch 13: train loss: 0.3646588623523712\n",
      "Epoch 13: train loss: 0.3718990385532379\n",
      "Epoch 13: train loss: 0.3486638069152832\n",
      "Epoch 13: train loss: 0.25960972905158997\n",
      "Epoch 13: train loss: 0.1396467089653015\n",
      "Epoch 13: train loss: 0.361856609582901\n",
      "Epoch 13: train loss: 0.33902570605278015\n",
      "Epoch 13: train loss: 0.33718249201774597\n",
      "Epoch 13: train loss: 0.4715833365917206\n",
      "Epoch 13: train loss: 0.35004380345344543\n",
      "Epoch 13: train loss: 0.45985522866249084\n",
      "Epoch 13: train loss: 0.42891597747802734\n",
      "Epoch 13: train loss: 0.2562229335308075\n",
      "Epoch 13: train loss: 0.265217661857605\n",
      "Epoch 13: train loss: 0.38266628980636597\n",
      "Epoch 13: train loss: 0.9526683688163757\n",
      "Epoch 14: train loss: 0.35039931535720825\n",
      "Epoch 14: train loss: 0.36934635043144226\n",
      "Epoch 14: train loss: 0.4486851692199707\n",
      "Epoch 14: train loss: 0.21214382350444794\n",
      "Epoch 14: train loss: 0.3388912081718445\n",
      "Epoch 14: train loss: 0.23910491168498993\n",
      "Epoch 14: train loss: 0.2930072546005249\n",
      "Epoch 14: train loss: 0.3479023575782776\n",
      "Epoch 14: train loss: 0.1911625862121582\n",
      "Epoch 14: train loss: 0.3674788177013397\n",
      "Epoch 14: train loss: 0.1824490875005722\n",
      "Epoch 14: train loss: 0.2666742503643036\n",
      "Epoch 14: train loss: 0.3868982791900635\n",
      "Epoch 14: train loss: 0.5686294436454773\n",
      "Epoch 14: train loss: 0.2953597605228424\n",
      "Epoch 14: train loss: 0.22138585150241852\n",
      "Epoch 14: train loss: 0.31427326798439026\n",
      "Epoch 14: train loss: 0.3880157768726349\n",
      "Epoch 14: train loss: 0.40471315383911133\n",
      "Epoch 14: train loss: 0.26574936509132385\n",
      "Epoch 14: train loss: 0.49755728244781494\n",
      "Epoch 14: train loss: 0.2291053831577301\n",
      "Epoch 14: train loss: 0.6030749678611755\n",
      "Epoch 14: train loss: 0.3447846472263336\n",
      "Epoch 14: train loss: 0.3065669536590576\n",
      "Epoch 14: train loss: 0.32782724499702454\n",
      "Epoch 14: train loss: 0.29006433486938477\n",
      "Epoch 14: train loss: 0.7868367433547974\n",
      "Epoch 14: train loss: 0.30780765414237976\n",
      "Epoch 14: train loss: 0.36769190430641174\n",
      "Epoch 14: train loss: 0.3347941040992737\n",
      "Epoch 14: train loss: 0.3478243052959442\n",
      "Epoch 14: train loss: 0.5102649927139282\n",
      "Epoch 14: train loss: 0.3535199761390686\n",
      "Epoch 14: train loss: 0.4734222888946533\n",
      "Epoch 14: train loss: 0.3371170163154602\n",
      "Epoch 14: train loss: 0.322868287563324\n",
      "Epoch 14: train loss: 0.24956245720386505\n",
      "Epoch 14: train loss: 0.29089900851249695\n",
      "Epoch 14: train loss: 0.2936316132545471\n",
      "Epoch 14: train loss: 0.5697841048240662\n",
      "Epoch 14: train loss: 0.40786492824554443\n",
      "Epoch 14: train loss: 0.3948197066783905\n",
      "Epoch 14: train loss: 0.44510555267333984\n",
      "Epoch 14: train loss: 0.3479871451854706\n",
      "Epoch 14: train loss: 0.3767944276332855\n",
      "Epoch 14: train loss: 0.3959677219390869\n",
      "Epoch 14: train loss: 0.1957833617925644\n",
      "Epoch 14: train loss: 0.36904415488243103\n",
      "Epoch 14: train loss: 0.33220532536506653\n",
      "Epoch 14: train loss: 0.38355106115341187\n",
      "Epoch 14: train loss: 0.4229321777820587\n",
      "Epoch 14: train loss: 0.513248085975647\n",
      "Epoch 14: train loss: 0.35496169328689575\n",
      "Epoch 14: train loss: 0.321675181388855\n",
      "Epoch 14: train loss: 0.5206628441810608\n",
      "Epoch 14: train loss: 0.2814389169216156\n",
      "Epoch 14: train loss: 0.41842907667160034\n",
      "Epoch 14: train loss: 0.4893370568752289\n",
      "Epoch 14: train loss: 0.2543449401855469\n",
      "Epoch 14: train loss: 0.3419177830219269\n",
      "Epoch 14: train loss: 0.45383211970329285\n",
      "Epoch 14: train loss: 0.39871877431869507\n",
      "Epoch 14: train loss: 0.29995858669281006\n",
      "Epoch 14: train loss: 0.4019753038883209\n",
      "Epoch 14: train loss: 0.3889108896255493\n",
      "Epoch 14: train loss: 0.41492027044296265\n",
      "Epoch 14: train loss: 0.34301483631134033\n",
      "Epoch 14: train loss: 0.31076955795288086\n",
      "Epoch 14: train loss: 0.2485256791114807\n",
      "Epoch 14: train loss: 0.29031723737716675\n",
      "Epoch 14: train loss: 0.39829060435295105\n",
      "Epoch 14: train loss: 0.32474613189697266\n",
      "Epoch 14: train loss: 0.3472849130630493\n",
      "Epoch 14: train loss: 0.32150548696517944\n",
      "Epoch 14: train loss: 0.2869400382041931\n",
      "Epoch 14: train loss: 0.4269113838672638\n",
      "Epoch 14: train loss: 0.10563494265079498\n",
      "Epoch 15: train loss: 0.456199586391449\n",
      "Epoch 15: train loss: 0.24908801913261414\n",
      "Epoch 15: train loss: 0.3197459280490875\n",
      "Epoch 15: train loss: 0.3873608410358429\n",
      "Epoch 15: train loss: 0.4059268832206726\n",
      "Epoch 15: train loss: 0.4320809841156006\n",
      "Epoch 15: train loss: 0.37778353691101074\n",
      "Epoch 15: train loss: 0.430021196603775\n",
      "Epoch 15: train loss: 0.36979958415031433\n",
      "Epoch 15: train loss: 0.35487085580825806\n",
      "Epoch 15: train loss: 0.38480499386787415\n",
      "Epoch 15: train loss: 0.5407077074050903\n",
      "Epoch 15: train loss: 0.32269275188446045\n",
      "Epoch 15: train loss: 0.46189454197883606\n",
      "Epoch 15: train loss: 0.3155694603919983\n",
      "Epoch 15: train loss: 0.30520865321159363\n",
      "Epoch 15: train loss: 0.3150252401828766\n",
      "Epoch 15: train loss: 0.3239611089229584\n",
      "Epoch 15: train loss: 0.3015013635158539\n",
      "Epoch 15: train loss: 0.3217953145503998\n",
      "Epoch 15: train loss: 0.4183047115802765\n",
      "Epoch 15: train loss: 0.44451484084129333\n",
      "Epoch 15: train loss: 0.2909259498119354\n",
      "Epoch 15: train loss: 0.292507141828537\n",
      "Epoch 15: train loss: 0.31698381900787354\n",
      "Epoch 15: train loss: 0.3125123381614685\n",
      "Epoch 15: train loss: 0.2740263044834137\n",
      "Epoch 15: train loss: 0.459730327129364\n",
      "Epoch 15: train loss: 0.34831511974334717\n",
      "Epoch 15: train loss: 0.24315135180950165\n",
      "Epoch 15: train loss: 0.2960803806781769\n",
      "Epoch 15: train loss: 0.2520906925201416\n",
      "Epoch 15: train loss: 0.5861769914627075\n",
      "Epoch 15: train loss: 0.42097797989845276\n",
      "Epoch 15: train loss: 0.4559449255466461\n",
      "Epoch 15: train loss: 0.2754713296890259\n",
      "Epoch 15: train loss: 0.2966872751712799\n",
      "Epoch 15: train loss: 0.24316321313381195\n",
      "Epoch 15: train loss: 0.27385830879211426\n",
      "Epoch 15: train loss: 0.2691400647163391\n",
      "Epoch 15: train loss: 0.3111359775066376\n",
      "Epoch 15: train loss: 0.5374466180801392\n",
      "Epoch 15: train loss: 0.3796836733818054\n",
      "Epoch 15: train loss: 0.2956278622150421\n",
      "Epoch 15: train loss: 0.3978641629219055\n",
      "Epoch 15: train loss: 0.3924637734889984\n",
      "Epoch 15: train loss: 0.42479032278060913\n",
      "Epoch 15: train loss: 0.2855675518512726\n",
      "Epoch 15: train loss: 0.3009272813796997\n",
      "Epoch 15: train loss: 0.2083631157875061\n",
      "Epoch 15: train loss: 0.3964462876319885\n",
      "Epoch 15: train loss: 0.25319063663482666\n",
      "Epoch 15: train loss: 0.6081737875938416\n",
      "Epoch 15: train loss: 0.43743446469306946\n",
      "Epoch 15: train loss: 0.27901405096054077\n",
      "Epoch 15: train loss: 0.2957868278026581\n",
      "Epoch 15: train loss: 0.43899238109588623\n",
      "Epoch 15: train loss: 0.38882049918174744\n",
      "Epoch 15: train loss: 0.5386230945587158\n",
      "Epoch 15: train loss: 0.33865994215011597\n",
      "Epoch 15: train loss: 0.43860578536987305\n",
      "Epoch 15: train loss: 0.2655446231365204\n",
      "Epoch 15: train loss: 0.41731250286102295\n",
      "Epoch 15: train loss: 0.2426554411649704\n",
      "Epoch 15: train loss: 0.20715704560279846\n",
      "Epoch 15: train loss: 0.5337916612625122\n",
      "Epoch 15: train loss: 0.2752878963947296\n",
      "Epoch 15: train loss: 0.2830142378807068\n",
      "Epoch 15: train loss: 0.4001803994178772\n",
      "Epoch 15: train loss: 0.28437578678131104\n",
      "Epoch 15: train loss: 0.3140036165714264\n",
      "Epoch 15: train loss: 0.559130847454071\n",
      "Epoch 15: train loss: 0.367948055267334\n",
      "Epoch 15: train loss: 0.16582688689231873\n",
      "Epoch 15: train loss: 0.37213483452796936\n",
      "Epoch 15: train loss: 0.33477428555488586\n",
      "Epoch 15: train loss: 0.4132474660873413\n",
      "Epoch 15: train loss: 0.2307942658662796\n",
      "Epoch 16: train loss: 0.4853191375732422\n",
      "Epoch 16: train loss: 0.5380025506019592\n",
      "Epoch 16: train loss: 0.4121249318122864\n",
      "Epoch 16: train loss: 0.33105340600013733\n",
      "Epoch 16: train loss: 0.33187809586524963\n",
      "Epoch 16: train loss: 0.2835167944431305\n",
      "Epoch 16: train loss: 0.3157702386379242\n",
      "Epoch 16: train loss: 0.47318145632743835\n",
      "Epoch 16: train loss: 0.32157525420188904\n",
      "Epoch 16: train loss: 0.45351552963256836\n",
      "Epoch 16: train loss: 0.22501322627067566\n",
      "Epoch 16: train loss: 0.35871148109436035\n",
      "Epoch 16: train loss: 0.4586082100868225\n",
      "Epoch 16: train loss: 0.3142593204975128\n",
      "Epoch 16: train loss: 0.405039519071579\n",
      "Epoch 16: train loss: 0.5948556661605835\n",
      "Epoch 16: train loss: 0.3162248432636261\n",
      "Epoch 16: train loss: 0.291732519865036\n",
      "Epoch 16: train loss: 0.5122109651565552\n",
      "Epoch 16: train loss: 0.5058010816574097\n",
      "Epoch 16: train loss: 0.2647898495197296\n",
      "Epoch 16: train loss: 0.4448670744895935\n",
      "Epoch 16: train loss: 0.3260156810283661\n",
      "Epoch 16: train loss: 0.27967366576194763\n",
      "Epoch 16: train loss: 0.2796989381313324\n",
      "Epoch 16: train loss: 0.2852947413921356\n",
      "Epoch 16: train loss: 0.315811425447464\n",
      "Epoch 16: train loss: 0.27541080117225647\n",
      "Epoch 16: train loss: 0.2337539792060852\n",
      "Epoch 16: train loss: 0.43812647461891174\n",
      "Epoch 16: train loss: 0.3034892976284027\n",
      "Epoch 16: train loss: 0.24415801465511322\n",
      "Epoch 16: train loss: 0.3587578237056732\n",
      "Epoch 16: train loss: 0.28025612235069275\n",
      "Epoch 16: train loss: 0.3677722215652466\n",
      "Epoch 16: train loss: 0.3515179455280304\n",
      "Epoch 16: train loss: 0.4912915527820587\n",
      "Epoch 16: train loss: 0.25696730613708496\n",
      "Epoch 16: train loss: 0.40362587571144104\n",
      "Epoch 16: train loss: 0.4502527117729187\n",
      "Epoch 16: train loss: 0.37445950508117676\n",
      "Epoch 16: train loss: 0.33878153562545776\n",
      "Epoch 16: train loss: 0.2436475306749344\n",
      "Epoch 16: train loss: 0.2030494213104248\n",
      "Epoch 16: train loss: 0.4124477505683899\n",
      "Epoch 16: train loss: 0.22649267315864563\n",
      "Epoch 16: train loss: 0.31884151697158813\n",
      "Epoch 16: train loss: 0.1985204517841339\n",
      "Epoch 16: train loss: 0.270409494638443\n",
      "Epoch 16: train loss: 0.523340106010437\n",
      "Epoch 16: train loss: 0.3088333010673523\n",
      "Epoch 16: train loss: 0.46080106496810913\n",
      "Epoch 16: train loss: 0.37312883138656616\n",
      "Epoch 16: train loss: 0.3143474757671356\n",
      "Epoch 16: train loss: 0.35238468647003174\n",
      "Epoch 16: train loss: 0.35864633321762085\n",
      "Epoch 16: train loss: 0.31356218457221985\n",
      "Epoch 16: train loss: 0.3558275103569031\n",
      "Epoch 16: train loss: 0.390340119600296\n",
      "Epoch 16: train loss: 0.33560675382614136\n",
      "Epoch 16: train loss: 0.43192198872566223\n",
      "Epoch 16: train loss: 0.32033127546310425\n",
      "Epoch 16: train loss: 0.29746562242507935\n",
      "Epoch 16: train loss: 0.41224485635757446\n",
      "Epoch 16: train loss: 0.24334056675434113\n",
      "Epoch 16: train loss: 0.3579249382019043\n",
      "Epoch 16: train loss: 0.6149080991744995\n",
      "Epoch 16: train loss: 0.2620234191417694\n",
      "Epoch 16: train loss: 0.32306140661239624\n",
      "Epoch 16: train loss: 0.37864673137664795\n",
      "Epoch 16: train loss: 0.3682441711425781\n",
      "Epoch 16: train loss: 0.328502357006073\n",
      "Epoch 16: train loss: 0.38128116726875305\n",
      "Epoch 16: train loss: 0.25958311557769775\n",
      "Epoch 16: train loss: 0.35427525639533997\n",
      "Epoch 16: train loss: 0.31731969118118286\n",
      "Epoch 16: train loss: 0.3564155697822571\n",
      "Epoch 16: train loss: 0.5603576898574829\n",
      "Epoch 17: train loss: 0.2793048918247223\n",
      "Epoch 17: train loss: 0.3326552212238312\n",
      "Epoch 17: train loss: 0.29627248644828796\n",
      "Epoch 17: train loss: 0.295883446931839\n",
      "Epoch 17: train loss: 0.40518805384635925\n",
      "Epoch 17: train loss: 0.3407324552536011\n",
      "Epoch 17: train loss: 0.3204992711544037\n",
      "Epoch 17: train loss: 0.37833115458488464\n",
      "Epoch 17: train loss: 0.34357595443725586\n",
      "Epoch 17: train loss: 0.3092731833457947\n",
      "Epoch 17: train loss: 0.3120526373386383\n",
      "Epoch 17: train loss: 0.42763862013816833\n",
      "Epoch 17: train loss: 0.4499801695346832\n",
      "Epoch 17: train loss: 0.4168072044849396\n",
      "Epoch 17: train loss: 0.3367510735988617\n",
      "Epoch 17: train loss: 0.3014150559902191\n",
      "Epoch 17: train loss: 0.3042839467525482\n",
      "Epoch 17: train loss: 0.5058026313781738\n",
      "Epoch 17: train loss: 0.45281001925468445\n",
      "Epoch 17: train loss: 0.3478724956512451\n",
      "Epoch 17: train loss: 0.33258959650993347\n",
      "Epoch 17: train loss: 0.5165441632270813\n",
      "Epoch 17: train loss: 0.2487468719482422\n",
      "Epoch 17: train loss: 0.2964504361152649\n",
      "Epoch 17: train loss: 0.4162909984588623\n",
      "Epoch 17: train loss: 0.3175734579563141\n",
      "Epoch 17: train loss: 0.14410144090652466\n",
      "Epoch 17: train loss: 0.28265735507011414\n",
      "Epoch 17: train loss: 0.4923611581325531\n",
      "Epoch 17: train loss: 0.26915764808654785\n",
      "Epoch 17: train loss: 0.5440194010734558\n",
      "Epoch 17: train loss: 0.4594423770904541\n",
      "Epoch 17: train loss: 0.4303271472454071\n",
      "Epoch 17: train loss: 0.25041598081588745\n",
      "Epoch 17: train loss: 0.34518224000930786\n",
      "Epoch 17: train loss: 0.488447904586792\n",
      "Epoch 17: train loss: 0.3828834295272827\n",
      "Epoch 17: train loss: 0.40614986419677734\n",
      "Epoch 17: train loss: 0.23842820525169373\n",
      "Epoch 17: train loss: 0.3136013150215149\n",
      "Epoch 17: train loss: 0.303171306848526\n",
      "Epoch 17: train loss: 0.370787113904953\n",
      "Epoch 17: train loss: 0.2935410737991333\n",
      "Epoch 17: train loss: 0.3020244538784027\n",
      "Epoch 17: train loss: 0.21397559344768524\n",
      "Epoch 17: train loss: 0.40550053119659424\n",
      "Epoch 17: train loss: 0.32123351097106934\n",
      "Epoch 17: train loss: 0.3627740442752838\n",
      "Epoch 17: train loss: 0.4069354832172394\n",
      "Epoch 17: train loss: 0.5922154784202576\n",
      "Epoch 17: train loss: 0.2197621613740921\n",
      "Epoch 17: train loss: 0.3414557874202728\n",
      "Epoch 17: train loss: 0.38183531165122986\n",
      "Epoch 17: train loss: 0.3690025806427002\n",
      "Epoch 17: train loss: 0.43501904606819153\n",
      "Epoch 17: train loss: 0.2603819966316223\n",
      "Epoch 17: train loss: 0.4745250344276428\n",
      "Epoch 17: train loss: 0.2547065019607544\n",
      "Epoch 17: train loss: 0.45231884717941284\n",
      "Epoch 17: train loss: 0.34254640340805054\n",
      "Epoch 17: train loss: 0.4043779671192169\n",
      "Epoch 17: train loss: 0.3144889771938324\n",
      "Epoch 17: train loss: 0.4333280622959137\n",
      "Epoch 17: train loss: 0.3377287983894348\n",
      "Epoch 17: train loss: 0.27704769372940063\n",
      "Epoch 17: train loss: 0.2865813970565796\n",
      "Epoch 17: train loss: 0.3469136357307434\n",
      "Epoch 17: train loss: 0.3302914500236511\n",
      "Epoch 17: train loss: 0.35299384593963623\n",
      "Epoch 17: train loss: 0.2410092055797577\n",
      "Epoch 17: train loss: 0.3514789938926697\n",
      "Epoch 17: train loss: 0.2955608665943146\n",
      "Epoch 17: train loss: 0.36390185356140137\n",
      "Epoch 17: train loss: 0.2655671536922455\n",
      "Epoch 17: train loss: 0.48904088139533997\n",
      "Epoch 17: train loss: 0.6845003366470337\n",
      "Epoch 17: train loss: 0.32285457849502563\n",
      "Epoch 17: train loss: 0.2725650370121002\n",
      "Epoch 18: train loss: 0.348593533039093\n",
      "Epoch 18: train loss: 0.34635475277900696\n",
      "Epoch 18: train loss: 0.4206938147544861\n",
      "Epoch 18: train loss: 0.3391570746898651\n",
      "Epoch 18: train loss: 0.2526583969593048\n",
      "Epoch 18: train loss: 0.39609238505363464\n",
      "Epoch 18: train loss: 0.41720113158226013\n",
      "Epoch 18: train loss: 0.24050381779670715\n",
      "Epoch 18: train loss: 0.3697768449783325\n",
      "Epoch 18: train loss: 0.2845250964164734\n",
      "Epoch 18: train loss: 0.31500157713890076\n",
      "Epoch 18: train loss: 0.23234835267066956\n",
      "Epoch 18: train loss: 0.2630581259727478\n",
      "Epoch 18: train loss: 0.29770562052726746\n",
      "Epoch 18: train loss: 0.29760146141052246\n",
      "Epoch 18: train loss: 0.2079257071018219\n",
      "Epoch 18: train loss: 0.2447565495967865\n",
      "Epoch 18: train loss: 0.3103198707103729\n",
      "Epoch 18: train loss: 0.2466176301240921\n",
      "Epoch 18: train loss: 0.35481056571006775\n",
      "Epoch 18: train loss: 0.4865019917488098\n",
      "Epoch 18: train loss: 0.34600314497947693\n",
      "Epoch 18: train loss: 0.37358081340789795\n",
      "Epoch 18: train loss: 0.5490906834602356\n",
      "Epoch 18: train loss: 0.2871251404285431\n",
      "Epoch 18: train loss: 0.3260550796985626\n",
      "Epoch 18: train loss: 0.3391530513763428\n",
      "Epoch 18: train loss: 0.4770609736442566\n",
      "Epoch 18: train loss: 0.37460067868232727\n",
      "Epoch 18: train loss: 0.19156084954738617\n",
      "Epoch 18: train loss: 0.3822188675403595\n",
      "Epoch 18: train loss: 0.3916819095611572\n",
      "Epoch 18: train loss: 0.4761383831501007\n",
      "Epoch 18: train loss: 0.31763699650764465\n",
      "Epoch 18: train loss: 0.2568194270133972\n",
      "Epoch 18: train loss: 0.27535516023635864\n",
      "Epoch 18: train loss: 0.290156751871109\n",
      "Epoch 18: train loss: 0.24880297482013702\n",
      "Epoch 18: train loss: 0.4035061001777649\n",
      "Epoch 18: train loss: 0.30484750866889954\n",
      "Epoch 18: train loss: 0.35677871108055115\n",
      "Epoch 18: train loss: 0.2340138852596283\n",
      "Epoch 18: train loss: 0.25682950019836426\n",
      "Epoch 18: train loss: 0.5153543949127197\n",
      "Epoch 18: train loss: 0.34558573365211487\n",
      "Epoch 18: train loss: 0.32404759526252747\n",
      "Epoch 18: train loss: 0.42535850405693054\n",
      "Epoch 18: train loss: 0.3862895965576172\n",
      "Epoch 18: train loss: 0.4423587918281555\n",
      "Epoch 18: train loss: 0.45301106572151184\n",
      "Epoch 18: train loss: 0.3696173429489136\n",
      "Epoch 18: train loss: 0.27873551845550537\n",
      "Epoch 18: train loss: 0.25703397393226624\n",
      "Epoch 18: train loss: 0.48191556334495544\n",
      "Epoch 18: train loss: 0.3026792109012604\n",
      "Epoch 18: train loss: 0.40365588665008545\n",
      "Epoch 18: train loss: 0.3442102372646332\n",
      "Epoch 18: train loss: 0.3378794193267822\n",
      "Epoch 18: train loss: 0.44277527928352356\n",
      "Epoch 18: train loss: 0.3277236521244049\n",
      "Epoch 18: train loss: 0.42128607630729675\n",
      "Epoch 18: train loss: 0.48259928822517395\n",
      "Epoch 18: train loss: 0.3525776267051697\n",
      "Epoch 18: train loss: 0.26212677359580994\n",
      "Epoch 18: train loss: 0.3876987099647522\n",
      "Epoch 18: train loss: 0.5373667478561401\n",
      "Epoch 18: train loss: 0.38579562306404114\n",
      "Epoch 18: train loss: 0.27894502878189087\n",
      "Epoch 18: train loss: 0.32038140296936035\n",
      "Epoch 18: train loss: 0.41001006960868835\n",
      "Epoch 18: train loss: 0.38430100679397583\n",
      "Epoch 18: train loss: 0.6153019666671753\n",
      "Epoch 18: train loss: 0.3286179006099701\n",
      "Epoch 18: train loss: 0.4531812369823456\n",
      "Epoch 18: train loss: 0.40710973739624023\n",
      "Epoch 18: train loss: 0.31450051069259644\n",
      "Epoch 18: train loss: 0.5614222288131714\n",
      "Epoch 18: train loss: 0.23999430239200592\n",
      "Epoch 19: train loss: 0.49272382259368896\n",
      "Epoch 19: train loss: 0.554293692111969\n",
      "Epoch 19: train loss: 0.21773578226566315\n",
      "Epoch 19: train loss: 0.3485465943813324\n",
      "Epoch 19: train loss: 0.2972155511379242\n",
      "Epoch 19: train loss: 0.6730034351348877\n",
      "Epoch 19: train loss: 0.38627856969833374\n",
      "Epoch 19: train loss: 0.3622942268848419\n",
      "Epoch 19: train loss: 0.33117368817329407\n",
      "Epoch 19: train loss: 0.3117198944091797\n",
      "Epoch 19: train loss: 0.42722243070602417\n",
      "Epoch 19: train loss: 0.27929970622062683\n",
      "Epoch 19: train loss: 0.2900993525981903\n",
      "Epoch 19: train loss: 0.42942678928375244\n",
      "Epoch 19: train loss: 0.35625553131103516\n",
      "Epoch 19: train loss: 0.4312567114830017\n",
      "Epoch 19: train loss: 0.3767433166503906\n",
      "Epoch 19: train loss: 0.3585112690925598\n",
      "Epoch 19: train loss: 0.19659346342086792\n",
      "Epoch 19: train loss: 0.2729693353176117\n",
      "Epoch 19: train loss: 0.4206189513206482\n",
      "Epoch 19: train loss: 0.21574780344963074\n",
      "Epoch 19: train loss: 0.43188461661338806\n",
      "Epoch 19: train loss: 0.2746257185935974\n",
      "Epoch 19: train loss: 0.627157986164093\n",
      "Epoch 19: train loss: 0.38896721601486206\n",
      "Epoch 19: train loss: 0.4574950635433197\n",
      "Epoch 19: train loss: 0.26855015754699707\n",
      "Epoch 19: train loss: 0.3304448127746582\n",
      "Epoch 19: train loss: 0.4106539785861969\n",
      "Epoch 19: train loss: 0.3007756173610687\n",
      "Epoch 19: train loss: 0.4049418568611145\n",
      "Epoch 19: train loss: 0.3976965844631195\n",
      "Epoch 19: train loss: 0.3406308889389038\n",
      "Epoch 19: train loss: 0.3227555453777313\n",
      "Epoch 19: train loss: 0.2699681520462036\n",
      "Epoch 19: train loss: 0.2219681590795517\n",
      "Epoch 19: train loss: 0.37923023104667664\n",
      "Epoch 19: train loss: 0.38957643508911133\n",
      "Epoch 19: train loss: 0.2884526550769806\n",
      "Epoch 19: train loss: 0.39130160212516785\n",
      "Epoch 19: train loss: 0.36663201451301575\n",
      "Epoch 19: train loss: 0.2714845836162567\n",
      "Epoch 19: train loss: 0.347805380821228\n",
      "Epoch 19: train loss: 0.26784443855285645\n",
      "Epoch 19: train loss: 0.334021657705307\n",
      "Epoch 19: train loss: 0.34412428736686707\n",
      "Epoch 19: train loss: 0.2941955029964447\n",
      "Epoch 19: train loss: 0.35968121886253357\n",
      "Epoch 19: train loss: 0.3397965133190155\n",
      "Epoch 19: train loss: 0.3880418837070465\n",
      "Epoch 19: train loss: 0.1855078488588333\n",
      "Epoch 19: train loss: 0.33632174134254456\n",
      "Epoch 19: train loss: 0.23846305906772614\n",
      "Epoch 19: train loss: 0.34204089641571045\n",
      "Epoch 19: train loss: 0.16263006627559662\n",
      "Epoch 19: train loss: 0.4356488585472107\n",
      "Epoch 19: train loss: 0.3115723729133606\n",
      "Epoch 19: train loss: 0.5052042007446289\n",
      "Epoch 19: train loss: 0.2435482293367386\n",
      "Epoch 19: train loss: 0.3097730278968811\n",
      "Epoch 19: train loss: 0.4664269685745239\n",
      "Epoch 19: train loss: 0.43192192912101746\n",
      "Epoch 19: train loss: 0.42431265115737915\n",
      "Epoch 19: train loss: 0.42463088035583496\n",
      "Epoch 19: train loss: 0.46456506848335266\n",
      "Epoch 19: train loss: 0.3959256708621979\n",
      "Epoch 19: train loss: 0.5594618320465088\n",
      "Epoch 19: train loss: 0.3647899925708771\n",
      "Epoch 19: train loss: 0.15726496279239655\n",
      "Epoch 19: train loss: 0.3465289771556854\n",
      "Epoch 19: train loss: 0.21727468073368073\n",
      "Epoch 19: train loss: 0.31759825348854065\n",
      "Epoch 19: train loss: 0.48192664980888367\n",
      "Epoch 19: train loss: 0.2345341593027115\n",
      "Epoch 19: train loss: 0.39965811371803284\n",
      "Epoch 19: train loss: 0.3166065514087677\n",
      "Epoch 19: train loss: 0.5731409788131714\n",
      "Epoch 20: train loss: 0.2906534969806671\n",
      "Epoch 20: train loss: 0.277572363615036\n",
      "Epoch 20: train loss: 0.3259120583534241\n",
      "Epoch 20: train loss: 0.22261440753936768\n",
      "Epoch 20: train loss: 0.49230289459228516\n",
      "Epoch 20: train loss: 0.3964187800884247\n",
      "Epoch 20: train loss: 0.2744608521461487\n",
      "Epoch 20: train loss: 0.22695818543434143\n",
      "Epoch 20: train loss: 0.3563494384288788\n",
      "Epoch 20: train loss: 0.39518606662750244\n",
      "Epoch 20: train loss: 0.31826117634773254\n",
      "Epoch 20: train loss: 0.1848270297050476\n",
      "Epoch 20: train loss: 0.3143575191497803\n",
      "Epoch 20: train loss: 0.3849602937698364\n",
      "Epoch 20: train loss: 0.3468093276023865\n",
      "Epoch 20: train loss: 0.3000660538673401\n",
      "Epoch 20: train loss: 0.3549867272377014\n",
      "Epoch 20: train loss: 0.3902338147163391\n",
      "Epoch 20: train loss: 0.38691723346710205\n",
      "Epoch 20: train loss: 0.42542967200279236\n",
      "Epoch 20: train loss: 0.39063137769699097\n",
      "Epoch 20: train loss: 0.28565895557403564\n",
      "Epoch 20: train loss: 0.265632688999176\n",
      "Epoch 20: train loss: 0.368535578250885\n",
      "Epoch 20: train loss: 0.37185028195381165\n",
      "Epoch 20: train loss: 0.1904032677412033\n",
      "Epoch 20: train loss: 0.4470306634902954\n",
      "Epoch 20: train loss: 0.42995235323905945\n",
      "Epoch 20: train loss: 0.3679887652397156\n",
      "Epoch 20: train loss: 0.3688454031944275\n",
      "Epoch 20: train loss: 0.18310636281967163\n",
      "Epoch 20: train loss: 0.3755255341529846\n",
      "Epoch 20: train loss: 0.3115518093109131\n",
      "Epoch 20: train loss: 0.301980197429657\n",
      "Epoch 20: train loss: 0.4115918278694153\n",
      "Epoch 20: train loss: 0.2987539768218994\n",
      "Epoch 20: train loss: 0.5642561316490173\n",
      "Epoch 20: train loss: 0.2981615364551544\n",
      "Epoch 20: train loss: 0.5408414006233215\n",
      "Epoch 20: train loss: 0.29048120975494385\n",
      "Epoch 20: train loss: 0.44311225414276123\n",
      "Epoch 20: train loss: 0.3044705092906952\n",
      "Epoch 20: train loss: 0.2236737161874771\n",
      "Epoch 20: train loss: 0.3806268274784088\n",
      "Epoch 20: train loss: 0.27233436703681946\n",
      "Epoch 20: train loss: 0.2497592270374298\n",
      "Epoch 20: train loss: 0.3861272931098938\n",
      "Epoch 20: train loss: 0.3702804148197174\n",
      "Epoch 20: train loss: 0.2766827344894409\n",
      "Epoch 20: train loss: 0.5288107991218567\n",
      "Epoch 20: train loss: 0.3390527367591858\n",
      "Epoch 20: train loss: 0.28218504786491394\n",
      "Epoch 20: train loss: 0.45313435792922974\n",
      "Epoch 20: train loss: 0.3819221556186676\n",
      "Epoch 20: train loss: 0.6387549042701721\n",
      "Epoch 20: train loss: 0.24648137390613556\n",
      "Epoch 20: train loss: 0.3360295295715332\n",
      "Epoch 20: train loss: 0.4165065586566925\n",
      "Epoch 20: train loss: 0.4411672055721283\n",
      "Epoch 20: train loss: 0.31446748971939087\n",
      "Epoch 20: train loss: 0.47069457173347473\n",
      "Epoch 20: train loss: 0.3040333688259125\n",
      "Epoch 20: train loss: 0.2800713777542114\n",
      "Epoch 20: train loss: 0.4126530885696411\n",
      "Epoch 20: train loss: 0.3075984716415405\n",
      "Epoch 20: train loss: 0.331190288066864\n",
      "Epoch 20: train loss: 0.3498585522174835\n",
      "Epoch 20: train loss: 0.4391997754573822\n",
      "Epoch 20: train loss: 0.32825198769569397\n",
      "Epoch 20: train loss: 0.42957085371017456\n",
      "Epoch 20: train loss: 0.3447325825691223\n",
      "Epoch 20: train loss: 0.31057795882225037\n",
      "Epoch 20: train loss: 0.612575113773346\n",
      "Epoch 20: train loss: 0.3873213231563568\n",
      "Epoch 20: train loss: 0.3880787491798401\n",
      "Epoch 20: train loss: 0.34311556816101074\n",
      "Epoch 20: train loss: 0.26934099197387695\n",
      "Epoch 20: train loss: 0.3315314054489136\n",
      "Epoch 21: train loss: 0.3802433907985687\n",
      "Epoch 21: train loss: 0.3833121061325073\n",
      "Epoch 21: train loss: 0.4799257218837738\n",
      "Epoch 21: train loss: 0.3895253837108612\n",
      "Epoch 21: train loss: 0.25903111696243286\n",
      "Epoch 21: train loss: 0.3476843237876892\n",
      "Epoch 21: train loss: 0.3763374090194702\n",
      "Epoch 21: train loss: 0.25600922107696533\n",
      "Epoch 21: train loss: 0.32449430227279663\n",
      "Epoch 21: train loss: 0.24952012300491333\n",
      "Epoch 21: train loss: 0.28228670358657837\n",
      "Epoch 21: train loss: 0.4487575888633728\n",
      "Epoch 21: train loss: 0.3092797100543976\n",
      "Epoch 21: train loss: 0.36873871088027954\n",
      "Epoch 21: train loss: 0.2859688699245453\n",
      "Epoch 21: train loss: 0.29810479283332825\n",
      "Epoch 21: train loss: 0.45416152477264404\n",
      "Epoch 21: train loss: 0.43534567952156067\n",
      "Epoch 21: train loss: 0.4486100673675537\n",
      "Epoch 21: train loss: 0.4841615557670593\n",
      "Epoch 21: train loss: 0.3720027804374695\n",
      "Epoch 21: train loss: 0.3802854120731354\n",
      "Epoch 21: train loss: 0.3068617582321167\n",
      "Epoch 21: train loss: 0.3183682858943939\n",
      "Epoch 21: train loss: 0.4983910024166107\n",
      "Epoch 21: train loss: 0.3681303560733795\n",
      "Epoch 21: train loss: 0.38283273577690125\n",
      "Epoch 21: train loss: 0.4362752139568329\n",
      "Epoch 21: train loss: 0.3011989891529083\n",
      "Epoch 21: train loss: 0.2636958658695221\n",
      "Epoch 21: train loss: 0.3293827474117279\n",
      "Epoch 21: train loss: 0.3311876952648163\n",
      "Epoch 21: train loss: 0.16059167683124542\n",
      "Epoch 21: train loss: 0.22571298480033875\n",
      "Epoch 21: train loss: 0.30271589756011963\n",
      "Epoch 21: train loss: 0.33360007405281067\n",
      "Epoch 21: train loss: 0.36222389340400696\n",
      "Epoch 21: train loss: 0.30091437697410583\n",
      "Epoch 21: train loss: 0.3659798204898834\n",
      "Epoch 21: train loss: 0.3690146207809448\n",
      "Epoch 21: train loss: 0.28536465764045715\n",
      "Epoch 21: train loss: 0.41518354415893555\n",
      "Epoch 21: train loss: 0.37005943059921265\n",
      "Epoch 21: train loss: 0.3054967522621155\n",
      "Epoch 21: train loss: 0.3736612796783447\n",
      "Epoch 21: train loss: 0.43288862705230713\n",
      "Epoch 21: train loss: 0.3111576735973358\n",
      "Epoch 21: train loss: 0.31391602754592896\n",
      "Epoch 21: train loss: 0.28253307938575745\n",
      "Epoch 21: train loss: 0.41828277707099915\n",
      "Epoch 21: train loss: 0.20452378690242767\n",
      "Epoch 21: train loss: 0.2573627233505249\n",
      "Epoch 21: train loss: 0.4687827527523041\n",
      "Epoch 21: train loss: 0.19672748446464539\n",
      "Epoch 21: train loss: 0.37112173438072205\n",
      "Epoch 21: train loss: 0.39480558037757874\n",
      "Epoch 21: train loss: 0.47835245728492737\n",
      "Epoch 21: train loss: 0.5192660093307495\n",
      "Epoch 21: train loss: 0.4289458096027374\n",
      "Epoch 21: train loss: 0.30080199241638184\n",
      "Epoch 21: train loss: 0.37809130549430847\n",
      "Epoch 21: train loss: 0.35964062809944153\n",
      "Epoch 21: train loss: 0.24056030809879303\n",
      "Epoch 21: train loss: 0.5198302865028381\n",
      "Epoch 21: train loss: 0.36733490228652954\n",
      "Epoch 21: train loss: 0.4003341794013977\n",
      "Epoch 21: train loss: 0.47211942076683044\n",
      "Epoch 21: train loss: 0.2868666350841522\n",
      "Epoch 21: train loss: 0.3670094907283783\n",
      "Epoch 21: train loss: 0.35478997230529785\n",
      "Epoch 21: train loss: 0.5792291164398193\n",
      "Epoch 21: train loss: 0.2651478946208954\n",
      "Epoch 21: train loss: 0.3950561285018921\n",
      "Epoch 21: train loss: 0.4275790750980377\n",
      "Epoch 21: train loss: 0.19660189747810364\n",
      "Epoch 21: train loss: 0.32526999711990356\n",
      "Epoch 21: train loss: 0.2871677577495575\n",
      "Epoch 21: train loss: 0.011530529707670212\n",
      "Epoch 22: train loss: 0.36656129360198975\n",
      "Epoch 22: train loss: 0.281114786863327\n",
      "Epoch 22: train loss: 0.4611164927482605\n",
      "Epoch 22: train loss: 0.5439352989196777\n",
      "Epoch 22: train loss: 0.3388730585575104\n",
      "Epoch 22: train loss: 0.4235605001449585\n",
      "Epoch 22: train loss: 0.347832053899765\n",
      "Epoch 22: train loss: 0.3482099175453186\n",
      "Epoch 22: train loss: 0.33865728974342346\n",
      "Epoch 22: train loss: 0.42272546887397766\n",
      "Epoch 22: train loss: 0.35670316219329834\n",
      "Epoch 22: train loss: 0.41444507241249084\n",
      "Epoch 22: train loss: 0.34698259830474854\n",
      "Epoch 22: train loss: 0.37455064058303833\n",
      "Epoch 22: train loss: 0.37799176573753357\n",
      "Epoch 22: train loss: 0.22592243552207947\n",
      "Epoch 22: train loss: 0.3340914845466614\n",
      "Epoch 22: train loss: 0.15618763864040375\n",
      "Epoch 22: train loss: 0.275582492351532\n",
      "Epoch 22: train loss: 0.3408537805080414\n",
      "Epoch 22: train loss: 0.2474883645772934\n",
      "Epoch 22: train loss: 0.5309595465660095\n",
      "Epoch 22: train loss: 0.18711192905902863\n",
      "Epoch 22: train loss: 0.4083453416824341\n",
      "Epoch 22: train loss: 0.20267866551876068\n",
      "Epoch 22: train loss: 0.5184625387191772\n",
      "Epoch 22: train loss: 0.2129458785057068\n",
      "Epoch 22: train loss: 0.37682804465293884\n",
      "Epoch 22: train loss: 0.24452565610408783\n",
      "Epoch 22: train loss: 0.5442233681678772\n",
      "Epoch 22: train loss: 0.5333265662193298\n",
      "Epoch 22: train loss: 0.25260910391807556\n",
      "Epoch 22: train loss: 0.4557704031467438\n",
      "Epoch 22: train loss: 0.18937234580516815\n",
      "Epoch 22: train loss: 0.46087589859962463\n",
      "Epoch 22: train loss: 0.4131695330142975\n",
      "Epoch 22: train loss: 0.39092037081718445\n",
      "Epoch 22: train loss: 0.35774335265159607\n",
      "Epoch 22: train loss: 0.408771276473999\n",
      "Epoch 22: train loss: 0.3499777913093567\n",
      "Epoch 22: train loss: 0.444160521030426\n",
      "Epoch 22: train loss: 0.2913081645965576\n",
      "Epoch 22: train loss: 0.20188553631305695\n",
      "Epoch 22: train loss: 0.25683364272117615\n",
      "Epoch 22: train loss: 0.35462096333503723\n",
      "Epoch 22: train loss: 0.4962158501148224\n",
      "Epoch 22: train loss: 0.2645382583141327\n",
      "Epoch 22: train loss: 0.4258117377758026\n",
      "Epoch 22: train loss: 0.37590980529785156\n",
      "Epoch 22: train loss: 0.5874572992324829\n",
      "Epoch 22: train loss: 0.3961716890335083\n",
      "Epoch 22: train loss: 0.31137141585350037\n",
      "Epoch 22: train loss: 0.40674692392349243\n",
      "Epoch 22: train loss: 0.2960491180419922\n",
      "Epoch 22: train loss: 0.39845114946365356\n",
      "Epoch 22: train loss: 0.2732139825820923\n",
      "Epoch 22: train loss: 0.39127177000045776\n",
      "Epoch 22: train loss: 0.2633792757987976\n",
      "Epoch 22: train loss: 0.3565342128276825\n",
      "Epoch 22: train loss: 0.4001302123069763\n",
      "Epoch 22: train loss: 0.43483078479766846\n",
      "Epoch 22: train loss: 0.3420255184173584\n",
      "Epoch 22: train loss: 0.30780455470085144\n",
      "Epoch 22: train loss: 0.45607462525367737\n",
      "Epoch 22: train loss: 0.25800153613090515\n",
      "Epoch 22: train loss: 0.34960830211639404\n",
      "Epoch 22: train loss: 0.5380586981773376\n",
      "Epoch 22: train loss: 0.27698370814323425\n",
      "Epoch 22: train loss: 0.36556899547576904\n",
      "Epoch 22: train loss: 0.3053067922592163\n",
      "Epoch 22: train loss: 0.33528971672058105\n",
      "Epoch 22: train loss: 0.269454687833786\n",
      "Epoch 22: train loss: 0.3085918128490448\n",
      "Epoch 22: train loss: 0.2827173173427582\n",
      "Epoch 22: train loss: 0.35082921385765076\n",
      "Epoch 22: train loss: 0.39910122752189636\n",
      "Epoch 22: train loss: 0.3480888307094574\n",
      "Epoch 22: train loss: 0.5082947015762329\n",
      "Epoch 23: train loss: 0.2135428488254547\n",
      "Epoch 23: train loss: 0.24388040602207184\n",
      "Epoch 23: train loss: 0.5714208483695984\n",
      "Epoch 23: train loss: 0.1857542097568512\n",
      "Epoch 23: train loss: 0.30228346586227417\n",
      "Epoch 23: train loss: 0.2544809579849243\n",
      "Epoch 23: train loss: 0.4013473689556122\n",
      "Epoch 23: train loss: 0.42922109365463257\n",
      "Epoch 23: train loss: 0.3677758276462555\n",
      "Epoch 23: train loss: 0.30650100111961365\n",
      "Epoch 23: train loss: 0.18969935178756714\n",
      "Epoch 23: train loss: 0.33515849709510803\n",
      "Epoch 23: train loss: 0.45305943489074707\n",
      "Epoch 23: train loss: 0.2016202211380005\n",
      "Epoch 23: train loss: 0.16297458112239838\n",
      "Epoch 23: train loss: 0.5036734342575073\n",
      "Epoch 23: train loss: 0.4201575219631195\n",
      "Epoch 23: train loss: 0.27472415566444397\n",
      "Epoch 23: train loss: 0.32680079340934753\n",
      "Epoch 23: train loss: 0.3895893096923828\n",
      "Epoch 23: train loss: 0.3982487618923187\n",
      "Epoch 23: train loss: 0.4543006718158722\n",
      "Epoch 23: train loss: 0.37492623925209045\n",
      "Epoch 23: train loss: 0.2887721359729767\n",
      "Epoch 23: train loss: 0.266964316368103\n",
      "Epoch 23: train loss: 0.3055430054664612\n",
      "Epoch 23: train loss: 0.36263468861579895\n",
      "Epoch 23: train loss: 0.4412391483783722\n",
      "Epoch 23: train loss: 0.4161049425601959\n",
      "Epoch 23: train loss: 0.3454798460006714\n",
      "Epoch 23: train loss: 0.40178459882736206\n",
      "Epoch 23: train loss: 0.20834216475486755\n",
      "Epoch 23: train loss: 0.48787757754325867\n",
      "Epoch 23: train loss: 0.33539608120918274\n",
      "Epoch 23: train loss: 0.36665523052215576\n",
      "Epoch 23: train loss: 0.3252823054790497\n",
      "Epoch 23: train loss: 0.5686447620391846\n",
      "Epoch 23: train loss: 0.37670302391052246\n",
      "Epoch 23: train loss: 0.4445086419582367\n",
      "Epoch 23: train loss: 0.2438593953847885\n",
      "Epoch 23: train loss: 0.39594388008117676\n",
      "Epoch 23: train loss: 0.42638909816741943\n",
      "Epoch 23: train loss: 0.33222320675849915\n",
      "Epoch 23: train loss: 0.22860217094421387\n",
      "Epoch 23: train loss: 0.2860207259654999\n",
      "Epoch 23: train loss: 0.4026249349117279\n",
      "Epoch 23: train loss: 0.5175211429595947\n",
      "Epoch 23: train loss: 0.4258415400981903\n",
      "Epoch 23: train loss: 0.45950815081596375\n",
      "Epoch 23: train loss: 0.1993141919374466\n",
      "Epoch 23: train loss: 0.360225111246109\n",
      "Epoch 23: train loss: 0.3796400725841522\n",
      "Epoch 23: train loss: 0.33895885944366455\n",
      "Epoch 23: train loss: 0.34402531385421753\n",
      "Epoch 23: train loss: 0.35676658153533936\n",
      "Epoch 23: train loss: 0.5687240958213806\n",
      "Epoch 23: train loss: 0.16848772764205933\n",
      "Epoch 23: train loss: 0.332634299993515\n",
      "Epoch 23: train loss: 0.28825339674949646\n",
      "Epoch 23: train loss: 0.4994860589504242\n",
      "Epoch 23: train loss: 0.265870064496994\n",
      "Epoch 23: train loss: 0.36398160457611084\n",
      "Epoch 23: train loss: 0.2705457806587219\n",
      "Epoch 23: train loss: 0.2959888279438019\n",
      "Epoch 23: train loss: 0.30435317754745483\n",
      "Epoch 23: train loss: 0.4727301597595215\n",
      "Epoch 23: train loss: 0.5325862169265747\n",
      "Epoch 23: train loss: 0.39777305722236633\n",
      "Epoch 23: train loss: 0.4111713171005249\n",
      "Epoch 23: train loss: 0.4251340627670288\n",
      "Epoch 23: train loss: 0.29930800199508667\n",
      "Epoch 23: train loss: 0.4093768000602722\n",
      "Epoch 23: train loss: 0.4578189551830292\n",
      "Epoch 23: train loss: 0.30074542760849\n",
      "Epoch 23: train loss: 0.36425116658210754\n",
      "Epoch 23: train loss: 0.3777526617050171\n",
      "Epoch 23: train loss: 0.3207351863384247\n",
      "Epoch 23: train loss: 0.09346814453601837\n",
      "Epoch 24: train loss: 0.31427884101867676\n",
      "Epoch 24: train loss: 0.36446186900138855\n",
      "Epoch 24: train loss: 0.22452129423618317\n",
      "Epoch 24: train loss: 0.26819854974746704\n",
      "Epoch 24: train loss: 0.33215728402137756\n",
      "Epoch 24: train loss: 0.36998251080513\n",
      "Epoch 24: train loss: 0.39465707540512085\n",
      "Epoch 24: train loss: 0.4987539052963257\n",
      "Epoch 24: train loss: 0.49388429522514343\n",
      "Epoch 24: train loss: 0.40455934405326843\n",
      "Epoch 24: train loss: 0.4600854814052582\n",
      "Epoch 24: train loss: 0.4770592749118805\n",
      "Epoch 24: train loss: 0.3184067904949188\n",
      "Epoch 24: train loss: 0.3392726480960846\n",
      "Epoch 24: train loss: 0.3337976634502411\n",
      "Epoch 24: train loss: 0.31883636116981506\n",
      "Epoch 24: train loss: 0.3339109420776367\n",
      "Epoch 24: train loss: 0.37923744320869446\n",
      "Epoch 24: train loss: 0.2768322825431824\n",
      "Epoch 24: train loss: 0.2567088007926941\n",
      "Epoch 24: train loss: 0.32061800360679626\n",
      "Epoch 24: train loss: 0.2528468370437622\n",
      "Epoch 24: train loss: 0.373829185962677\n",
      "Epoch 24: train loss: 0.3335943818092346\n",
      "Epoch 24: train loss: 0.38735413551330566\n",
      "Epoch 24: train loss: 0.31863993406295776\n",
      "Epoch 24: train loss: 0.3778042793273926\n",
      "Epoch 24: train loss: 0.17523901164531708\n",
      "Epoch 24: train loss: 0.40934619307518005\n",
      "Epoch 24: train loss: 0.28274959325790405\n",
      "Epoch 24: train loss: 0.32791876792907715\n",
      "Epoch 24: train loss: 0.32300031185150146\n",
      "Epoch 24: train loss: 0.5011864304542542\n",
      "Epoch 24: train loss: 0.34413856267929077\n",
      "Epoch 24: train loss: 0.3929024934768677\n",
      "Epoch 24: train loss: 0.32560473680496216\n",
      "Epoch 24: train loss: 0.32355278730392456\n",
      "Epoch 24: train loss: 0.34829986095428467\n",
      "Epoch 24: train loss: 0.26932647824287415\n",
      "Epoch 24: train loss: 0.4827222228050232\n",
      "Epoch 24: train loss: 0.3306984603404999\n",
      "Epoch 24: train loss: 0.2392665594816208\n",
      "Epoch 24: train loss: 0.33367636799812317\n",
      "Epoch 24: train loss: 0.461496502161026\n",
      "Epoch 24: train loss: 0.3697001338005066\n",
      "Epoch 24: train loss: 0.3647874593734741\n",
      "Epoch 24: train loss: 0.2916263937950134\n",
      "Epoch 24: train loss: 0.40273353457450867\n",
      "Epoch 24: train loss: 0.49924373626708984\n",
      "Epoch 24: train loss: 0.36000728607177734\n",
      "Epoch 24: train loss: 0.33346766233444214\n",
      "Epoch 24: train loss: 0.33265820145606995\n",
      "Epoch 24: train loss: 0.49123960733413696\n",
      "Epoch 24: train loss: 0.6696288585662842\n",
      "Epoch 24: train loss: 0.414310097694397\n",
      "Epoch 24: train loss: 0.44921445846557617\n",
      "Epoch 24: train loss: 0.5200063586235046\n",
      "Epoch 24: train loss: 0.39789268374443054\n",
      "Epoch 24: train loss: 0.36898601055145264\n",
      "Epoch 24: train loss: 0.3573605716228485\n",
      "Epoch 24: train loss: 0.2774518132209778\n",
      "Epoch 24: train loss: 0.4358740746974945\n",
      "Epoch 24: train loss: 0.3262462615966797\n",
      "Epoch 24: train loss: 0.3027775287628174\n",
      "Epoch 24: train loss: 0.3948288559913635\n",
      "Epoch 24: train loss: 0.2475367933511734\n",
      "Epoch 24: train loss: 0.40051889419555664\n",
      "Epoch 24: train loss: 0.19375406205654144\n",
      "Epoch 24: train loss: 0.33151307702064514\n",
      "Epoch 24: train loss: 0.3173319399356842\n",
      "Epoch 24: train loss: 0.14543281495571136\n",
      "Epoch 24: train loss: 0.6632795333862305\n",
      "Epoch 24: train loss: 0.41459521651268005\n",
      "Epoch 24: train loss: 0.3618433475494385\n",
      "Epoch 24: train loss: 0.23731990158557892\n",
      "Epoch 24: train loss: 0.30014291405677795\n",
      "Epoch 24: train loss: 0.2934064269065857\n",
      "Epoch 24: train loss: 0.1058509573340416\n",
      "Epoch 25: train loss: 0.5008650422096252\n",
      "Epoch 25: train loss: 0.44913843274116516\n",
      "Epoch 25: train loss: 0.36726683378219604\n",
      "Epoch 25: train loss: 0.24532543122768402\n",
      "Epoch 25: train loss: 0.3502662181854248\n",
      "Epoch 25: train loss: 0.2965426445007324\n",
      "Epoch 25: train loss: 0.4089094400405884\n",
      "Epoch 25: train loss: 0.30509549379348755\n",
      "Epoch 25: train loss: 0.39170387387275696\n",
      "Epoch 25: train loss: 0.2575231194496155\n",
      "Epoch 25: train loss: 0.345507949590683\n",
      "Epoch 25: train loss: 0.3030036687850952\n",
      "Epoch 25: train loss: 0.26182079315185547\n",
      "Epoch 25: train loss: 0.2574255168437958\n",
      "Epoch 25: train loss: 0.363368421792984\n",
      "Epoch 25: train loss: 0.20309415459632874\n",
      "Epoch 25: train loss: 0.27060142159461975\n",
      "Epoch 25: train loss: 0.48863252997398376\n",
      "Epoch 25: train loss: 0.2890969216823578\n",
      "Epoch 25: train loss: 0.5323572158813477\n",
      "Epoch 25: train loss: 0.2974686026573181\n",
      "Epoch 25: train loss: 0.3726999759674072\n",
      "Epoch 25: train loss: 0.39786168932914734\n",
      "Epoch 25: train loss: 0.4563082158565521\n",
      "Epoch 25: train loss: 0.32218512892723083\n",
      "Epoch 25: train loss: 0.4329356551170349\n",
      "Epoch 25: train loss: 0.3257538378238678\n",
      "Epoch 25: train loss: 0.32818764448165894\n",
      "Epoch 25: train loss: 0.43221333622932434\n",
      "Epoch 25: train loss: 0.3533240258693695\n",
      "Epoch 25: train loss: 0.1407507210969925\n",
      "Epoch 25: train loss: 0.5960409045219421\n",
      "Epoch 25: train loss: 0.3472754955291748\n",
      "Epoch 25: train loss: 0.28460511565208435\n",
      "Epoch 25: train loss: 0.6276834011077881\n",
      "Epoch 25: train loss: 0.32315921783447266\n",
      "Epoch 25: train loss: 0.4480050206184387\n",
      "Epoch 25: train loss: 0.24773862957954407\n",
      "Epoch 25: train loss: 0.3207755386829376\n",
      "Epoch 25: train loss: 0.27964022755622864\n",
      "Epoch 25: train loss: 0.2554972767829895\n",
      "Epoch 25: train loss: 0.24710097908973694\n",
      "Epoch 25: train loss: 0.30826714634895325\n",
      "Epoch 25: train loss: 0.2125486135482788\n",
      "Epoch 25: train loss: 0.3670800030231476\n",
      "Epoch 25: train loss: 0.26186469197273254\n",
      "Epoch 25: train loss: 0.2622906565666199\n",
      "Epoch 25: train loss: 0.4011007249355316\n",
      "Epoch 25: train loss: 0.4224817454814911\n",
      "Epoch 25: train loss: 0.15900348126888275\n",
      "Epoch 25: train loss: 0.4423798620700836\n",
      "Epoch 25: train loss: 0.29487699270248413\n",
      "Epoch 25: train loss: 0.23721344769001007\n",
      "Epoch 25: train loss: 0.27654358744621277\n",
      "Epoch 25: train loss: 0.6300864815711975\n",
      "Epoch 25: train loss: 0.3618810772895813\n",
      "Epoch 25: train loss: 0.4893299639225006\n",
      "Epoch 25: train loss: 0.27506062388420105\n",
      "Epoch 25: train loss: 0.38657093048095703\n",
      "Epoch 25: train loss: 0.4495411515235901\n",
      "Epoch 25: train loss: 0.32433614134788513\n",
      "Epoch 25: train loss: 0.473175048828125\n",
      "Epoch 25: train loss: 0.2992110550403595\n",
      "Epoch 25: train loss: 0.3435889780521393\n",
      "Epoch 25: train loss: 0.43691486120224\n",
      "Epoch 25: train loss: 0.2737719416618347\n",
      "Epoch 25: train loss: 0.2692223787307739\n",
      "Epoch 25: train loss: 0.40263834595680237\n",
      "Epoch 25: train loss: 0.36334577202796936\n",
      "Epoch 25: train loss: 0.4251636564731598\n",
      "Epoch 25: train loss: 0.3793012499809265\n",
      "Epoch 25: train loss: 0.36363714933395386\n",
      "Epoch 25: train loss: 0.6215364933013916\n",
      "Epoch 25: train loss: 0.5862561464309692\n",
      "Epoch 25: train loss: 0.30265963077545166\n",
      "Epoch 25: train loss: 0.30455881357192993\n",
      "Epoch 25: train loss: 0.3889329135417938\n",
      "Epoch 25: train loss: 0.07054705172777176\n",
      "Epoch 26: train loss: 0.52236008644104\n",
      "Epoch 26: train loss: 0.28046780824661255\n",
      "Epoch 26: train loss: 0.4429396688938141\n",
      "Epoch 26: train loss: 0.25648483633995056\n",
      "Epoch 26: train loss: 0.22500279545783997\n",
      "Epoch 26: train loss: 0.2985055148601532\n",
      "Epoch 26: train loss: 0.34916776418685913\n",
      "Epoch 26: train loss: 0.2890491783618927\n",
      "Epoch 26: train loss: 0.39685139060020447\n",
      "Epoch 26: train loss: 0.5192108154296875\n",
      "Epoch 26: train loss: 0.5366420745849609\n",
      "Epoch 26: train loss: 0.27755871415138245\n",
      "Epoch 26: train loss: 0.4164672791957855\n",
      "Epoch 26: train loss: 0.2788805663585663\n",
      "Epoch 26: train loss: 0.30840492248535156\n",
      "Epoch 26: train loss: 0.43983346223831177\n",
      "Epoch 26: train loss: 0.4259132444858551\n",
      "Epoch 26: train loss: 0.35646602511405945\n",
      "Epoch 26: train loss: 0.40671974420547485\n",
      "Epoch 26: train loss: 0.3486602008342743\n",
      "Epoch 26: train loss: 0.5161875486373901\n",
      "Epoch 26: train loss: 0.37335291504859924\n",
      "Epoch 26: train loss: 0.3222622573375702\n",
      "Epoch 26: train loss: 0.32504159212112427\n",
      "Epoch 26: train loss: 0.29650038480758667\n",
      "Epoch 26: train loss: 0.24078676104545593\n",
      "Epoch 26: train loss: 0.4398961365222931\n",
      "Epoch 26: train loss: 0.3368106782436371\n",
      "Epoch 26: train loss: 0.3126744329929352\n",
      "Epoch 26: train loss: 0.3059858977794647\n",
      "Epoch 26: train loss: 0.3885129988193512\n",
      "Epoch 26: train loss: 0.2623721659183502\n",
      "Epoch 26: train loss: 0.318457692861557\n",
      "Epoch 26: train loss: 0.5183124542236328\n",
      "Epoch 26: train loss: 0.21862809360027313\n",
      "Epoch 26: train loss: 0.27832794189453125\n",
      "Epoch 26: train loss: 0.22642101347446442\n",
      "Epoch 26: train loss: 0.6092416048049927\n",
      "Epoch 26: train loss: 0.4438934624195099\n",
      "Epoch 26: train loss: 0.4038170576095581\n",
      "Epoch 26: train loss: 0.25538477301597595\n",
      "Epoch 26: train loss: 0.42382508516311646\n",
      "Epoch 26: train loss: 0.48466941714286804\n",
      "Epoch 26: train loss: 0.3573385179042816\n",
      "Epoch 26: train loss: 0.38257718086242676\n",
      "Epoch 26: train loss: 0.5029016137123108\n",
      "Epoch 26: train loss: 0.3728427290916443\n",
      "Epoch 26: train loss: 0.46994689106941223\n",
      "Epoch 26: train loss: 0.3752360939979553\n",
      "Epoch 26: train loss: 0.31668752431869507\n",
      "Epoch 26: train loss: 0.20751571655273438\n",
      "Epoch 26: train loss: 0.19345764815807343\n",
      "Epoch 26: train loss: 0.22920724749565125\n",
      "Epoch 26: train loss: 0.2410174161195755\n",
      "Epoch 26: train loss: 0.6069537401199341\n",
      "Epoch 26: train loss: 0.1366313099861145\n",
      "Epoch 26: train loss: 0.21829023957252502\n",
      "Epoch 26: train loss: 0.48391127586364746\n",
      "Epoch 26: train loss: 0.3209969103336334\n",
      "Epoch 26: train loss: 0.2679736912250519\n",
      "Epoch 26: train loss: 0.6244849562644958\n",
      "Epoch 26: train loss: 0.3211217224597931\n",
      "Epoch 26: train loss: 0.28295838832855225\n",
      "Epoch 26: train loss: 0.41245386004447937\n",
      "Epoch 26: train loss: 0.3726685643196106\n",
      "Epoch 26: train loss: 0.41083481907844543\n",
      "Epoch 26: train loss: 0.3260810375213623\n",
      "Epoch 26: train loss: 0.24794751405715942\n",
      "Epoch 26: train loss: 0.31295570731163025\n",
      "Epoch 26: train loss: 0.3633016347885132\n",
      "Epoch 26: train loss: 0.43651995062828064\n",
      "Epoch 26: train loss: 0.31937962770462036\n",
      "Epoch 26: train loss: 0.3021174371242523\n",
      "Epoch 26: train loss: 0.3681672513484955\n",
      "Epoch 26: train loss: 0.3604404330253601\n",
      "Epoch 26: train loss: 0.3129444718360901\n",
      "Epoch 26: train loss: 0.460539847612381\n",
      "Epoch 26: train loss: 0.08165042847394943\n",
      "Epoch 27: train loss: 0.5225716829299927\n",
      "Epoch 27: train loss: 0.39465761184692383\n",
      "Epoch 27: train loss: 0.3796244263648987\n",
      "Epoch 27: train loss: 0.2822731137275696\n",
      "Epoch 27: train loss: 0.26694899797439575\n",
      "Epoch 27: train loss: 0.2350444346666336\n",
      "Epoch 27: train loss: 0.32560136914253235\n",
      "Epoch 27: train loss: 0.26597169041633606\n",
      "Epoch 27: train loss: 0.2058486044406891\n",
      "Epoch 27: train loss: 0.2822993993759155\n",
      "Epoch 27: train loss: 0.2729380428791046\n",
      "Epoch 27: train loss: 0.4115578532218933\n",
      "Epoch 27: train loss: 0.28514719009399414\n",
      "Epoch 27: train loss: 0.26567742228507996\n",
      "Epoch 27: train loss: 0.4296127259731293\n",
      "Epoch 27: train loss: 0.39561164379119873\n",
      "Epoch 27: train loss: 0.5313008427619934\n",
      "Epoch 27: train loss: 0.3759935200214386\n",
      "Epoch 27: train loss: 0.3344483971595764\n",
      "Epoch 27: train loss: 0.34580403566360474\n",
      "Epoch 27: train loss: 0.3385990858078003\n",
      "Epoch 27: train loss: 0.22944752871990204\n",
      "Epoch 27: train loss: 0.6076744198799133\n",
      "Epoch 27: train loss: 0.519149899482727\n",
      "Epoch 27: train loss: 0.4305889308452606\n",
      "Epoch 27: train loss: 0.44470009207725525\n",
      "Epoch 27: train loss: 0.3941647410392761\n",
      "Epoch 27: train loss: 0.3926891088485718\n",
      "Epoch 27: train loss: 0.4212542772293091\n",
      "Epoch 27: train loss: 0.4760923981666565\n",
      "Epoch 27: train loss: 0.27488234639167786\n",
      "Epoch 27: train loss: 0.48128971457481384\n",
      "Epoch 27: train loss: 0.4393833577632904\n",
      "Epoch 27: train loss: 0.36381691694259644\n",
      "Epoch 27: train loss: 0.3675936460494995\n",
      "Epoch 27: train loss: 0.4165616035461426\n",
      "Epoch 27: train loss: 0.29327523708343506\n",
      "Epoch 27: train loss: 0.17698341608047485\n",
      "Epoch 27: train loss: 0.2485751509666443\n",
      "Epoch 27: train loss: 0.3604782521724701\n",
      "Epoch 27: train loss: 0.23940975964069366\n",
      "Epoch 27: train loss: 0.2011118233203888\n",
      "Epoch 27: train loss: 0.36070194840431213\n",
      "Epoch 27: train loss: 0.4029013514518738\n",
      "Epoch 27: train loss: 0.4194944202899933\n",
      "Epoch 27: train loss: 0.3306275010108948\n",
      "Epoch 27: train loss: 0.32936835289001465\n",
      "Epoch 27: train loss: 0.2751685380935669\n",
      "Epoch 27: train loss: 0.550987958908081\n",
      "Epoch 27: train loss: 0.3201698958873749\n",
      "Epoch 27: train loss: 0.5455412864685059\n",
      "Epoch 27: train loss: 0.2650633454322815\n",
      "Epoch 27: train loss: 0.18321417272090912\n",
      "Epoch 27: train loss: 0.2641896605491638\n",
      "Epoch 27: train loss: 0.49917280673980713\n",
      "Epoch 27: train loss: 0.31899964809417725\n",
      "Epoch 27: train loss: 0.2919815182685852\n",
      "Epoch 27: train loss: 0.29435500502586365\n",
      "Epoch 27: train loss: 0.5404626131057739\n",
      "Epoch 27: train loss: 0.32838958501815796\n",
      "Epoch 27: train loss: 0.22806844115257263\n",
      "Epoch 27: train loss: 0.3012921214103699\n",
      "Epoch 27: train loss: 0.2846629023551941\n",
      "Epoch 27: train loss: 0.2488931566476822\n",
      "Epoch 27: train loss: 0.34489384293556213\n",
      "Epoch 27: train loss: 0.5204212069511414\n",
      "Epoch 27: train loss: 0.4798482060432434\n",
      "Epoch 27: train loss: 0.4022596776485443\n",
      "Epoch 27: train loss: 0.35742732882499695\n",
      "Epoch 27: train loss: 0.302862286567688\n",
      "Epoch 27: train loss: 0.41299307346343994\n",
      "Epoch 27: train loss: 0.31083840131759644\n",
      "Epoch 27: train loss: 0.431881308555603\n",
      "Epoch 27: train loss: 0.272060364484787\n",
      "Epoch 27: train loss: 0.3654009699821472\n",
      "Epoch 27: train loss: 0.34110066294670105\n",
      "Epoch 27: train loss: 0.3824038803577423\n",
      "Epoch 27: train loss: 0.048363614827394485\n",
      "Epoch 28: train loss: 0.4198416471481323\n",
      "Epoch 28: train loss: 0.2526760995388031\n",
      "Epoch 28: train loss: 0.2750369906425476\n",
      "Epoch 28: train loss: 0.2512655258178711\n",
      "Epoch 28: train loss: 0.694365382194519\n",
      "Epoch 28: train loss: 0.4341602325439453\n",
      "Epoch 28: train loss: 0.4828540086746216\n",
      "Epoch 28: train loss: 0.4863201081752777\n",
      "Epoch 28: train loss: 0.3424166440963745\n",
      "Epoch 28: train loss: 0.15170979499816895\n",
      "Epoch 28: train loss: 0.41835853457450867\n",
      "Epoch 28: train loss: 0.4461326003074646\n",
      "Epoch 28: train loss: 0.3282741904258728\n",
      "Epoch 28: train loss: 0.40689218044281006\n",
      "Epoch 28: train loss: 0.2683088779449463\n",
      "Epoch 28: train loss: 0.47323277592658997\n",
      "Epoch 28: train loss: 0.4494549632072449\n",
      "Epoch 28: train loss: 0.4110608398914337\n",
      "Epoch 28: train loss: 0.363025963306427\n",
      "Epoch 28: train loss: 0.3235449194908142\n",
      "Epoch 28: train loss: 0.2319382280111313\n",
      "Epoch 28: train loss: 0.3940856456756592\n",
      "Epoch 28: train loss: 0.4714500904083252\n",
      "Epoch 28: train loss: 0.28237876296043396\n",
      "Epoch 28: train loss: 0.3127081096172333\n",
      "Epoch 28: train loss: 0.3243770897388458\n",
      "Epoch 28: train loss: 0.3937287926673889\n",
      "Epoch 28: train loss: 0.25674673914909363\n",
      "Epoch 28: train loss: 0.5089985132217407\n",
      "Epoch 28: train loss: 0.3920060694217682\n",
      "Epoch 28: train loss: 0.3361377716064453\n",
      "Epoch 28: train loss: 0.24963076412677765\n",
      "Epoch 28: train loss: 0.4436750113964081\n",
      "Epoch 28: train loss: 0.3010549545288086\n",
      "Epoch 28: train loss: 0.17054390907287598\n",
      "Epoch 28: train loss: 0.3927556574344635\n",
      "Epoch 28: train loss: 0.2911822199821472\n",
      "Epoch 28: train loss: 0.2303587645292282\n",
      "Epoch 28: train loss: 0.4082840085029602\n",
      "Epoch 28: train loss: 0.21918679773807526\n",
      "Epoch 28: train loss: 0.3884955048561096\n",
      "Epoch 28: train loss: 0.4817519187927246\n",
      "Epoch 28: train loss: 0.24263174831867218\n",
      "Epoch 28: train loss: 0.3027816712856293\n",
      "Epoch 28: train loss: 0.24206668138504028\n",
      "Epoch 28: train loss: 0.3929893970489502\n",
      "Epoch 28: train loss: 0.22992755472660065\n",
      "Epoch 28: train loss: 0.39718499779701233\n",
      "Epoch 28: train loss: 0.2755277454853058\n",
      "Epoch 28: train loss: 0.30962440371513367\n",
      "Epoch 28: train loss: 0.3486243188381195\n",
      "Epoch 28: train loss: 0.5100185871124268\n",
      "Epoch 28: train loss: 0.317701518535614\n",
      "Epoch 28: train loss: 0.24715851247310638\n",
      "Epoch 28: train loss: 0.1667458415031433\n",
      "Epoch 28: train loss: 0.4296654760837555\n",
      "Epoch 28: train loss: 0.38045549392700195\n",
      "Epoch 28: train loss: 0.27775150537490845\n",
      "Epoch 28: train loss: 0.4387436807155609\n",
      "Epoch 28: train loss: 0.22503668069839478\n",
      "Epoch 28: train loss: 0.3440178632736206\n",
      "Epoch 28: train loss: 0.3665510416030884\n",
      "Epoch 28: train loss: 0.24221646785736084\n",
      "Epoch 28: train loss: 0.5674594640731812\n",
      "Epoch 28: train loss: 0.3103981018066406\n",
      "Epoch 28: train loss: 0.35859256982803345\n",
      "Epoch 28: train loss: 0.20958083868026733\n",
      "Epoch 28: train loss: 0.31762099266052246\n",
      "Epoch 28: train loss: 0.42440465092658997\n",
      "Epoch 28: train loss: 0.283719003200531\n",
      "Epoch 28: train loss: 0.602878749370575\n",
      "Epoch 28: train loss: 0.5627855658531189\n",
      "Epoch 28: train loss: 0.2802404761314392\n",
      "Epoch 28: train loss: 0.3916921019554138\n",
      "Epoch 28: train loss: 0.315581351518631\n",
      "Epoch 28: train loss: 0.39886415004730225\n",
      "Epoch 28: train loss: 0.4151573181152344\n",
      "Epoch 28: train loss: 0.7277237772941589\n",
      "Epoch 29: train loss: 0.4340679347515106\n",
      "Epoch 29: train loss: 0.2962014079093933\n",
      "Epoch 29: train loss: 0.4631071388721466\n",
      "Epoch 29: train loss: 0.3178684115409851\n",
      "Epoch 29: train loss: 0.3761502206325531\n",
      "Epoch 29: train loss: 0.27030497789382935\n",
      "Epoch 29: train loss: 0.3009611666202545\n",
      "Epoch 29: train loss: 0.29955047369003296\n",
      "Epoch 29: train loss: 0.3156779706478119\n",
      "Epoch 29: train loss: 0.3337685465812683\n",
      "Epoch 29: train loss: 0.36806413531303406\n",
      "Epoch 29: train loss: 0.351068913936615\n",
      "Epoch 29: train loss: 0.33031636476516724\n",
      "Epoch 29: train loss: 0.3581767678260803\n",
      "Epoch 29: train loss: 0.40212705731391907\n",
      "Epoch 29: train loss: 0.31414592266082764\n",
      "Epoch 29: train loss: 0.30844900012016296\n",
      "Epoch 29: train loss: 0.3548128604888916\n",
      "Epoch 29: train loss: 0.48274415731430054\n",
      "Epoch 29: train loss: 0.4694521427154541\n",
      "Epoch 29: train loss: 0.4944288432598114\n",
      "Epoch 29: train loss: 0.380336731672287\n",
      "Epoch 29: train loss: 0.3414446711540222\n",
      "Epoch 29: train loss: 0.3492158353328705\n",
      "Epoch 29: train loss: 0.5529184937477112\n",
      "Epoch 29: train loss: 0.3291621506214142\n",
      "Epoch 29: train loss: 0.3848843276500702\n",
      "Epoch 29: train loss: 0.2959866225719452\n",
      "Epoch 29: train loss: 0.38089519739151\n",
      "Epoch 29: train loss: 0.30478307604789734\n",
      "Epoch 29: train loss: 0.4998161792755127\n",
      "Epoch 29: train loss: 0.41040298342704773\n",
      "Epoch 29: train loss: 0.29728952050209045\n",
      "Epoch 29: train loss: 0.37340521812438965\n",
      "Epoch 29: train loss: 0.38129982352256775\n",
      "Epoch 29: train loss: 0.2462301403284073\n",
      "Epoch 29: train loss: 0.16310474276542664\n",
      "Epoch 29: train loss: 0.30755987763404846\n",
      "Epoch 29: train loss: 0.3393687903881073\n",
      "Epoch 29: train loss: 0.3881406784057617\n",
      "Epoch 29: train loss: 0.4167202413082123\n",
      "Epoch 29: train loss: 0.4299234449863434\n",
      "Epoch 29: train loss: 0.2867541015148163\n",
      "Epoch 29: train loss: 0.5273953676223755\n",
      "Epoch 29: train loss: 0.397367000579834\n",
      "Epoch 29: train loss: 0.4006306231021881\n",
      "Epoch 29: train loss: 0.3044748604297638\n",
      "Epoch 29: train loss: 0.22183628380298615\n",
      "Epoch 29: train loss: 0.4941668212413788\n",
      "Epoch 29: train loss: 0.49232256412506104\n",
      "Epoch 29: train loss: 0.23839005827903748\n",
      "Epoch 29: train loss: 0.27664801478385925\n",
      "Epoch 29: train loss: 0.3668254017829895\n",
      "Epoch 29: train loss: 0.2330164760351181\n",
      "Epoch 29: train loss: 0.49384361505508423\n",
      "Epoch 29: train loss: 0.38594383001327515\n",
      "Epoch 29: train loss: 0.5002515912055969\n",
      "Epoch 29: train loss: 0.5169535279273987\n",
      "Epoch 29: train loss: 0.4702565371990204\n",
      "Epoch 29: train loss: 0.44569331407546997\n",
      "Epoch 29: train loss: 0.2342175543308258\n",
      "Epoch 29: train loss: 0.42475706338882446\n",
      "Epoch 29: train loss: 0.26367759704589844\n",
      "Epoch 29: train loss: 0.3474763035774231\n",
      "Epoch 29: train loss: 0.23636119067668915\n",
      "Epoch 29: train loss: 0.5012467503547668\n",
      "Epoch 29: train loss: 0.43937888741493225\n",
      "Epoch 29: train loss: 0.3129796087741852\n",
      "Epoch 29: train loss: 0.39668089151382446\n",
      "Epoch 29: train loss: 0.21423950791358948\n",
      "Epoch 29: train loss: 0.26275163888931274\n",
      "Epoch 29: train loss: 0.27513861656188965\n",
      "Epoch 29: train loss: 0.26453015208244324\n",
      "Epoch 29: train loss: 0.35815325379371643\n",
      "Epoch 29: train loss: 0.47997695207595825\n",
      "Epoch 29: train loss: 0.21308711171150208\n",
      "Epoch 29: train loss: 0.47206544876098633\n",
      "Epoch 29: train loss: 0.2377602756023407\n",
      "Epoch 30: train loss: 0.24689064919948578\n",
      "Epoch 30: train loss: 0.43212467432022095\n",
      "Epoch 30: train loss: 0.2957538068294525\n",
      "Epoch 30: train loss: 0.4718056917190552\n",
      "Epoch 30: train loss: 0.33118221163749695\n",
      "Epoch 30: train loss: 0.4222395718097687\n",
      "Epoch 30: train loss: 0.22886203229427338\n",
      "Epoch 30: train loss: 0.4194929599761963\n",
      "Epoch 30: train loss: 0.3476870059967041\n",
      "Epoch 30: train loss: 0.39325106143951416\n",
      "Epoch 30: train loss: 0.3323390483856201\n",
      "Epoch 30: train loss: 0.4644008278846741\n",
      "Epoch 30: train loss: 0.30264684557914734\n",
      "Epoch 30: train loss: 0.4792685806751251\n",
      "Epoch 30: train loss: 0.47216328978538513\n",
      "Epoch 30: train loss: 0.38624298572540283\n",
      "Epoch 30: train loss: 0.3072093725204468\n",
      "Epoch 30: train loss: 0.39873623847961426\n",
      "Epoch 30: train loss: 0.33869364857673645\n",
      "Epoch 30: train loss: 0.39024364948272705\n",
      "Epoch 30: train loss: 0.5414652824401855\n",
      "Epoch 30: train loss: 0.29152554273605347\n",
      "Epoch 30: train loss: 0.2801274359226227\n",
      "Epoch 30: train loss: 0.3135601282119751\n",
      "Epoch 30: train loss: 0.2365969866514206\n",
      "Epoch 30: train loss: 0.35114794969558716\n",
      "Epoch 30: train loss: 0.36255818605422974\n",
      "Epoch 30: train loss: 0.41459736227989197\n",
      "Epoch 30: train loss: 0.3350681960582733\n",
      "Epoch 30: train loss: 0.25942105054855347\n",
      "Epoch 30: train loss: 0.5121555328369141\n",
      "Epoch 30: train loss: 0.20336611568927765\n",
      "Epoch 30: train loss: 0.401763916015625\n",
      "Epoch 30: train loss: 0.3490760624408722\n",
      "Epoch 30: train loss: 0.4482426345348358\n",
      "Epoch 30: train loss: 0.4084468483924866\n",
      "Epoch 30: train loss: 0.2778230309486389\n",
      "Epoch 30: train loss: 0.48020946979522705\n",
      "Epoch 30: train loss: 0.2621249258518219\n",
      "Epoch 30: train loss: 0.28040140867233276\n",
      "Epoch 30: train loss: 0.3070968687534332\n",
      "Epoch 30: train loss: 0.4166678190231323\n",
      "Epoch 30: train loss: 0.4008517563343048\n",
      "Epoch 30: train loss: 0.35008546710014343\n",
      "Epoch 30: train loss: 0.35966575145721436\n",
      "Epoch 30: train loss: 0.3885401487350464\n",
      "Epoch 30: train loss: 0.5222188234329224\n",
      "Epoch 30: train loss: 0.49080848693847656\n",
      "Epoch 30: train loss: 0.31954506039619446\n",
      "Epoch 30: train loss: 0.21261470019817352\n",
      "Epoch 30: train loss: 0.2806825041770935\n",
      "Epoch 30: train loss: 0.381653368473053\n",
      "Epoch 30: train loss: 0.35681888461112976\n",
      "Epoch 30: train loss: 0.2969646453857422\n",
      "Epoch 30: train loss: 0.2548247277736664\n",
      "Epoch 30: train loss: 0.3312869668006897\n",
      "Epoch 30: train loss: 0.3572898805141449\n",
      "Epoch 30: train loss: 0.19526350498199463\n",
      "Epoch 30: train loss: 0.27346667647361755\n",
      "Epoch 30: train loss: 0.39125481247901917\n",
      "Epoch 30: train loss: 0.2756963074207306\n",
      "Epoch 30: train loss: 0.24862141907215118\n",
      "Epoch 30: train loss: 0.12795521318912506\n",
      "Epoch 30: train loss: 0.2554169297218323\n",
      "Epoch 30: train loss: 0.5366944670677185\n",
      "Epoch 30: train loss: 0.35720294713974\n",
      "Epoch 30: train loss: 0.44574835896492004\n",
      "Epoch 30: train loss: 0.3736925721168518\n",
      "Epoch 30: train loss: 0.31719696521759033\n",
      "Epoch 30: train loss: 0.30821168422698975\n",
      "Epoch 30: train loss: 0.2640168070793152\n",
      "Epoch 30: train loss: 0.3469061851501465\n",
      "Epoch 30: train loss: 0.3543626070022583\n",
      "Epoch 30: train loss: 0.46553850173950195\n",
      "Epoch 30: train loss: 0.4079176187515259\n",
      "Epoch 30: train loss: 0.40593937039375305\n",
      "Epoch 30: train loss: 0.35394519567489624\n",
      "Epoch 30: train loss: 0.23222137987613678\n",
      "Epoch 31: train loss: 0.35489988327026367\n",
      "Epoch 31: train loss: 0.27421239018440247\n",
      "Epoch 31: train loss: 0.3800058662891388\n",
      "Epoch 31: train loss: 0.31149548292160034\n",
      "Epoch 31: train loss: 0.2763724625110626\n",
      "Epoch 31: train loss: 0.2988368570804596\n",
      "Epoch 31: train loss: 0.47829577326774597\n",
      "Epoch 31: train loss: 0.4021663963794708\n",
      "Epoch 31: train loss: 0.37575221061706543\n",
      "Epoch 31: train loss: 0.43892228603363037\n",
      "Epoch 31: train loss: 0.31225624680519104\n",
      "Epoch 31: train loss: 0.289422869682312\n",
      "Epoch 31: train loss: 0.33635446429252625\n",
      "Epoch 31: train loss: 0.35712379217147827\n",
      "Epoch 31: train loss: 0.22396567463874817\n",
      "Epoch 31: train loss: 0.30642229318618774\n",
      "Epoch 31: train loss: 0.37662652134895325\n",
      "Epoch 31: train loss: 0.5358163714408875\n",
      "Epoch 31: train loss: 0.36103612184524536\n",
      "Epoch 31: train loss: 0.2591722309589386\n",
      "Epoch 31: train loss: 0.3989851772785187\n",
      "Epoch 31: train loss: 0.4671541452407837\n",
      "Epoch 31: train loss: 0.48530101776123047\n",
      "Epoch 31: train loss: 0.6216742992401123\n",
      "Epoch 31: train loss: 0.31750109791755676\n",
      "Epoch 31: train loss: 0.28581300377845764\n",
      "Epoch 31: train loss: 0.28062862157821655\n",
      "Epoch 31: train loss: 0.3635813593864441\n",
      "Epoch 31: train loss: 0.29508212208747864\n",
      "Epoch 31: train loss: 0.33307647705078125\n",
      "Epoch 31: train loss: 0.275272935628891\n",
      "Epoch 31: train loss: 0.26579785346984863\n",
      "Epoch 31: train loss: 0.3201001286506653\n",
      "Epoch 31: train loss: 0.47912847995758057\n",
      "Epoch 31: train loss: 0.3113259971141815\n",
      "Epoch 31: train loss: 0.4169929027557373\n",
      "Epoch 31: train loss: 0.32433217763900757\n",
      "Epoch 31: train loss: 0.2964576482772827\n",
      "Epoch 31: train loss: 0.406160444021225\n",
      "Epoch 31: train loss: 0.4449715316295624\n",
      "Epoch 31: train loss: 0.2937019169330597\n",
      "Epoch 31: train loss: 0.4731186032295227\n",
      "Epoch 31: train loss: 0.3186066448688507\n",
      "Epoch 31: train loss: 0.3509480953216553\n",
      "Epoch 31: train loss: 0.4608192443847656\n",
      "Epoch 31: train loss: 0.4238549470901489\n",
      "Epoch 31: train loss: 0.33724305033683777\n",
      "Epoch 31: train loss: 0.28228819370269775\n",
      "Epoch 31: train loss: 0.33868134021759033\n",
      "Epoch 31: train loss: 0.1727449744939804\n",
      "Epoch 31: train loss: 0.20583929121494293\n",
      "Epoch 31: train loss: 0.3186338543891907\n",
      "Epoch 31: train loss: 0.3420846164226532\n",
      "Epoch 31: train loss: 0.39183327555656433\n",
      "Epoch 31: train loss: 0.3398345410823822\n",
      "Epoch 31: train loss: 0.19868765771389008\n",
      "Epoch 31: train loss: 0.4226187467575073\n",
      "Epoch 31: train loss: 0.3012954592704773\n",
      "Epoch 31: train loss: 0.3839128613471985\n",
      "Epoch 31: train loss: 0.38683515787124634\n",
      "Epoch 31: train loss: 0.4576132595539093\n",
      "Epoch 31: train loss: 0.235429048538208\n",
      "Epoch 31: train loss: 0.37349236011505127\n",
      "Epoch 31: train loss: 0.44205108284950256\n",
      "Epoch 31: train loss: 0.17812171578407288\n",
      "Epoch 31: train loss: 0.4436114728450775\n",
      "Epoch 31: train loss: 0.3561141788959503\n",
      "Epoch 31: train loss: 0.48078209161758423\n",
      "Epoch 31: train loss: 0.4264853894710541\n",
      "Epoch 31: train loss: 0.36939719319343567\n",
      "Epoch 31: train loss: 0.36190298199653625\n",
      "Epoch 31: train loss: 0.36659976840019226\n",
      "Epoch 31: train loss: 0.41125255823135376\n",
      "Epoch 31: train loss: 0.3814820647239685\n",
      "Epoch 31: train loss: 0.3361521065235138\n",
      "Epoch 31: train loss: 0.3208268880844116\n",
      "Epoch 31: train loss: 0.3953589200973511\n",
      "Epoch 31: train loss: 0.5224283933639526\n",
      "Epoch 32: train loss: 0.5007225871086121\n",
      "Epoch 32: train loss: 0.2837013304233551\n",
      "Epoch 32: train loss: 0.2829086184501648\n",
      "Epoch 32: train loss: 0.3000342547893524\n",
      "Epoch 32: train loss: 0.3933250904083252\n",
      "Epoch 32: train loss: 0.534548819065094\n",
      "Epoch 32: train loss: 0.3206453323364258\n",
      "Epoch 32: train loss: 0.43818750977516174\n",
      "Epoch 32: train loss: 0.33641839027404785\n",
      "Epoch 32: train loss: 0.39713647961616516\n",
      "Epoch 32: train loss: 0.5056600570678711\n",
      "Epoch 32: train loss: 0.3004378378391266\n",
      "Epoch 32: train loss: 0.44585737586021423\n",
      "Epoch 32: train loss: 0.45952436327934265\n",
      "Epoch 32: train loss: 0.48258692026138306\n",
      "Epoch 32: train loss: 0.37533751130104065\n",
      "Epoch 32: train loss: 0.28519511222839355\n",
      "Epoch 32: train loss: 0.19918574392795563\n",
      "Epoch 32: train loss: 0.37856876850128174\n",
      "Epoch 32: train loss: 0.34896618127822876\n",
      "Epoch 32: train loss: 0.5359405875205994\n",
      "Epoch 32: train loss: 0.39728260040283203\n",
      "Epoch 32: train loss: 0.4503105580806732\n",
      "Epoch 32: train loss: 0.3883307874202728\n",
      "Epoch 32: train loss: 0.2227700650691986\n",
      "Epoch 32: train loss: 0.29195675253868103\n",
      "Epoch 32: train loss: 0.335547536611557\n",
      "Epoch 32: train loss: 0.15740542113780975\n",
      "Epoch 32: train loss: 0.1379375010728836\n",
      "Epoch 32: train loss: 0.24898338317871094\n",
      "Epoch 32: train loss: 0.5888046622276306\n",
      "Epoch 32: train loss: 0.6036666035652161\n",
      "Epoch 32: train loss: 0.4762628674507141\n",
      "Epoch 32: train loss: 0.6511437296867371\n",
      "Epoch 32: train loss: 0.20327097177505493\n",
      "Epoch 32: train loss: 0.27517884969711304\n",
      "Epoch 32: train loss: 0.2552374601364136\n",
      "Epoch 32: train loss: 0.23556044697761536\n",
      "Epoch 32: train loss: 0.4156782031059265\n",
      "Epoch 32: train loss: 0.29200562834739685\n",
      "Epoch 32: train loss: 0.3435631990432739\n",
      "Epoch 32: train loss: 0.2870722711086273\n",
      "Epoch 32: train loss: 0.3588718771934509\n",
      "Epoch 32: train loss: 0.4308735430240631\n",
      "Epoch 32: train loss: 0.43741393089294434\n",
      "Epoch 32: train loss: 0.2806190550327301\n",
      "Epoch 32: train loss: 0.34032753109931946\n",
      "Epoch 32: train loss: 0.4132845401763916\n",
      "Epoch 32: train loss: 0.4241633117198944\n",
      "Epoch 32: train loss: 0.29628005623817444\n",
      "Epoch 32: train loss: 0.33202049136161804\n",
      "Epoch 32: train loss: 0.48351046442985535\n",
      "Epoch 32: train loss: 0.31794559955596924\n",
      "Epoch 32: train loss: 0.28956329822540283\n",
      "Epoch 32: train loss: 0.35261815786361694\n",
      "Epoch 32: train loss: 0.2201565057039261\n",
      "Epoch 32: train loss: 0.44582399725914\n",
      "Epoch 32: train loss: 0.4016406238079071\n",
      "Epoch 32: train loss: 0.43388381600379944\n",
      "Epoch 32: train loss: 0.3870140314102173\n",
      "Epoch 32: train loss: 0.49241626262664795\n",
      "Epoch 32: train loss: 0.29709476232528687\n",
      "Epoch 32: train loss: 0.30046531558036804\n",
      "Epoch 32: train loss: 0.3012075126171112\n",
      "Epoch 32: train loss: 0.36431342363357544\n",
      "Epoch 32: train loss: 0.3197110593318939\n",
      "Epoch 32: train loss: 0.25583481788635254\n",
      "Epoch 32: train loss: 0.19557636976242065\n",
      "Epoch 32: train loss: 0.3120546340942383\n",
      "Epoch 32: train loss: 0.36874353885650635\n",
      "Epoch 32: train loss: 0.18344078958034515\n",
      "Epoch 32: train loss: 0.2822129726409912\n",
      "Epoch 32: train loss: 0.2951071262359619\n",
      "Epoch 32: train loss: 0.2562848627567291\n",
      "Epoch 32: train loss: 0.2148580253124237\n",
      "Epoch 32: train loss: 0.6635912656784058\n",
      "Epoch 32: train loss: 0.31987059116363525\n",
      "Epoch 32: train loss: 0.17155753076076508\n",
      "Epoch 33: train loss: 0.2885821759700775\n",
      "Epoch 33: train loss: 0.36565396189689636\n",
      "Epoch 33: train loss: 0.28362277150154114\n",
      "Epoch 33: train loss: 0.36698293685913086\n",
      "Epoch 33: train loss: 0.4932090938091278\n",
      "Epoch 33: train loss: 0.3112833797931671\n",
      "Epoch 33: train loss: 0.43645012378692627\n",
      "Epoch 33: train loss: 0.3717489540576935\n",
      "Epoch 33: train loss: 0.3803066909313202\n",
      "Epoch 33: train loss: 0.4101506769657135\n",
      "Epoch 33: train loss: 0.528590738773346\n",
      "Epoch 33: train loss: 0.40399888157844543\n",
      "Epoch 33: train loss: 0.2564854621887207\n",
      "Epoch 33: train loss: 0.3135456144809723\n",
      "Epoch 33: train loss: 0.31575027108192444\n",
      "Epoch 33: train loss: 0.3034885823726654\n",
      "Epoch 33: train loss: 0.2849505543708801\n",
      "Epoch 33: train loss: 0.3244622051715851\n",
      "Epoch 33: train loss: 0.38239800930023193\n",
      "Epoch 33: train loss: 0.45962852239608765\n",
      "Epoch 33: train loss: 0.4258476793766022\n",
      "Epoch 33: train loss: 0.3065508306026459\n",
      "Epoch 33: train loss: 0.4056272804737091\n",
      "Epoch 33: train loss: 0.3876679241657257\n",
      "Epoch 33: train loss: 0.33304619789123535\n",
      "Epoch 33: train loss: 0.35731154680252075\n",
      "Epoch 33: train loss: 0.4525553584098816\n",
      "Epoch 33: train loss: 0.28688299655914307\n",
      "Epoch 33: train loss: 0.2951386570930481\n",
      "Epoch 33: train loss: 0.3232654929161072\n",
      "Epoch 33: train loss: 0.40621504187583923\n",
      "Epoch 33: train loss: 0.25031182169914246\n",
      "Epoch 33: train loss: 0.40729251503944397\n",
      "Epoch 33: train loss: 0.43508777022361755\n",
      "Epoch 33: train loss: 0.3951305150985718\n",
      "Epoch 33: train loss: 0.36062121391296387\n",
      "Epoch 33: train loss: 0.487056702375412\n",
      "Epoch 33: train loss: 0.37007609009742737\n",
      "Epoch 33: train loss: 0.4969412386417389\n",
      "Epoch 33: train loss: 0.2938879430294037\n",
      "Epoch 33: train loss: 0.36765265464782715\n",
      "Epoch 33: train loss: 0.35988831520080566\n",
      "Epoch 33: train loss: 0.34633001685142517\n",
      "Epoch 33: train loss: 0.2967166304588318\n",
      "Epoch 33: train loss: 0.39070969820022583\n",
      "Epoch 33: train loss: 0.37505701184272766\n",
      "Epoch 33: train loss: 0.3193797469139099\n",
      "Epoch 33: train loss: 0.3643024265766144\n",
      "Epoch 33: train loss: 0.225844606757164\n",
      "Epoch 33: train loss: 0.36119434237480164\n",
      "Epoch 33: train loss: 0.4174686074256897\n",
      "Epoch 33: train loss: 0.23518681526184082\n",
      "Epoch 33: train loss: 0.4517398476600647\n",
      "Epoch 33: train loss: 0.37045547366142273\n",
      "Epoch 33: train loss: 0.5258157253265381\n",
      "Epoch 33: train loss: 0.17256630957126617\n",
      "Epoch 33: train loss: 0.40946733951568604\n",
      "Epoch 33: train loss: 0.3001755475997925\n",
      "Epoch 33: train loss: 0.2792946398258209\n",
      "Epoch 33: train loss: 0.2451387643814087\n",
      "Epoch 33: train loss: 0.16797107458114624\n",
      "Epoch 33: train loss: 0.4548797011375427\n",
      "Epoch 33: train loss: 0.3438376188278198\n",
      "Epoch 33: train loss: 0.14224949479103088\n",
      "Epoch 33: train loss: 0.4004698395729065\n",
      "Epoch 33: train loss: 0.4704541563987732\n",
      "Epoch 33: train loss: 0.2837982475757599\n",
      "Epoch 33: train loss: 0.3452267646789551\n",
      "Epoch 33: train loss: 0.28949546813964844\n",
      "Epoch 33: train loss: 0.6178808808326721\n",
      "Epoch 33: train loss: 0.3301302194595337\n",
      "Epoch 33: train loss: 0.3221229612827301\n",
      "Epoch 33: train loss: 0.2081010639667511\n",
      "Epoch 33: train loss: 0.30541983246803284\n",
      "Epoch 33: train loss: 0.29120492935180664\n",
      "Epoch 33: train loss: 0.43835365772247314\n",
      "Epoch 33: train loss: 0.34772202372550964\n",
      "Epoch 33: train loss: 0.20512434840202332\n",
      "Epoch 34: train loss: 0.2747132182121277\n",
      "Epoch 34: train loss: 0.38494715094566345\n",
      "Epoch 34: train loss: 0.45143401622772217\n",
      "Epoch 34: train loss: 0.26620617508888245\n",
      "Epoch 34: train loss: 0.3137775957584381\n",
      "Epoch 34: train loss: 0.27295276522636414\n",
      "Epoch 34: train loss: 0.413873553276062\n",
      "Epoch 34: train loss: 0.4059208929538727\n",
      "Epoch 34: train loss: 0.3320162892341614\n",
      "Epoch 34: train loss: 0.46888089179992676\n",
      "Epoch 34: train loss: 0.33856895565986633\n",
      "Epoch 34: train loss: 0.40912896394729614\n",
      "Epoch 34: train loss: 0.44035103917121887\n",
      "Epoch 34: train loss: 0.3104991614818573\n",
      "Epoch 34: train loss: 0.32064175605773926\n",
      "Epoch 34: train loss: 0.2716764211654663\n",
      "Epoch 34: train loss: 0.3146486282348633\n",
      "Epoch 34: train loss: 0.4355681538581848\n",
      "Epoch 34: train loss: 0.3962732255458832\n",
      "Epoch 34: train loss: 0.4400603473186493\n",
      "Epoch 34: train loss: 0.37826311588287354\n",
      "Epoch 34: train loss: 0.29797884821891785\n",
      "Epoch 34: train loss: 0.4081374406814575\n",
      "Epoch 34: train loss: 0.5129753947257996\n",
      "Epoch 34: train loss: 0.28189149498939514\n",
      "Epoch 34: train loss: 0.3907068371772766\n",
      "Epoch 34: train loss: 0.33432134985923767\n",
      "Epoch 34: train loss: 0.22705328464508057\n",
      "Epoch 34: train loss: 0.2959253489971161\n",
      "Epoch 34: train loss: 0.1763320416212082\n",
      "Epoch 34: train loss: 0.5117027759552002\n",
      "Epoch 34: train loss: 0.5399495959281921\n",
      "Epoch 34: train loss: 0.3651748299598694\n",
      "Epoch 34: train loss: 0.33297866582870483\n",
      "Epoch 34: train loss: 0.23813003301620483\n",
      "Epoch 34: train loss: 0.5376788973808289\n",
      "Epoch 34: train loss: 0.3108508586883545\n",
      "Epoch 34: train loss: 0.48578688502311707\n",
      "Epoch 34: train loss: 0.25587043166160583\n",
      "Epoch 34: train loss: 0.2647397518157959\n",
      "Epoch 34: train loss: 0.42572808265686035\n",
      "Epoch 34: train loss: 0.41063278913497925\n",
      "Epoch 34: train loss: 0.38845932483673096\n",
      "Epoch 34: train loss: 0.24075599014759064\n",
      "Epoch 34: train loss: 0.24518965184688568\n",
      "Epoch 34: train loss: 0.35687321424484253\n",
      "Epoch 34: train loss: 0.39745956659317017\n",
      "Epoch 34: train loss: 0.29745832085609436\n",
      "Epoch 34: train loss: 0.5118935108184814\n",
      "Epoch 34: train loss: 0.3908615708351135\n",
      "Epoch 34: train loss: 0.2774806618690491\n",
      "Epoch 34: train loss: 0.2663257420063019\n",
      "Epoch 34: train loss: 0.5488623380661011\n",
      "Epoch 34: train loss: 0.3489333987236023\n",
      "Epoch 34: train loss: 0.26127955317497253\n",
      "Epoch 34: train loss: 0.31141361594200134\n",
      "Epoch 34: train loss: 0.2774215042591095\n",
      "Epoch 34: train loss: 0.31620070338249207\n",
      "Epoch 34: train loss: 0.46319714188575745\n",
      "Epoch 34: train loss: 0.39864423871040344\n",
      "Epoch 34: train loss: 0.3446592390537262\n",
      "Epoch 34: train loss: 0.23830974102020264\n",
      "Epoch 34: train loss: 0.28091296553611755\n",
      "Epoch 34: train loss: 0.2727981507778168\n",
      "Epoch 34: train loss: 0.17829740047454834\n",
      "Epoch 34: train loss: 0.34743884205818176\n",
      "Epoch 34: train loss: 0.43621379137039185\n",
      "Epoch 34: train loss: 0.13876043260097504\n",
      "Epoch 34: train loss: 0.28286871314048767\n",
      "Epoch 34: train loss: 0.3136327564716339\n",
      "Epoch 34: train loss: 0.4536343216896057\n",
      "Epoch 34: train loss: 0.48431918025016785\n",
      "Epoch 34: train loss: 0.5305365920066833\n",
      "Epoch 34: train loss: 0.4292658567428589\n",
      "Epoch 34: train loss: 0.27812737226486206\n",
      "Epoch 34: train loss: 0.3850610852241516\n",
      "Epoch 34: train loss: 0.27938878536224365\n",
      "Epoch 34: train loss: 0.578335165977478\n",
      "Epoch 35: train loss: 0.3451022505760193\n",
      "Epoch 35: train loss: 0.3360952138900757\n",
      "Epoch 35: train loss: 0.21086019277572632\n",
      "Epoch 35: train loss: 0.45921462774276733\n",
      "Epoch 35: train loss: 0.3774127960205078\n",
      "Epoch 35: train loss: 0.363677442073822\n",
      "Epoch 35: train loss: 0.5709179043769836\n",
      "Epoch 35: train loss: 0.34514206647872925\n",
      "Epoch 35: train loss: 0.36149927973747253\n",
      "Epoch 35: train loss: 0.2904953062534332\n",
      "Epoch 35: train loss: 0.43349769711494446\n",
      "Epoch 35: train loss: 0.1405899077653885\n",
      "Epoch 35: train loss: 0.6491578221321106\n",
      "Epoch 35: train loss: 0.3074093461036682\n",
      "Epoch 35: train loss: 0.3486541509628296\n",
      "Epoch 35: train loss: 0.35097068548202515\n",
      "Epoch 35: train loss: 0.2774669826030731\n",
      "Epoch 35: train loss: 0.2666283845901489\n",
      "Epoch 35: train loss: 0.6181995272636414\n",
      "Epoch 35: train loss: 0.3163106143474579\n",
      "Epoch 35: train loss: 0.14317919313907623\n",
      "Epoch 35: train loss: 0.3526003360748291\n",
      "Epoch 35: train loss: 0.35795357823371887\n",
      "Epoch 35: train loss: 0.42018967866897583\n",
      "Epoch 35: train loss: 0.39584869146347046\n",
      "Epoch 35: train loss: 0.32582083344459534\n",
      "Epoch 35: train loss: 0.2904339134693146\n",
      "Epoch 35: train loss: 0.3689640164375305\n",
      "Epoch 35: train loss: 0.4838191866874695\n",
      "Epoch 35: train loss: 0.3583712577819824\n",
      "Epoch 35: train loss: 0.2915983498096466\n",
      "Epoch 35: train loss: 0.36496680974960327\n",
      "Epoch 35: train loss: 0.49097326397895813\n",
      "Epoch 35: train loss: 0.316720187664032\n",
      "Epoch 35: train loss: 0.34767159819602966\n",
      "Epoch 35: train loss: 0.27756622433662415\n",
      "Epoch 35: train loss: 0.3749566674232483\n",
      "Epoch 35: train loss: 0.33921414613723755\n",
      "Epoch 35: train loss: 0.402582049369812\n",
      "Epoch 35: train loss: 0.3510168790817261\n",
      "Epoch 35: train loss: 0.26994460821151733\n",
      "Epoch 35: train loss: 0.33441361784935\n",
      "Epoch 35: train loss: 0.26292577385902405\n",
      "Epoch 35: train loss: 0.27353328466415405\n",
      "Epoch 35: train loss: 0.39103636145591736\n",
      "Epoch 35: train loss: 0.30565837025642395\n",
      "Epoch 35: train loss: 0.45902571082115173\n",
      "Epoch 35: train loss: 0.44226735830307007\n",
      "Epoch 35: train loss: 0.5066209435462952\n",
      "Epoch 35: train loss: 0.38686904311180115\n",
      "Epoch 35: train loss: 0.5316686630249023\n",
      "Epoch 35: train loss: 0.3264334201812744\n",
      "Epoch 35: train loss: 0.4347538948059082\n",
      "Epoch 35: train loss: 0.4008103907108307\n",
      "Epoch 35: train loss: 0.3057129979133606\n",
      "Epoch 35: train loss: 0.30950117111206055\n",
      "Epoch 35: train loss: 0.3977876901626587\n",
      "Epoch 35: train loss: 0.27993547916412354\n",
      "Epoch 35: train loss: 0.34926360845565796\n",
      "Epoch 35: train loss: 0.3282732665538788\n",
      "Epoch 35: train loss: 0.3757311701774597\n",
      "Epoch 35: train loss: 0.35153311491012573\n",
      "Epoch 35: train loss: 0.33519446849823\n",
      "Epoch 35: train loss: 0.24813659489154816\n",
      "Epoch 35: train loss: 0.3974393606185913\n",
      "Epoch 35: train loss: 0.3913319408893585\n",
      "Epoch 35: train loss: 0.3484441339969635\n",
      "Epoch 35: train loss: 0.3918997347354889\n",
      "Epoch 35: train loss: 0.3716222941875458\n",
      "Epoch 35: train loss: 0.2799699902534485\n",
      "Epoch 35: train loss: 0.246003657579422\n",
      "Epoch 35: train loss: 0.25577595829963684\n",
      "Epoch 35: train loss: 0.29501667618751526\n",
      "Epoch 35: train loss: 0.14353018999099731\n",
      "Epoch 35: train loss: 0.3288581967353821\n",
      "Epoch 35: train loss: 0.5093604922294617\n",
      "Epoch 35: train loss: 0.41302821040153503\n",
      "Epoch 35: train loss: 0.16711759567260742\n",
      "Epoch 36: train loss: 0.3180381655693054\n",
      "Epoch 36: train loss: 0.5084483623504639\n",
      "Epoch 36: train loss: 0.5062400698661804\n",
      "Epoch 36: train loss: 0.4265924394130707\n",
      "Epoch 36: train loss: 0.4959159195423126\n",
      "Epoch 36: train loss: 0.21872730553150177\n",
      "Epoch 36: train loss: 0.2743924856185913\n",
      "Epoch 36: train loss: 0.3324297368526459\n",
      "Epoch 36: train loss: 0.31540200114250183\n",
      "Epoch 36: train loss: 0.34861379861831665\n",
      "Epoch 36: train loss: 0.42907238006591797\n",
      "Epoch 36: train loss: 0.2724640667438507\n",
      "Epoch 36: train loss: 0.2879618704319\n",
      "Epoch 36: train loss: 0.4345698952674866\n",
      "Epoch 36: train loss: 0.23776905238628387\n",
      "Epoch 36: train loss: 0.4498872756958008\n",
      "Epoch 36: train loss: 0.2684924602508545\n",
      "Epoch 36: train loss: 0.4978231191635132\n",
      "Epoch 36: train loss: 0.34755855798721313\n",
      "Epoch 36: train loss: 0.32316359877586365\n",
      "Epoch 36: train loss: 0.5576017498970032\n",
      "Epoch 36: train loss: 0.23921751976013184\n",
      "Epoch 36: train loss: 0.30079978704452515\n",
      "Epoch 36: train loss: 0.38301876187324524\n",
      "Epoch 36: train loss: 0.25380411744117737\n",
      "Epoch 36: train loss: 0.5408082008361816\n",
      "Epoch 36: train loss: 0.4842394292354584\n",
      "Epoch 36: train loss: 0.38931703567504883\n",
      "Epoch 36: train loss: 0.25290465354919434\n",
      "Epoch 36: train loss: 0.43644845485687256\n",
      "Epoch 36: train loss: 0.34252989292144775\n",
      "Epoch 36: train loss: 0.467794269323349\n",
      "Epoch 36: train loss: 0.3030625581741333\n",
      "Epoch 36: train loss: 0.3005468249320984\n",
      "Epoch 36: train loss: 0.2918734550476074\n",
      "Epoch 36: train loss: 0.24381068348884583\n",
      "Epoch 36: train loss: 0.37019893527030945\n",
      "Epoch 36: train loss: 0.35532012581825256\n",
      "Epoch 36: train loss: 0.48576998710632324\n",
      "Epoch 36: train loss: 0.2178308069705963\n",
      "Epoch 36: train loss: 0.23207978904247284\n",
      "Epoch 36: train loss: 0.16315998136997223\n",
      "Epoch 36: train loss: 0.3760761618614197\n",
      "Epoch 36: train loss: 0.5852267742156982\n",
      "Epoch 36: train loss: 0.39970672130584717\n",
      "Epoch 36: train loss: 0.3702542185783386\n",
      "Epoch 36: train loss: 0.22089049220085144\n",
      "Epoch 36: train loss: 0.3619363307952881\n",
      "Epoch 36: train loss: 0.4811098277568817\n",
      "Epoch 36: train loss: 0.22696226835250854\n",
      "Epoch 36: train loss: 0.2790793180465698\n",
      "Epoch 36: train loss: 0.40507587790489197\n",
      "Epoch 36: train loss: 0.36731454730033875\n",
      "Epoch 36: train loss: 0.3699067533016205\n",
      "Epoch 36: train loss: 0.26069924235343933\n",
      "Epoch 36: train loss: 0.39182931184768677\n",
      "Epoch 36: train loss: 0.16974884271621704\n",
      "Epoch 36: train loss: 0.36068224906921387\n",
      "Epoch 36: train loss: 0.3455892503261566\n",
      "Epoch 36: train loss: 0.5295754671096802\n",
      "Epoch 36: train loss: 0.2829500436782837\n",
      "Epoch 36: train loss: 0.4194176495075226\n",
      "Epoch 36: train loss: 0.3608236014842987\n",
      "Epoch 36: train loss: 0.3765292763710022\n",
      "Epoch 36: train loss: 0.2856740653514862\n",
      "Epoch 36: train loss: 0.3404671549797058\n",
      "Epoch 36: train loss: 0.4070720970630646\n",
      "Epoch 36: train loss: 0.38348037004470825\n",
      "Epoch 36: train loss: 0.35302239656448364\n",
      "Epoch 36: train loss: 0.1627359241247177\n",
      "Epoch 36: train loss: 0.43599626421928406\n",
      "Epoch 36: train loss: 0.3233703374862671\n",
      "Epoch 36: train loss: 0.2665620446205139\n",
      "Epoch 36: train loss: 0.33235546946525574\n",
      "Epoch 36: train loss: 0.26043209433555603\n",
      "Epoch 36: train loss: 0.3981042206287384\n",
      "Epoch 36: train loss: 0.37818217277526855\n",
      "Epoch 36: train loss: 0.1663956642150879\n",
      "Epoch 37: train loss: 0.43814679980278015\n",
      "Epoch 37: train loss: 0.40889933705329895\n",
      "Epoch 37: train loss: 0.37728941440582275\n",
      "Epoch 37: train loss: 0.38506171107292175\n",
      "Epoch 37: train loss: 0.4071686267852783\n",
      "Epoch 37: train loss: 0.3879151940345764\n",
      "Epoch 37: train loss: 0.24232105910778046\n",
      "Epoch 37: train loss: 0.4094076156616211\n",
      "Epoch 37: train loss: 0.2822689414024353\n",
      "Epoch 37: train loss: 0.28929978609085083\n",
      "Epoch 37: train loss: 0.2857773005962372\n",
      "Epoch 37: train loss: 0.42711156606674194\n",
      "Epoch 37: train loss: 0.41904333233833313\n",
      "Epoch 37: train loss: 0.5680699944496155\n",
      "Epoch 37: train loss: 0.2569979727268219\n",
      "Epoch 37: train loss: 0.18829886615276337\n",
      "Epoch 37: train loss: 0.40954533219337463\n",
      "Epoch 37: train loss: 0.3205842077732086\n",
      "Epoch 37: train loss: 0.31519174575805664\n",
      "Epoch 37: train loss: 0.28272125124931335\n",
      "Epoch 37: train loss: 0.2713189721107483\n",
      "Epoch 37: train loss: 0.27127575874328613\n",
      "Epoch 37: train loss: 0.35040420293807983\n",
      "Epoch 37: train loss: 0.3334324359893799\n",
      "Epoch 37: train loss: 0.5941612720489502\n",
      "Epoch 37: train loss: 0.4478950500488281\n",
      "Epoch 37: train loss: 0.28781619668006897\n",
      "Epoch 37: train loss: 0.3275912404060364\n",
      "Epoch 37: train loss: 0.29652613401412964\n",
      "Epoch 37: train loss: 0.4360826909542084\n",
      "Epoch 37: train loss: 0.3434247076511383\n",
      "Epoch 37: train loss: 0.291368305683136\n",
      "Epoch 37: train loss: 0.23471783101558685\n",
      "Epoch 37: train loss: 0.36394691467285156\n",
      "Epoch 37: train loss: 0.2452315241098404\n",
      "Epoch 37: train loss: 0.2532707750797272\n",
      "Epoch 37: train loss: 0.36459946632385254\n",
      "Epoch 37: train loss: 0.38827452063560486\n",
      "Epoch 37: train loss: 0.3804076611995697\n",
      "Epoch 37: train loss: 0.2954736649990082\n",
      "Epoch 37: train loss: 0.2720716893672943\n",
      "Epoch 37: train loss: 0.38078397512435913\n",
      "Epoch 37: train loss: 0.3934406340122223\n",
      "Epoch 37: train loss: 0.25761857628822327\n",
      "Epoch 37: train loss: 0.38853883743286133\n",
      "Epoch 37: train loss: 0.35335275530815125\n",
      "Epoch 37: train loss: 0.33074110746383667\n",
      "Epoch 37: train loss: 0.5573416352272034\n",
      "Epoch 37: train loss: 0.435054212808609\n",
      "Epoch 37: train loss: 0.16543300449848175\n",
      "Epoch 37: train loss: 0.24229662120342255\n",
      "Epoch 37: train loss: 0.4779956340789795\n",
      "Epoch 37: train loss: 0.2299979329109192\n",
      "Epoch 37: train loss: 0.34122079610824585\n",
      "Epoch 37: train loss: 0.32912275195121765\n",
      "Epoch 37: train loss: 0.3600800633430481\n",
      "Epoch 37: train loss: 0.3209993541240692\n",
      "Epoch 37: train loss: 0.3111051023006439\n",
      "Epoch 37: train loss: 0.33060380816459656\n",
      "Epoch 37: train loss: 0.37692704796791077\n",
      "Epoch 37: train loss: 0.42906802892684937\n",
      "Epoch 37: train loss: 0.5663961172103882\n",
      "Epoch 37: train loss: 0.5214825868606567\n",
      "Epoch 37: train loss: 0.31906023621559143\n",
      "Epoch 37: train loss: 0.3924705684185028\n",
      "Epoch 37: train loss: 0.3220696747303009\n",
      "Epoch 37: train loss: 0.34039318561553955\n",
      "Epoch 37: train loss: 0.20358943939208984\n",
      "Epoch 37: train loss: 0.35191068053245544\n",
      "Epoch 37: train loss: 0.2294437736272812\n",
      "Epoch 37: train loss: 0.6242804527282715\n",
      "Epoch 37: train loss: 0.4512629806995392\n",
      "Epoch 37: train loss: 0.24706032872200012\n",
      "Epoch 37: train loss: 0.5267288088798523\n",
      "Epoch 37: train loss: 0.5188918709754944\n",
      "Epoch 37: train loss: 0.27054283022880554\n",
      "Epoch 37: train loss: 0.38201406598091125\n",
      "Epoch 37: train loss: 0.595177173614502\n",
      "Epoch 38: train loss: 0.3469626307487488\n",
      "Epoch 38: train loss: 0.36748215556144714\n",
      "Epoch 38: train loss: 0.3075367510318756\n",
      "Epoch 38: train loss: 0.5677282214164734\n",
      "Epoch 38: train loss: 0.34082329273223877\n",
      "Epoch 38: train loss: 0.39059576392173767\n",
      "Epoch 38: train loss: 0.48754575848579407\n",
      "Epoch 38: train loss: 0.37816765904426575\n",
      "Epoch 38: train loss: 0.281327486038208\n",
      "Epoch 38: train loss: 0.4674219489097595\n",
      "Epoch 38: train loss: 0.31438004970550537\n",
      "Epoch 38: train loss: 0.31718364357948303\n",
      "Epoch 38: train loss: 0.31478095054626465\n",
      "Epoch 38: train loss: 0.35563674569129944\n",
      "Epoch 38: train loss: 0.2772561013698578\n",
      "Epoch 38: train loss: 0.5615936517715454\n",
      "Epoch 38: train loss: 0.35126641392707825\n",
      "Epoch 38: train loss: 0.4562216103076935\n",
      "Epoch 38: train loss: 0.2353294938802719\n",
      "Epoch 38: train loss: 0.33150678873062134\n",
      "Epoch 38: train loss: 0.4090324342250824\n",
      "Epoch 38: train loss: 0.32913491129875183\n",
      "Epoch 38: train loss: 0.26894474029541016\n",
      "Epoch 38: train loss: 0.5444557070732117\n",
      "Epoch 38: train loss: 0.1627243161201477\n",
      "Epoch 38: train loss: 0.24767185747623444\n",
      "Epoch 38: train loss: 0.2852386236190796\n",
      "Epoch 38: train loss: 0.36070504784584045\n",
      "Epoch 38: train loss: 0.2404412031173706\n",
      "Epoch 38: train loss: 0.43583574891090393\n",
      "Epoch 38: train loss: 0.2932346761226654\n",
      "Epoch 38: train loss: 0.47952476143836975\n",
      "Epoch 38: train loss: 0.2934802174568176\n",
      "Epoch 38: train loss: 0.3312857449054718\n",
      "Epoch 38: train loss: 0.2547013759613037\n",
      "Epoch 38: train loss: 0.31133055686950684\n",
      "Epoch 38: train loss: 0.441268652677536\n",
      "Epoch 38: train loss: 0.3418237566947937\n",
      "Epoch 38: train loss: 0.2805864214897156\n",
      "Epoch 38: train loss: 0.5692004561424255\n",
      "Epoch 38: train loss: 0.336468368768692\n",
      "Epoch 38: train loss: 0.36439085006713867\n",
      "Epoch 38: train loss: 0.3926834762096405\n",
      "Epoch 38: train loss: 0.4290967881679535\n",
      "Epoch 38: train loss: 0.415962278842926\n",
      "Epoch 38: train loss: 0.21431864798069\n",
      "Epoch 38: train loss: 0.25563913583755493\n",
      "Epoch 38: train loss: 0.2579837143421173\n",
      "Epoch 38: train loss: 0.1512719690799713\n",
      "Epoch 38: train loss: 0.3089735805988312\n",
      "Epoch 38: train loss: 0.3182845711708069\n",
      "Epoch 38: train loss: 0.5440555810928345\n",
      "Epoch 38: train loss: 0.4227938950061798\n",
      "Epoch 38: train loss: 0.49241334199905396\n",
      "Epoch 38: train loss: 0.3332616686820984\n",
      "Epoch 38: train loss: 0.4657573401927948\n",
      "Epoch 38: train loss: 0.2519298493862152\n",
      "Epoch 38: train loss: 0.36143988370895386\n",
      "Epoch 38: train loss: 0.2882695198059082\n",
      "Epoch 38: train loss: 0.33753663301467896\n",
      "Epoch 38: train loss: 0.5365301966667175\n",
      "Epoch 38: train loss: 0.3930390179157257\n",
      "Epoch 38: train loss: 0.2561350166797638\n",
      "Epoch 38: train loss: 0.42381253838539124\n",
      "Epoch 38: train loss: 0.33901336789131165\n",
      "Epoch 38: train loss: 0.3891236484050751\n",
      "Epoch 38: train loss: 0.33178964257240295\n",
      "Epoch 38: train loss: 0.2978932559490204\n",
      "Epoch 38: train loss: 0.41551291942596436\n",
      "Epoch 38: train loss: 0.38505083322525024\n",
      "Epoch 38: train loss: 0.2608456611633301\n",
      "Epoch 38: train loss: 0.2477790117263794\n",
      "Epoch 38: train loss: 0.35440874099731445\n",
      "Epoch 38: train loss: 0.31825709342956543\n",
      "Epoch 38: train loss: 0.46782204508781433\n",
      "Epoch 38: train loss: 0.31222957372665405\n",
      "Epoch 38: train loss: 0.3366851806640625\n",
      "Epoch 38: train loss: 0.2728815972805023\n",
      "Epoch 39: train loss: 0.25586333870887756\n",
      "Epoch 39: train loss: 0.45768168568611145\n",
      "Epoch 39: train loss: 0.43678009510040283\n",
      "Epoch 39: train loss: 0.3336040675640106\n",
      "Epoch 39: train loss: 0.41873428225517273\n",
      "Epoch 39: train loss: 0.43982797861099243\n",
      "Epoch 39: train loss: 0.5235379338264465\n",
      "Epoch 39: train loss: 0.32632380723953247\n",
      "Epoch 39: train loss: 0.21387232840061188\n",
      "Epoch 39: train loss: 0.5689541101455688\n",
      "Epoch 39: train loss: 0.3541412949562073\n",
      "Epoch 39: train loss: 0.3330526649951935\n",
      "Epoch 39: train loss: 0.3585412800312042\n",
      "Epoch 39: train loss: 0.41008996963500977\n",
      "Epoch 39: train loss: 0.46551913022994995\n",
      "Epoch 39: train loss: 0.3716987669467926\n",
      "Epoch 39: train loss: 0.19770453870296478\n",
      "Epoch 39: train loss: 0.2636767327785492\n",
      "Epoch 39: train loss: 0.33620908856391907\n",
      "Epoch 39: train loss: 0.23650012910366058\n",
      "Epoch 39: train loss: 0.4882749915122986\n",
      "Epoch 39: train loss: 0.2641860544681549\n",
      "Epoch 39: train loss: 0.4980946481227875\n",
      "Epoch 39: train loss: 0.4498177766799927\n",
      "Epoch 39: train loss: 0.17119954526424408\n",
      "Epoch 39: train loss: 0.4760793149471283\n",
      "Epoch 39: train loss: 0.2747592031955719\n",
      "Epoch 39: train loss: 0.332332581281662\n",
      "Epoch 39: train loss: 0.31584134697914124\n",
      "Epoch 39: train loss: 0.7030678391456604\n",
      "Epoch 39: train loss: 0.2984670400619507\n",
      "Epoch 39: train loss: 0.41706809401512146\n",
      "Epoch 39: train loss: 0.36628416180610657\n",
      "Epoch 39: train loss: 0.44087278842926025\n",
      "Epoch 39: train loss: 0.36713141202926636\n",
      "Epoch 39: train loss: 0.25493693351745605\n",
      "Epoch 39: train loss: 0.34076666831970215\n",
      "Epoch 39: train loss: 0.2289591282606125\n",
      "Epoch 39: train loss: 0.4175075888633728\n",
      "Epoch 39: train loss: 0.33368176221847534\n",
      "Epoch 39: train loss: 0.2494480311870575\n",
      "Epoch 39: train loss: 0.36860305070877075\n",
      "Epoch 39: train loss: 0.31068938970565796\n",
      "Epoch 39: train loss: 0.47211989760398865\n",
      "Epoch 39: train loss: 0.3556400537490845\n",
      "Epoch 39: train loss: 0.21366941928863525\n",
      "Epoch 39: train loss: 0.2201133370399475\n",
      "Epoch 39: train loss: 0.5784164667129517\n",
      "Epoch 39: train loss: 0.36909160017967224\n",
      "Epoch 39: train loss: 0.24930240213871002\n",
      "Epoch 39: train loss: 0.25785988569259644\n",
      "Epoch 39: train loss: 0.39359140396118164\n",
      "Epoch 39: train loss: 0.4406777620315552\n",
      "Epoch 39: train loss: 0.3273448050022125\n",
      "Epoch 39: train loss: 0.30930307507514954\n",
      "Epoch 39: train loss: 0.41039684414863586\n",
      "Epoch 39: train loss: 0.35710829496383667\n",
      "Epoch 39: train loss: 0.3405715525150299\n",
      "Epoch 39: train loss: 0.35671311616897583\n",
      "Epoch 39: train loss: 0.3776696026325226\n",
      "Epoch 39: train loss: 0.3665688633918762\n",
      "Epoch 39: train loss: 0.44216153025627136\n",
      "Epoch 39: train loss: 0.3491646647453308\n",
      "Epoch 39: train loss: 0.2659263610839844\n",
      "Epoch 39: train loss: 0.3025621771812439\n",
      "Epoch 39: train loss: 0.33905860781669617\n",
      "Epoch 39: train loss: 0.20381256937980652\n",
      "Epoch 39: train loss: 0.39062973856925964\n",
      "Epoch 39: train loss: 0.39880964159965515\n",
      "Epoch 39: train loss: 0.3255758285522461\n",
      "Epoch 39: train loss: 0.21888044476509094\n",
      "Epoch 39: train loss: 0.44505202770233154\n",
      "Epoch 39: train loss: 0.33962273597717285\n",
      "Epoch 39: train loss: 0.32863160967826843\n",
      "Epoch 39: train loss: 0.38317784667015076\n",
      "Epoch 39: train loss: 0.49148109555244446\n",
      "Epoch 39: train loss: 0.35413897037506104\n",
      "Epoch 39: train loss: 0.2625579237937927\n",
      "Epoch 40: train loss: 0.1920827478170395\n",
      "Epoch 40: train loss: 0.2665077745914459\n",
      "Epoch 40: train loss: 0.14988389611244202\n",
      "Epoch 40: train loss: 0.1763206422328949\n",
      "Epoch 40: train loss: 0.41896435618400574\n",
      "Epoch 40: train loss: 0.4693112373352051\n",
      "Epoch 40: train loss: 0.5510712265968323\n",
      "Epoch 40: train loss: 0.2770927846431732\n",
      "Epoch 40: train loss: 0.2790972590446472\n",
      "Epoch 40: train loss: 0.35572823882102966\n",
      "Epoch 40: train loss: 0.23127219080924988\n",
      "Epoch 40: train loss: 0.2375219166278839\n",
      "Epoch 40: train loss: 0.16056199371814728\n",
      "Epoch 40: train loss: 0.3764571249485016\n",
      "Epoch 40: train loss: 0.1955210268497467\n",
      "Epoch 40: train loss: 0.6131682395935059\n",
      "Epoch 40: train loss: 0.5012884736061096\n",
      "Epoch 40: train loss: 0.3501216471195221\n",
      "Epoch 40: train loss: 0.4967065751552582\n",
      "Epoch 40: train loss: 0.2351521998643875\n",
      "Epoch 40: train loss: 0.2524922788143158\n",
      "Epoch 40: train loss: 0.4888424873352051\n",
      "Epoch 40: train loss: 0.4512558579444885\n",
      "Epoch 40: train loss: 0.32758286595344543\n",
      "Epoch 40: train loss: 0.269521564245224\n",
      "Epoch 40: train loss: 0.36794087290763855\n",
      "Epoch 40: train loss: 0.35976552963256836\n",
      "Epoch 40: train loss: 0.2886829078197479\n",
      "Epoch 40: train loss: 0.31956782937049866\n",
      "Epoch 40: train loss: 0.40846240520477295\n",
      "Epoch 40: train loss: 0.4485311508178711\n",
      "Epoch 40: train loss: 0.4788331687450409\n",
      "Epoch 40: train loss: 0.49839550256729126\n",
      "Epoch 40: train loss: 0.41562512516975403\n",
      "Epoch 40: train loss: 0.4483228921890259\n",
      "Epoch 40: train loss: 0.34585490822792053\n",
      "Epoch 40: train loss: 0.3971056640148163\n",
      "Epoch 40: train loss: 0.2355712652206421\n",
      "Epoch 40: train loss: 0.337265282869339\n",
      "Epoch 40: train loss: 0.3612312078475952\n",
      "Epoch 40: train loss: 0.4292351007461548\n",
      "Epoch 40: train loss: 0.4248723089694977\n",
      "Epoch 40: train loss: 0.39369136095046997\n",
      "Epoch 40: train loss: 0.3732317090034485\n",
      "Epoch 40: train loss: 0.36038723587989807\n",
      "Epoch 40: train loss: 0.43648219108581543\n",
      "Epoch 40: train loss: 0.26263508200645447\n",
      "Epoch 40: train loss: 0.4263482689857483\n",
      "Epoch 40: train loss: 0.2063954770565033\n",
      "Epoch 40: train loss: 0.2752078175544739\n",
      "Epoch 40: train loss: 0.46608299016952515\n",
      "Epoch 40: train loss: 0.37421879172325134\n",
      "Epoch 40: train loss: 0.33870089054107666\n",
      "Epoch 40: train loss: 0.284245103597641\n",
      "Epoch 40: train loss: 0.6076993346214294\n",
      "Epoch 40: train loss: 0.5496608018875122\n",
      "Epoch 40: train loss: 0.2641598880290985\n",
      "Epoch 40: train loss: 0.19273366034030914\n",
      "Epoch 40: train loss: 0.28558796644210815\n",
      "Epoch 40: train loss: 0.4705162048339844\n",
      "Epoch 40: train loss: 0.3639424741268158\n",
      "Epoch 40: train loss: 0.2782847583293915\n",
      "Epoch 40: train loss: 0.38523221015930176\n",
      "Epoch 40: train loss: 0.30610889196395874\n",
      "Epoch 40: train loss: 0.3493669927120209\n",
      "Epoch 40: train loss: 0.288348913192749\n",
      "Epoch 40: train loss: 0.26620981097221375\n",
      "Epoch 40: train loss: 0.2803550958633423\n",
      "Epoch 40: train loss: 0.3163241147994995\n",
      "Epoch 40: train loss: 0.28412696719169617\n",
      "Epoch 40: train loss: 0.4899854063987732\n",
      "Epoch 40: train loss: 0.3969581723213196\n",
      "Epoch 40: train loss: 0.3580356538295746\n",
      "Epoch 40: train loss: 0.2926904559135437\n",
      "Epoch 40: train loss: 0.4711180031299591\n",
      "Epoch 40: train loss: 0.34613946080207825\n",
      "Epoch 40: train loss: 0.3428334593772888\n",
      "Epoch 40: train loss: 0.6011263132095337\n",
      "Epoch 41: train loss: 0.4198288023471832\n",
      "Epoch 41: train loss: 0.31062087416648865\n",
      "Epoch 41: train loss: 0.38363325595855713\n",
      "Epoch 41: train loss: 0.35652148723602295\n",
      "Epoch 41: train loss: 0.3460635542869568\n",
      "Epoch 41: train loss: 0.4661265015602112\n",
      "Epoch 41: train loss: 0.44811493158340454\n",
      "Epoch 41: train loss: 0.33724960684776306\n",
      "Epoch 41: train loss: 0.40945956110954285\n",
      "Epoch 41: train loss: 0.24474690854549408\n",
      "Epoch 41: train loss: 0.3456818461418152\n",
      "Epoch 41: train loss: 0.22327964007854462\n",
      "Epoch 41: train loss: 0.3079339265823364\n",
      "Epoch 41: train loss: 0.4304012656211853\n",
      "Epoch 41: train loss: 0.3194615840911865\n",
      "Epoch 41: train loss: 0.3891836702823639\n",
      "Epoch 41: train loss: 0.398950457572937\n",
      "Epoch 41: train loss: 0.3806379437446594\n",
      "Epoch 41: train loss: 0.6184688210487366\n",
      "Epoch 41: train loss: 0.21324369311332703\n",
      "Epoch 41: train loss: 0.39440250396728516\n",
      "Epoch 41: train loss: 0.43198075890541077\n",
      "Epoch 41: train loss: 0.39826470613479614\n",
      "Epoch 41: train loss: 0.3563006520271301\n",
      "Epoch 41: train loss: 0.5952096581459045\n",
      "Epoch 41: train loss: 0.35687577724456787\n",
      "Epoch 41: train loss: 0.34019169211387634\n",
      "Epoch 41: train loss: 0.24342003464698792\n",
      "Epoch 41: train loss: 0.35568374395370483\n",
      "Epoch 41: train loss: 0.47070229053497314\n",
      "Epoch 41: train loss: 0.43504616618156433\n",
      "Epoch 41: train loss: 0.3095906376838684\n",
      "Epoch 41: train loss: 0.44359585642814636\n",
      "Epoch 41: train loss: 0.41196656227111816\n",
      "Epoch 41: train loss: 0.40746235847473145\n",
      "Epoch 41: train loss: 0.40633606910705566\n",
      "Epoch 41: train loss: 0.359969824552536\n",
      "Epoch 41: train loss: 0.2551455795764923\n",
      "Epoch 41: train loss: 0.30907928943634033\n",
      "Epoch 41: train loss: 0.4751386344432831\n",
      "Epoch 41: train loss: 0.25139808654785156\n",
      "Epoch 41: train loss: 0.5137094855308533\n",
      "Epoch 41: train loss: 0.44844603538513184\n",
      "Epoch 41: train loss: 0.3128693103790283\n",
      "Epoch 41: train loss: 0.33559560775756836\n",
      "Epoch 41: train loss: 0.3567540645599365\n",
      "Epoch 41: train loss: 0.21496190130710602\n",
      "Epoch 41: train loss: 0.26474013924598694\n",
      "Epoch 41: train loss: 0.4305616617202759\n",
      "Epoch 41: train loss: 0.37522822618484497\n",
      "Epoch 41: train loss: 0.4184224009513855\n",
      "Epoch 41: train loss: 0.36832159757614136\n",
      "Epoch 41: train loss: 0.23931542038917542\n",
      "Epoch 41: train loss: 0.41662508249282837\n",
      "Epoch 41: train loss: 0.25272104144096375\n",
      "Epoch 41: train loss: 0.4818286597728729\n",
      "Epoch 41: train loss: 0.3003668189048767\n",
      "Epoch 41: train loss: 0.2671605050563812\n",
      "Epoch 41: train loss: 0.45625460147857666\n",
      "Epoch 41: train loss: 0.3649800419807434\n",
      "Epoch 41: train loss: 0.359163761138916\n",
      "Epoch 41: train loss: 0.25338226556777954\n",
      "Epoch 41: train loss: 0.29376012086868286\n",
      "Epoch 41: train loss: 0.40120190382003784\n",
      "Epoch 41: train loss: 0.3096560835838318\n",
      "Epoch 41: train loss: 0.35378026962280273\n",
      "Epoch 41: train loss: 0.3802507519721985\n",
      "Epoch 41: train loss: 0.3415892422199249\n",
      "Epoch 41: train loss: 0.40027502179145813\n",
      "Epoch 41: train loss: 0.19092239439487457\n",
      "Epoch 41: train loss: 0.5474690794944763\n",
      "Epoch 41: train loss: 0.24947619438171387\n",
      "Epoch 41: train loss: 0.1379876434803009\n",
      "Epoch 41: train loss: 0.4211488962173462\n",
      "Epoch 41: train loss: 0.21289518475532532\n",
      "Epoch 41: train loss: 0.3207261562347412\n",
      "Epoch 41: train loss: 0.24859236180782318\n",
      "Epoch 41: train loss: 0.13231942057609558\n",
      "Epoch 42: train loss: 0.4169510304927826\n",
      "Epoch 42: train loss: 0.3021649420261383\n",
      "Epoch 42: train loss: 0.14427897334098816\n",
      "Epoch 42: train loss: 0.27345362305641174\n",
      "Epoch 42: train loss: 0.41543054580688477\n",
      "Epoch 42: train loss: 0.5563838481903076\n",
      "Epoch 42: train loss: 0.3396410048007965\n",
      "Epoch 42: train loss: 0.3089204430580139\n",
      "Epoch 42: train loss: 0.61183100938797\n",
      "Epoch 42: train loss: 0.32458001375198364\n",
      "Epoch 42: train loss: 0.3208911418914795\n",
      "Epoch 42: train loss: 0.42833980917930603\n",
      "Epoch 42: train loss: 0.3185640871524811\n",
      "Epoch 42: train loss: 0.31185227632522583\n",
      "Epoch 42: train loss: 0.3380073606967926\n",
      "Epoch 42: train loss: 0.34578174352645874\n",
      "Epoch 42: train loss: 0.2592892348766327\n",
      "Epoch 42: train loss: 0.24233002960681915\n",
      "Epoch 42: train loss: 0.3514668047428131\n",
      "Epoch 42: train loss: 0.5078893899917603\n",
      "Epoch 42: train loss: 0.39194682240486145\n",
      "Epoch 42: train loss: 0.3541787564754486\n",
      "Epoch 42: train loss: 0.16857963800430298\n",
      "Epoch 42: train loss: 0.46659693121910095\n",
      "Epoch 42: train loss: 0.4140818417072296\n",
      "Epoch 42: train loss: 0.2798026502132416\n",
      "Epoch 42: train loss: 0.2869655191898346\n",
      "Epoch 42: train loss: 0.25120067596435547\n",
      "Epoch 42: train loss: 0.43162405490875244\n",
      "Epoch 42: train loss: 0.2933584451675415\n",
      "Epoch 42: train loss: 0.4033958315849304\n",
      "Epoch 42: train loss: 0.4236966669559479\n",
      "Epoch 42: train loss: 0.5593575835227966\n",
      "Epoch 42: train loss: 0.25855204463005066\n",
      "Epoch 42: train loss: 0.2895427942276001\n",
      "Epoch 42: train loss: 0.3285515308380127\n",
      "Epoch 42: train loss: 0.3575111925601959\n",
      "Epoch 42: train loss: 0.2808164060115814\n",
      "Epoch 42: train loss: 0.4404652416706085\n",
      "Epoch 42: train loss: 0.30507317185401917\n",
      "Epoch 42: train loss: 0.3059973418712616\n",
      "Epoch 42: train loss: 0.3574666976928711\n",
      "Epoch 42: train loss: 0.4397752285003662\n",
      "Epoch 42: train loss: 0.18457679450511932\n",
      "Epoch 42: train loss: 0.2943898141384125\n",
      "Epoch 42: train loss: 0.4364602863788605\n",
      "Epoch 42: train loss: 0.4223199486732483\n",
      "Epoch 42: train loss: 0.3307267129421234\n",
      "Epoch 42: train loss: 0.36803051829338074\n",
      "Epoch 42: train loss: 0.3804143965244293\n",
      "Epoch 42: train loss: 0.3231213390827179\n",
      "Epoch 42: train loss: 0.30657586455345154\n",
      "Epoch 42: train loss: 0.30805590748786926\n",
      "Epoch 42: train loss: 0.44384515285491943\n",
      "Epoch 42: train loss: 0.38217684626579285\n",
      "Epoch 42: train loss: 0.2838963568210602\n",
      "Epoch 42: train loss: 0.29756972193717957\n",
      "Epoch 42: train loss: 0.2460271418094635\n",
      "Epoch 42: train loss: 0.3248085379600525\n",
      "Epoch 42: train loss: 0.17894652485847473\n",
      "Epoch 42: train loss: 0.3292723596096039\n",
      "Epoch 42: train loss: 0.43561628460884094\n",
      "Epoch 42: train loss: 0.4149416387081146\n",
      "Epoch 42: train loss: 0.4079788327217102\n",
      "Epoch 42: train loss: 0.29423126578330994\n",
      "Epoch 42: train loss: 0.4424517750740051\n",
      "Epoch 42: train loss: 0.3023398220539093\n",
      "Epoch 42: train loss: 0.4912761151790619\n",
      "Epoch 42: train loss: 0.4048100411891937\n",
      "Epoch 42: train loss: 0.4533613622188568\n",
      "Epoch 42: train loss: 0.3137434720993042\n",
      "Epoch 42: train loss: 0.5014023184776306\n",
      "Epoch 42: train loss: 0.32289692759513855\n",
      "Epoch 42: train loss: 0.38278162479400635\n",
      "Epoch 42: train loss: 0.465547114610672\n",
      "Epoch 42: train loss: 0.4366476237773895\n",
      "Epoch 42: train loss: 0.19879886507987976\n",
      "Epoch 42: train loss: 0.818668782711029\n",
      "Epoch 43: train loss: 0.29791128635406494\n",
      "Epoch 43: train loss: 0.32717904448509216\n",
      "Epoch 43: train loss: 0.39919254183769226\n",
      "Epoch 43: train loss: 0.39917320013046265\n",
      "Epoch 43: train loss: 0.28217005729675293\n",
      "Epoch 43: train loss: 0.5116708874702454\n",
      "Epoch 43: train loss: 0.38076522946357727\n",
      "Epoch 43: train loss: 0.3134405314922333\n",
      "Epoch 43: train loss: 0.367204487323761\n",
      "Epoch 43: train loss: 0.3050011694431305\n",
      "Epoch 43: train loss: 0.28781700134277344\n",
      "Epoch 43: train loss: 0.5596768260002136\n",
      "Epoch 43: train loss: 0.3178909122943878\n",
      "Epoch 43: train loss: 0.47705575823783875\n",
      "Epoch 43: train loss: 0.3495323657989502\n",
      "Epoch 43: train loss: 0.20200498402118683\n",
      "Epoch 43: train loss: 0.3784327208995819\n",
      "Epoch 43: train loss: 0.3046300411224365\n",
      "Epoch 43: train loss: 0.5467051863670349\n",
      "Epoch 43: train loss: 0.40667611360549927\n",
      "Epoch 43: train loss: 0.26565104722976685\n",
      "Epoch 43: train loss: 0.3645986020565033\n",
      "Epoch 43: train loss: 0.3743380308151245\n",
      "Epoch 43: train loss: 0.4924449622631073\n",
      "Epoch 43: train loss: 0.42602452635765076\n",
      "Epoch 43: train loss: 0.29588526487350464\n",
      "Epoch 43: train loss: 0.47917017340660095\n",
      "Epoch 43: train loss: 0.43227341771125793\n",
      "Epoch 43: train loss: 0.37178951501846313\n",
      "Epoch 43: train loss: 0.27302977442741394\n",
      "Epoch 43: train loss: 0.2785520851612091\n",
      "Epoch 43: train loss: 0.3252794146537781\n",
      "Epoch 43: train loss: 0.41100773215293884\n",
      "Epoch 43: train loss: 0.21424546837806702\n",
      "Epoch 43: train loss: 0.37410518527030945\n",
      "Epoch 43: train loss: 0.34774652123451233\n",
      "Epoch 43: train loss: 0.4188240170478821\n",
      "Epoch 43: train loss: 0.33483004570007324\n",
      "Epoch 43: train loss: 0.35875192284584045\n",
      "Epoch 43: train loss: 0.3334029018878937\n",
      "Epoch 43: train loss: 0.3211168646812439\n",
      "Epoch 43: train loss: 0.2037549465894699\n",
      "Epoch 43: train loss: 0.5369940996170044\n",
      "Epoch 43: train loss: 0.30428647994995117\n",
      "Epoch 43: train loss: 0.28789883852005005\n",
      "Epoch 43: train loss: 0.3302442133426666\n",
      "Epoch 43: train loss: 0.5547913908958435\n",
      "Epoch 43: train loss: 0.4298425316810608\n",
      "Epoch 43: train loss: 0.35811302065849304\n",
      "Epoch 43: train loss: 0.423881858587265\n",
      "Epoch 43: train loss: 0.20225943624973297\n",
      "Epoch 43: train loss: 0.4718039333820343\n",
      "Epoch 43: train loss: 0.20486605167388916\n",
      "Epoch 43: train loss: 0.3865233361721039\n",
      "Epoch 43: train loss: 0.3683355450630188\n",
      "Epoch 43: train loss: 0.3580380082130432\n",
      "Epoch 43: train loss: 0.3093498945236206\n",
      "Epoch 43: train loss: 0.3247204124927521\n",
      "Epoch 43: train loss: 0.4417133927345276\n",
      "Epoch 43: train loss: 0.2565873861312866\n",
      "Epoch 43: train loss: 0.2610989212989807\n",
      "Epoch 43: train loss: 0.27682021260261536\n",
      "Epoch 43: train loss: 0.37098056077957153\n",
      "Epoch 43: train loss: 0.29126667976379395\n",
      "Epoch 43: train loss: 0.5881093740463257\n",
      "Epoch 43: train loss: 0.3610021471977234\n",
      "Epoch 43: train loss: 0.28282061219215393\n",
      "Epoch 43: train loss: 0.17331765592098236\n",
      "Epoch 43: train loss: 0.4184238016605377\n",
      "Epoch 43: train loss: 0.39383164048194885\n",
      "Epoch 43: train loss: 0.301400750875473\n",
      "Epoch 43: train loss: 0.3352571725845337\n",
      "Epoch 43: train loss: 0.37806156277656555\n",
      "Epoch 43: train loss: 0.2444821149110794\n",
      "Epoch 43: train loss: 0.35108599066734314\n",
      "Epoch 43: train loss: 0.21365734934806824\n",
      "Epoch 43: train loss: 0.2822132408618927\n",
      "Epoch 43: train loss: 0.8983541131019592\n",
      "Epoch 44: train loss: 0.26735544204711914\n",
      "Epoch 44: train loss: 0.44119003415107727\n",
      "Epoch 44: train loss: 0.29826536774635315\n",
      "Epoch 44: train loss: 0.24701502919197083\n",
      "Epoch 44: train loss: 0.40823596715927124\n",
      "Epoch 44: train loss: 0.3380048871040344\n",
      "Epoch 44: train loss: 0.2125954031944275\n",
      "Epoch 44: train loss: 0.4749147891998291\n",
      "Epoch 44: train loss: 0.2393311858177185\n",
      "Epoch 44: train loss: 0.3496169149875641\n",
      "Epoch 44: train loss: 0.4339127540588379\n",
      "Epoch 44: train loss: 0.43535080552101135\n",
      "Epoch 44: train loss: 0.27145853638648987\n",
      "Epoch 44: train loss: 0.3845204710960388\n",
      "Epoch 44: train loss: 0.17411737143993378\n",
      "Epoch 44: train loss: 0.32181933522224426\n",
      "Epoch 44: train loss: 0.3840799033641815\n",
      "Epoch 44: train loss: 0.26229774951934814\n",
      "Epoch 44: train loss: 0.3010721504688263\n",
      "Epoch 44: train loss: 0.31959930062294006\n",
      "Epoch 44: train loss: 0.3637191951274872\n",
      "Epoch 44: train loss: 0.5503814816474915\n",
      "Epoch 44: train loss: 0.3742857277393341\n",
      "Epoch 44: train loss: 0.38180962204933167\n",
      "Epoch 44: train loss: 0.5234511494636536\n",
      "Epoch 44: train loss: 0.24709036946296692\n",
      "Epoch 44: train loss: 0.3087393045425415\n",
      "Epoch 44: train loss: 0.4986134469509125\n",
      "Epoch 44: train loss: 0.3849940001964569\n",
      "Epoch 44: train loss: 0.41303589940071106\n",
      "Epoch 44: train loss: 0.3794035315513611\n",
      "Epoch 44: train loss: 0.38105276226997375\n",
      "Epoch 44: train loss: 0.286488801240921\n",
      "Epoch 44: train loss: 0.31055155396461487\n",
      "Epoch 44: train loss: 0.35035669803619385\n",
      "Epoch 44: train loss: 0.41789090633392334\n",
      "Epoch 44: train loss: 0.24216820299625397\n",
      "Epoch 44: train loss: 0.2282828539609909\n",
      "Epoch 44: train loss: 0.3919863700866699\n",
      "Epoch 44: train loss: 0.2546086609363556\n",
      "Epoch 44: train loss: 0.4424777925014496\n",
      "Epoch 44: train loss: 0.22299140691757202\n",
      "Epoch 44: train loss: 0.21064400672912598\n",
      "Epoch 44: train loss: 0.42799633741378784\n",
      "Epoch 44: train loss: 0.2850813567638397\n",
      "Epoch 44: train loss: 0.43525010347366333\n",
      "Epoch 44: train loss: 0.30732935667037964\n",
      "Epoch 44: train loss: 0.22211825847625732\n",
      "Epoch 44: train loss: 0.2178913652896881\n",
      "Epoch 44: train loss: 0.32463064789772034\n",
      "Epoch 44: train loss: 0.14831262826919556\n",
      "Epoch 44: train loss: 0.4589880108833313\n",
      "Epoch 44: train loss: 0.3908672332763672\n",
      "Epoch 44: train loss: 0.5498791933059692\n",
      "Epoch 44: train loss: 0.3176927864551544\n",
      "Epoch 44: train loss: 0.3180183470249176\n",
      "Epoch 44: train loss: 0.500895082950592\n",
      "Epoch 44: train loss: 0.24515753984451294\n",
      "Epoch 44: train loss: 0.3020513653755188\n",
      "Epoch 44: train loss: 0.5532310009002686\n",
      "Epoch 44: train loss: 0.39686867594718933\n",
      "Epoch 44: train loss: 0.4549335837364197\n",
      "Epoch 44: train loss: 0.36537495255470276\n",
      "Epoch 44: train loss: 0.48381394147872925\n",
      "Epoch 44: train loss: 0.3741948902606964\n",
      "Epoch 44: train loss: 0.6140908002853394\n",
      "Epoch 44: train loss: 0.35275402665138245\n",
      "Epoch 44: train loss: 0.39614254236221313\n",
      "Epoch 44: train loss: 0.38105466961860657\n",
      "Epoch 44: train loss: 0.28395533561706543\n",
      "Epoch 44: train loss: 0.2935979664325714\n",
      "Epoch 44: train loss: 0.30242881178855896\n",
      "Epoch 44: train loss: 0.27125129103660583\n",
      "Epoch 44: train loss: 0.29824593663215637\n",
      "Epoch 44: train loss: 0.472688227891922\n",
      "Epoch 44: train loss: 0.3132025897502899\n",
      "Epoch 44: train loss: 0.309401273727417\n",
      "Epoch 44: train loss: 0.31777000427246094\n",
      "Epoch 45: train loss: 0.520737886428833\n",
      "Epoch 45: train loss: 0.29060855507850647\n",
      "Epoch 45: train loss: 0.38791704177856445\n",
      "Epoch 45: train loss: 0.29902130365371704\n",
      "Epoch 45: train loss: 0.5774548649787903\n",
      "Epoch 45: train loss: 0.26240861415863037\n",
      "Epoch 45: train loss: 0.3854637145996094\n",
      "Epoch 45: train loss: 0.42382150888442993\n",
      "Epoch 45: train loss: 0.23104773461818695\n",
      "Epoch 45: train loss: 0.2674540579319\n",
      "Epoch 45: train loss: 0.4371696412563324\n",
      "Epoch 45: train loss: 0.2964459955692291\n",
      "Epoch 45: train loss: 0.22186189889907837\n",
      "Epoch 45: train loss: 0.22130483388900757\n",
      "Epoch 45: train loss: 0.31249430775642395\n",
      "Epoch 45: train loss: 0.2807560861110687\n",
      "Epoch 45: train loss: 0.3415323793888092\n",
      "Epoch 45: train loss: 0.5346633195877075\n",
      "Epoch 45: train loss: 0.3693253695964813\n",
      "Epoch 45: train loss: 0.2603079378604889\n",
      "Epoch 45: train loss: 0.28663623332977295\n",
      "Epoch 45: train loss: 0.2994380593299866\n",
      "Epoch 45: train loss: 0.35990920662879944\n",
      "Epoch 45: train loss: 0.34247514605522156\n",
      "Epoch 45: train loss: 0.47563424706459045\n",
      "Epoch 45: train loss: 0.2287125289440155\n",
      "Epoch 45: train loss: 0.19042037427425385\n",
      "Epoch 45: train loss: 0.3349756896495819\n",
      "Epoch 45: train loss: 0.23928454518318176\n",
      "Epoch 45: train loss: 0.38168179988861084\n",
      "Epoch 45: train loss: 0.361735463142395\n",
      "Epoch 45: train loss: 0.3132557272911072\n",
      "Epoch 45: train loss: 0.17117232084274292\n",
      "Epoch 45: train loss: 0.2742535173892975\n",
      "Epoch 45: train loss: 0.6113297343254089\n",
      "Epoch 45: train loss: 0.5497511029243469\n",
      "Epoch 45: train loss: 0.29993531107902527\n",
      "Epoch 45: train loss: 0.6195767521858215\n",
      "Epoch 45: train loss: 0.25631245970726013\n",
      "Epoch 45: train loss: 0.29513075947761536\n",
      "Epoch 45: train loss: 0.27040958404541016\n",
      "Epoch 45: train loss: 0.3853781223297119\n",
      "Epoch 45: train loss: 0.2600668668746948\n",
      "Epoch 45: train loss: 0.24565522372722626\n",
      "Epoch 45: train loss: 0.42244651913642883\n",
      "Epoch 45: train loss: 0.2994009554386139\n",
      "Epoch 45: train loss: 0.4629315137863159\n",
      "Epoch 45: train loss: 0.40000754594802856\n",
      "Epoch 45: train loss: 0.3300224840641022\n",
      "Epoch 45: train loss: 0.3996771574020386\n",
      "Epoch 45: train loss: 0.33029720187187195\n",
      "Epoch 45: train loss: 0.46115729212760925\n",
      "Epoch 45: train loss: 0.3322608172893524\n",
      "Epoch 45: train loss: 0.5153106451034546\n",
      "Epoch 45: train loss: 0.3476642072200775\n",
      "Epoch 45: train loss: 0.3398092985153198\n",
      "Epoch 45: train loss: 0.3337208926677704\n",
      "Epoch 45: train loss: 0.38370466232299805\n",
      "Epoch 45: train loss: 0.23012970387935638\n",
      "Epoch 45: train loss: 0.28643789887428284\n",
      "Epoch 45: train loss: 0.4392828047275543\n",
      "Epoch 45: train loss: 0.31804463267326355\n",
      "Epoch 45: train loss: 0.37326258420944214\n",
      "Epoch 45: train loss: 0.400285929441452\n",
      "Epoch 45: train loss: 0.44401678442955017\n",
      "Epoch 45: train loss: 0.29349076747894287\n",
      "Epoch 45: train loss: 0.38227787613868713\n",
      "Epoch 45: train loss: 0.30740195512771606\n",
      "Epoch 45: train loss: 0.25843745470046997\n",
      "Epoch 45: train loss: 0.21236447989940643\n",
      "Epoch 45: train loss: 0.4391670823097229\n",
      "Epoch 45: train loss: 0.4835929274559021\n",
      "Epoch 45: train loss: 0.4588361382484436\n",
      "Epoch 45: train loss: 0.3533368706703186\n",
      "Epoch 45: train loss: 0.36531609296798706\n",
      "Epoch 45: train loss: 0.402720183134079\n",
      "Epoch 45: train loss: 0.3248237669467926\n",
      "Epoch 45: train loss: 1.0046098232269287\n",
      "Epoch 46: train loss: 0.3477974534034729\n",
      "Epoch 46: train loss: 0.32838359475135803\n",
      "Epoch 46: train loss: 0.4452216327190399\n",
      "Epoch 46: train loss: 0.3226030468940735\n",
      "Epoch 46: train loss: 0.3257206678390503\n",
      "Epoch 46: train loss: 0.3564169704914093\n",
      "Epoch 46: train loss: 0.518899142742157\n",
      "Epoch 46: train loss: 0.4175337255001068\n",
      "Epoch 46: train loss: 0.5237857699394226\n",
      "Epoch 46: train loss: 0.500102162361145\n",
      "Epoch 46: train loss: 0.39498966932296753\n",
      "Epoch 46: train loss: 0.4072147011756897\n",
      "Epoch 46: train loss: 0.449547678232193\n",
      "Epoch 46: train loss: 0.3540472388267517\n",
      "Epoch 46: train loss: 0.29937708377838135\n",
      "Epoch 46: train loss: 0.3680032193660736\n",
      "Epoch 46: train loss: 0.26589471101760864\n",
      "Epoch 46: train loss: 0.2329239547252655\n",
      "Epoch 46: train loss: 0.4011218249797821\n",
      "Epoch 46: train loss: 0.33485931158065796\n",
      "Epoch 46: train loss: 0.39543554186820984\n",
      "Epoch 46: train loss: 0.2747175991535187\n",
      "Epoch 46: train loss: 0.36354878544807434\n",
      "Epoch 46: train loss: 0.580431342124939\n",
      "Epoch 46: train loss: 0.2958700954914093\n",
      "Epoch 46: train loss: 0.6273692846298218\n",
      "Epoch 46: train loss: 0.47619399428367615\n",
      "Epoch 46: train loss: 0.2505186200141907\n",
      "Epoch 46: train loss: 0.23063793778419495\n",
      "Epoch 46: train loss: 0.4165269136428833\n",
      "Epoch 46: train loss: 0.3904617130756378\n",
      "Epoch 46: train loss: 0.40377289056777954\n",
      "Epoch 46: train loss: 0.40427082777023315\n",
      "Epoch 46: train loss: 0.3772236704826355\n",
      "Epoch 46: train loss: 0.3601301610469818\n",
      "Epoch 46: train loss: 0.2806265950202942\n",
      "Epoch 46: train loss: 0.30266979336738586\n",
      "Epoch 46: train loss: 0.48026636242866516\n",
      "Epoch 46: train loss: 0.4379432499408722\n",
      "Epoch 46: train loss: 0.23967450857162476\n",
      "Epoch 46: train loss: 0.4657159447669983\n",
      "Epoch 46: train loss: 0.453570157289505\n",
      "Epoch 46: train loss: 0.28124767541885376\n",
      "Epoch 46: train loss: 0.35701408982276917\n",
      "Epoch 46: train loss: 0.2844886779785156\n",
      "Epoch 46: train loss: 0.2992284297943115\n",
      "Epoch 46: train loss: 0.36904191970825195\n",
      "Epoch 46: train loss: 0.2537095248699188\n",
      "Epoch 46: train loss: 0.3215971887111664\n",
      "Epoch 46: train loss: 0.38711124658584595\n",
      "Epoch 46: train loss: 0.29566794633865356\n",
      "Epoch 46: train loss: 0.31676870584487915\n",
      "Epoch 46: train loss: 0.5418692231178284\n",
      "Epoch 46: train loss: 0.44158536195755005\n",
      "Epoch 46: train loss: 0.4197060763835907\n",
      "Epoch 46: train loss: 0.24683929979801178\n",
      "Epoch 46: train loss: 0.3124067485332489\n",
      "Epoch 46: train loss: 0.2816792130470276\n",
      "Epoch 46: train loss: 0.26087433099746704\n",
      "Epoch 46: train loss: 0.2964145541191101\n",
      "Epoch 46: train loss: 0.2928451597690582\n",
      "Epoch 46: train loss: 0.30972540378570557\n",
      "Epoch 46: train loss: 0.3956732451915741\n",
      "Epoch 46: train loss: 0.3763711452484131\n",
      "Epoch 46: train loss: 0.2072366327047348\n",
      "Epoch 46: train loss: 0.4819582998752594\n",
      "Epoch 46: train loss: 0.39347633719444275\n",
      "Epoch 46: train loss: 0.30472058057785034\n",
      "Epoch 46: train loss: 0.3978615403175354\n",
      "Epoch 46: train loss: 0.2890136241912842\n",
      "Epoch 46: train loss: 0.3297932744026184\n",
      "Epoch 46: train loss: 0.20486347377300262\n",
      "Epoch 46: train loss: 0.3960321545600891\n",
      "Epoch 46: train loss: 0.2692652940750122\n",
      "Epoch 46: train loss: 0.4557262659072876\n",
      "Epoch 46: train loss: 0.3568466603755951\n",
      "Epoch 46: train loss: 0.25051385164260864\n",
      "Epoch 46: train loss: 0.13277266919612885\n",
      "Epoch 47: train loss: 0.23447975516319275\n",
      "Epoch 47: train loss: 0.39086437225341797\n",
      "Epoch 47: train loss: 0.5452049374580383\n",
      "Epoch 47: train loss: 0.38403475284576416\n",
      "Epoch 47: train loss: 0.5418758392333984\n",
      "Epoch 47: train loss: 0.535425066947937\n",
      "Epoch 47: train loss: 0.32218819856643677\n",
      "Epoch 47: train loss: 0.3276163935661316\n",
      "Epoch 47: train loss: 0.2590010464191437\n",
      "Epoch 47: train loss: 0.3937680423259735\n",
      "Epoch 47: train loss: 0.2954704463481903\n",
      "Epoch 47: train loss: 0.2966611385345459\n",
      "Epoch 47: train loss: 0.40244510769844055\n",
      "Epoch 47: train loss: 0.29415053129196167\n",
      "Epoch 47: train loss: 0.4059299826622009\n",
      "Epoch 47: train loss: 0.4085099697113037\n",
      "Epoch 47: train loss: 0.2761308550834656\n",
      "Epoch 47: train loss: 0.3721381723880768\n",
      "Epoch 47: train loss: 0.3665556013584137\n",
      "Epoch 47: train loss: 0.31810879707336426\n",
      "Epoch 47: train loss: 0.4460386037826538\n",
      "Epoch 47: train loss: 0.23978470265865326\n",
      "Epoch 47: train loss: 0.5339423418045044\n",
      "Epoch 47: train loss: 0.3972298800945282\n",
      "Epoch 47: train loss: 0.49553030729293823\n",
      "Epoch 47: train loss: 0.3704330325126648\n",
      "Epoch 47: train loss: 0.36958542466163635\n",
      "Epoch 47: train loss: 0.43248865008354187\n",
      "Epoch 47: train loss: 0.27826768159866333\n",
      "Epoch 47: train loss: 0.47555485367774963\n",
      "Epoch 47: train loss: 0.21377567946910858\n",
      "Epoch 47: train loss: 0.24447226524353027\n",
      "Epoch 47: train loss: 0.3062435984611511\n",
      "Epoch 47: train loss: 0.3321393132209778\n",
      "Epoch 47: train loss: 0.28005552291870117\n",
      "Epoch 47: train loss: 0.4002244174480438\n",
      "Epoch 47: train loss: 0.34277084469795227\n",
      "Epoch 47: train loss: 0.42168405652046204\n",
      "Epoch 47: train loss: 0.33482739329338074\n",
      "Epoch 47: train loss: 0.2804606556892395\n",
      "Epoch 47: train loss: 0.26705607771873474\n",
      "Epoch 47: train loss: 0.2759101986885071\n",
      "Epoch 47: train loss: 0.32820653915405273\n",
      "Epoch 47: train loss: 0.4114987850189209\n",
      "Epoch 47: train loss: 0.396617591381073\n",
      "Epoch 47: train loss: 0.22995182871818542\n",
      "Epoch 47: train loss: 0.5160350799560547\n",
      "Epoch 47: train loss: 0.3295653164386749\n",
      "Epoch 47: train loss: 0.3002261221408844\n",
      "Epoch 47: train loss: 0.5537378191947937\n",
      "Epoch 47: train loss: 0.46945786476135254\n",
      "Epoch 47: train loss: 0.5731446146965027\n",
      "Epoch 47: train loss: 0.30638185143470764\n",
      "Epoch 47: train loss: 0.3048856556415558\n",
      "Epoch 47: train loss: 0.2313976138830185\n",
      "Epoch 47: train loss: 0.36465978622436523\n",
      "Epoch 47: train loss: 0.36179283261299133\n",
      "Epoch 47: train loss: 0.3420426845550537\n",
      "Epoch 47: train loss: 0.4013540744781494\n",
      "Epoch 47: train loss: 0.38363468647003174\n",
      "Epoch 47: train loss: 0.3552543520927429\n",
      "Epoch 47: train loss: 0.299893319606781\n",
      "Epoch 47: train loss: 0.29299986362457275\n",
      "Epoch 47: train loss: 0.3517937958240509\n",
      "Epoch 47: train loss: 0.302889883518219\n",
      "Epoch 47: train loss: 0.3732650876045227\n",
      "Epoch 47: train loss: 0.4274247884750366\n",
      "Epoch 47: train loss: 0.34121301770210266\n",
      "Epoch 47: train loss: 0.2972186505794525\n",
      "Epoch 47: train loss: 0.5008302927017212\n",
      "Epoch 47: train loss: 0.19913530349731445\n",
      "Epoch 47: train loss: 0.2121136337518692\n",
      "Epoch 47: train loss: 0.3263666331768036\n",
      "Epoch 47: train loss: 0.3177518844604492\n",
      "Epoch 47: train loss: 0.2810370922088623\n",
      "Epoch 47: train loss: 0.23000529408454895\n",
      "Epoch 47: train loss: 0.35360899567604065\n",
      "Epoch 47: train loss: 0.643904983997345\n",
      "Epoch 48: train loss: 0.4597991704940796\n",
      "Epoch 48: train loss: 0.4001961350440979\n",
      "Epoch 48: train loss: 0.4068065881729126\n",
      "Epoch 48: train loss: 0.3912056088447571\n",
      "Epoch 48: train loss: 0.32514718174934387\n",
      "Epoch 48: train loss: 0.4670182168483734\n",
      "Epoch 48: train loss: 0.21889173984527588\n",
      "Epoch 48: train loss: 0.2943841814994812\n",
      "Epoch 48: train loss: 0.3092775046825409\n",
      "Epoch 48: train loss: 0.6031364798545837\n",
      "Epoch 48: train loss: 0.39036890864372253\n",
      "Epoch 48: train loss: 0.3282593786716461\n",
      "Epoch 48: train loss: 0.39557668566703796\n",
      "Epoch 48: train loss: 0.3583942949771881\n",
      "Epoch 48: train loss: 0.38502562046051025\n",
      "Epoch 48: train loss: 0.2994624972343445\n",
      "Epoch 48: train loss: 0.3559100925922394\n",
      "Epoch 48: train loss: 0.22011703252792358\n",
      "Epoch 48: train loss: 0.2790278196334839\n",
      "Epoch 48: train loss: 0.3743364214897156\n",
      "Epoch 48: train loss: 0.25606200098991394\n",
      "Epoch 48: train loss: 0.22450853884220123\n",
      "Epoch 48: train loss: 0.3976121246814728\n",
      "Epoch 48: train loss: 0.3386146128177643\n",
      "Epoch 48: train loss: 0.3504714369773865\n",
      "Epoch 48: train loss: 0.41657206416130066\n",
      "Epoch 48: train loss: 0.23771211504936218\n",
      "Epoch 48: train loss: 0.3626664876937866\n",
      "Epoch 48: train loss: 0.1992075890302658\n",
      "Epoch 48: train loss: 0.21452592313289642\n",
      "Epoch 48: train loss: 0.47521448135375977\n",
      "Epoch 48: train loss: 0.5034950971603394\n",
      "Epoch 48: train loss: 0.28568845987319946\n",
      "Epoch 48: train loss: 0.4265378415584564\n",
      "Epoch 48: train loss: 0.31056416034698486\n",
      "Epoch 48: train loss: 0.4292096495628357\n",
      "Epoch 48: train loss: 0.3461136519908905\n",
      "Epoch 48: train loss: 0.24136893451213837\n",
      "Epoch 48: train loss: 0.30498528480529785\n",
      "Epoch 48: train loss: 0.39861974120140076\n",
      "Epoch 48: train loss: 0.568389356136322\n",
      "Epoch 48: train loss: 0.3777627944946289\n",
      "Epoch 48: train loss: 0.3379318118095398\n",
      "Epoch 48: train loss: 0.2763802111148834\n",
      "Epoch 48: train loss: 0.4197841286659241\n",
      "Epoch 48: train loss: 0.4392341375350952\n",
      "Epoch 48: train loss: 0.44242656230926514\n",
      "Epoch 48: train loss: 0.35832223296165466\n",
      "Epoch 48: train loss: 0.40637481212615967\n",
      "Epoch 48: train loss: 0.21469862759113312\n",
      "Epoch 48: train loss: 0.25714051723480225\n",
      "Epoch 48: train loss: 0.31774604320526123\n",
      "Epoch 48: train loss: 0.28098008036613464\n",
      "Epoch 48: train loss: 0.2981918454170227\n",
      "Epoch 48: train loss: 0.1087297573685646\n",
      "Epoch 48: train loss: 0.4111863076686859\n",
      "Epoch 48: train loss: 0.3172050714492798\n",
      "Epoch 48: train loss: 0.41651567816734314\n",
      "Epoch 48: train loss: 0.489554762840271\n",
      "Epoch 48: train loss: 0.3906557559967041\n",
      "Epoch 48: train loss: 0.2877817451953888\n",
      "Epoch 48: train loss: 0.5206202864646912\n",
      "Epoch 48: train loss: 0.22054770588874817\n",
      "Epoch 48: train loss: 0.3278683125972748\n",
      "Epoch 48: train loss: 0.26767832040786743\n",
      "Epoch 48: train loss: 0.31824448704719543\n",
      "Epoch 48: train loss: 0.36933469772338867\n",
      "Epoch 48: train loss: 0.3365468978881836\n",
      "Epoch 48: train loss: 0.48297280073165894\n",
      "Epoch 48: train loss: 0.41725555062294006\n",
      "Epoch 48: train loss: 0.2895635962486267\n",
      "Epoch 48: train loss: 0.3777219355106354\n",
      "Epoch 48: train loss: 0.3421172499656677\n",
      "Epoch 48: train loss: 0.3349398374557495\n",
      "Epoch 48: train loss: 0.41786712408065796\n",
      "Epoch 48: train loss: 0.5067425966262817\n",
      "Epoch 48: train loss: 0.15109659731388092\n",
      "Epoch 48: train loss: 0.49964872002601624\n",
      "Epoch 49: train loss: 0.3161885142326355\n",
      "Epoch 49: train loss: 0.3149890899658203\n",
      "Epoch 49: train loss: 0.40454521775245667\n",
      "Epoch 49: train loss: 0.32316961884498596\n",
      "Epoch 49: train loss: 0.29156723618507385\n",
      "Epoch 49: train loss: 0.4241155683994293\n",
      "Epoch 49: train loss: 0.4074464738368988\n",
      "Epoch 49: train loss: 0.2048625499010086\n",
      "Epoch 49: train loss: 0.31791606545448303\n",
      "Epoch 49: train loss: 0.2756713330745697\n",
      "Epoch 49: train loss: 0.3278033137321472\n",
      "Epoch 49: train loss: 0.332145094871521\n",
      "Epoch 49: train loss: 0.24116170406341553\n",
      "Epoch 49: train loss: 0.6543972492218018\n",
      "Epoch 49: train loss: 0.27779561281204224\n",
      "Epoch 49: train loss: 0.25832054018974304\n",
      "Epoch 49: train loss: 0.3820105493068695\n",
      "Epoch 49: train loss: 0.3919161558151245\n",
      "Epoch 49: train loss: 0.14959847927093506\n",
      "Epoch 49: train loss: 0.34020912647247314\n",
      "Epoch 49: train loss: 0.34518271684646606\n",
      "Epoch 49: train loss: 0.44801875948905945\n",
      "Epoch 49: train loss: 0.3779154419898987\n",
      "Epoch 49: train loss: 0.34205162525177\n",
      "Epoch 49: train loss: 0.3210330307483673\n",
      "Epoch 49: train loss: 0.4027632772922516\n",
      "Epoch 49: train loss: 0.33196306228637695\n",
      "Epoch 49: train loss: 0.5434963703155518\n",
      "Epoch 49: train loss: 0.3439828157424927\n",
      "Epoch 49: train loss: 0.3546905219554901\n",
      "Epoch 49: train loss: 0.4267977476119995\n",
      "Epoch 49: train loss: 0.2506406307220459\n",
      "Epoch 49: train loss: 0.34666717052459717\n",
      "Epoch 49: train loss: 0.4238660931587219\n",
      "Epoch 49: train loss: 0.3532179594039917\n",
      "Epoch 49: train loss: 0.33446335792541504\n",
      "Epoch 49: train loss: 0.31522294878959656\n",
      "Epoch 49: train loss: 0.3842591941356659\n",
      "Epoch 49: train loss: 0.30365505814552307\n",
      "Epoch 49: train loss: 0.45291951298713684\n",
      "Epoch 49: train loss: 0.38168519735336304\n",
      "Epoch 49: train loss: 0.3332829177379608\n",
      "Epoch 49: train loss: 0.20763219892978668\n",
      "Epoch 49: train loss: 0.27578312158584595\n",
      "Epoch 49: train loss: 0.21725480258464813\n",
      "Epoch 49: train loss: 0.3318527936935425\n",
      "Epoch 49: train loss: 0.27373015880584717\n",
      "Epoch 49: train loss: 0.46434587240219116\n",
      "Epoch 49: train loss: 0.3570905923843384\n",
      "Epoch 49: train loss: 0.28060126304626465\n",
      "Epoch 49: train loss: 0.46109554171562195\n",
      "Epoch 49: train loss: 0.31790220737457275\n",
      "Epoch 49: train loss: 0.3927432894706726\n",
      "Epoch 49: train loss: 0.5050244331359863\n",
      "Epoch 49: train loss: 0.3047346770763397\n",
      "Epoch 49: train loss: 0.34208229184150696\n",
      "Epoch 49: train loss: 0.3592939078807831\n",
      "Epoch 49: train loss: 0.3168356418609619\n",
      "Epoch 49: train loss: 0.18017539381980896\n",
      "Epoch 49: train loss: 0.2880637049674988\n",
      "Epoch 49: train loss: 0.3064824044704437\n",
      "Epoch 49: train loss: 0.49107468128204346\n",
      "Epoch 49: train loss: 0.45327913761138916\n",
      "Epoch 49: train loss: 0.48177048563957214\n",
      "Epoch 49: train loss: 0.30857527256011963\n",
      "Epoch 49: train loss: 0.34623944759368896\n",
      "Epoch 49: train loss: 0.4338838458061218\n",
      "Epoch 49: train loss: 0.3102845549583435\n",
      "Epoch 49: train loss: 0.2871723473072052\n",
      "Epoch 49: train loss: 0.478542685508728\n",
      "Epoch 49: train loss: 0.24225054681301117\n",
      "Epoch 49: train loss: 0.3777018189430237\n",
      "Epoch 49: train loss: 0.48428115248680115\n",
      "Epoch 49: train loss: 0.3165692985057831\n",
      "Epoch 49: train loss: 0.43661025166511536\n",
      "Epoch 49: train loss: 0.45273280143737793\n",
      "Epoch 49: train loss: 0.3100375235080719\n",
      "Epoch 49: train loss: 0.4203653037548065\n",
      "Evaluations on training data\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.648     0.489     0.558       470\n",
      "good credit (1)      0.886     0.937     0.911      1998\n",
      "\n",
      "       accuracy                          0.852      2468\n",
      "      macro avg      0.767     0.713     0.734      2468\n",
      "   weighted avg      0.841     0.852     0.844      2468\n",
      "\n",
      "Evaluations on testing data\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      " bad credit (0)      0.621     0.504     0.557       117\n",
      "good credit (1)      0.889     0.928     0.908       500\n",
      "\n",
      "       accuracy                          0.848       617\n",
      "      macro avg      0.755     0.716     0.732       617\n",
      "   weighted avg      0.838     0.848     0.841       617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get a clf#\n",
    "torch.manual_seed(101)\n",
    "np.random.seed(1)\n",
    "random_state = check_random_state(10)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "d = dataset\n",
    "torch_model = proplace.train_clf(d.X1_train, d.y1_train, d.X1_test, d.y1_test, 20, epochs=50, data_name=\"compas\", save_clf=False,\n",
    "                        load_clf=False)\n",
    "model = proplace.InnModel(dataset, torch_model, 20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [01:39<00:00,  9.93s/it]\n",
      "100%|| 10/10 [00:45<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "m2s = proplace.retrain_models(dataset, epochs=50) + proplace.retrain_models_leave_some_out(dataset, epochs=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare for exps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from exputils import *\n",
    "clf = model\n",
    "test_xs_vals, test_xs, test_xs_carla, utildataset, nodes = get_test_data(model, m2s, dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Baselines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2635it [01:10, 37.36it/s]\n",
      "50it [00:41,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "1.0 >>> validity\n",
      "1.0 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 21.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.006712798616364821 >>> robustness - delta validity, avg bound\n",
      "0.03892371392318255 >>> l1 cost\n",
      "0.86 >>> percentage of inliers dataset class 1 points\n",
      "1.2410574092128752 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ours\n",
    "rnnce_ces, treer, X_class1_clf_robust = run_rnnce(d, clf, nodes, utildataset, 0.025, test_xs_vals)\n",
    "proplace_ces = run_proplace(clf, nodes, utildataset, 0.025, treer, X_class1_clf_robust, test_xs_vals)\n",
    "proplace_scores = eval_ces(clf, m2s, proplace_ces, d, utildataset, test_xs_vals, 0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Timeout - No Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "[INFO] Counterfactual Explanation Found [wachter.py wachter_recourse]\n",
      "============================\n",
      "0.58 >>> validity\n",
      "0.5659999999999998 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 32.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 -0.3819811003183235 >>> robustness - delta validity, avg bound\n",
      "0.16963035890591982 >>> l1 cost\n",
      "0.4482758620689655 >>> percentage of inliers dataset class 1 points\n",
      "1.8088584491928488 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wach_ces = run_wachter(clf, test_xs_carla)\n",
    "wach_scores = eval_ces(clf, m2s, wach_ces, d, utildataset, test_xs_vals,  delta=0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [06:40,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "1.0 >>> validity\n",
      "0.9969999999999999 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 21.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62 0.03066817466969085 >>> robustness - delta validity, avg bound\n",
      "0.04319718658904799 >>> l1 cost\n",
      "0.78 >>> percentage of inliers dataset class 1 points\n",
      "1.3390591933671456 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rbr_ces = run_rbr(clf, d, random_state, test_xs_vals)\n",
    "rbr_scores = eval_ces(clf, m2s, rbr_ces, d, utildataset, test_xs_vals, 0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2635it [00:04, 597.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1822, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:01, 47.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "1.0 >>> validity\n",
      "1.0 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 19.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 0.29518415654362185 >>> robustness - delta validity, avg bound\n",
      "0.05007915558126085 >>> l1 cost\n",
      "0.94 >>> percentage of inliers dataset class 1 points\n",
      "1.1115520645680954 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "robx_ces = run_robx(clf, d, test_xs_vals)\n",
    "robx_scores = eval_ces(clf, m2s, robx_ces, d, utildataset, test_xs_vals, 0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start generating LIME coefficients [model.py get_counterfactuals]\n",
      "[INFO] Finished generating LIME coefficients [model.py get_counterfactuals]\n",
      "============================\n",
      "0.98 >>> validity\n",
      "0.9800000000000002 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 21.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.2017563019735396 >>> robustness - delta validity, avg bound\n",
      "0.2186056390734865 >>> l1 cost\n",
      "0.0 >>> percentage of inliers dataset class 1 points\n",
      "2.8384434240260816 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "roar_params = {\n",
    "        \"lr\": 0.01,\n",
    "        \"lambda_\": 0.01,\n",
    "        \"delta_max\": 0.004,\n",
    "        \"norm\": 1,\n",
    "        \"t_max_min\": 3,\n",
    "        \"loss_type\": \"MSE\",\n",
    "        \"y_target\": [1],\n",
    "        \"binary_cat_features\": False,\n",
    "        \"loss_threshold\": 1e-3,\n",
    "        \"discretize\": False,\n",
    "        \"sample\": False,\n",
    "        \"lime_seed\": 0,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    "roar_ces = run_roar(clf, test_xs_carla, roar_params)\n",
    "roar_scores = eval_ces(clf, m2s, roar_ces, d, utildataset, test_xs_vals, delta=0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:03, 63.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [02:06, 62.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [03:08, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [04:12, 63.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [05:17, 63.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [06:23, 64.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [06:54, 53.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [07:58, 56.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [09:00, 58.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [10:04, 60.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [11:08, 61.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [12:13, 62.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [13:18, 63.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [14:23, 63.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [15:28, 64.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [16:34, 64.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [17:40, 65.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [18:46, 65.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [19:51, 65.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [20:29, 56.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [21:36, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [22:44, 62.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [23:52, 63.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [24:59, 65.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [26:07, 65.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [27:15, 66.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [28:24, 67.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [29:33, 67.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [30:07, 57.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [31:16, 61.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [32:27, 63.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [33:38, 66.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [34:49, 67.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [36:00, 68.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [37:12, 69.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [38:23, 70.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [39:36, 70.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [40:49, 71.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [42:03, 72.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [43:17, 72.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [44:27, 71.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [45:02, 60.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [46:13, 63.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [47:24, 65.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [48:35, 67.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [49:51, 70.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [51:07, 71.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [52:24, 73.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [53:39, 73.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n",
      "[WARNING] No encoder specified. Using k-d trees to represent class prototypes. [cfproto.py fit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [55:07, 66.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "1.0 >>> validity\n",
      "1.0 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.2865079344205055 >>> robustness - delta validity, avg bound\n",
      "0.08393428906575501 >>> l1 cost\n",
      "0.76 >>> percentage of inliers dataset class 1 points\n",
      "1.3607431983140341 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "protor_ces = run_proto_r(clf, d, utildataset, test_xs_vals, nodes, delta_target=0.025)\n",
    "protor_scores = eval_ces(clf, m2s, protor_ces, d, utildataset, test_xs_vals, 0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [02:58,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "1.0 >>> validity\n",
      "0.998 >>> robustness - M2 validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:02, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.29246658294369426 >>> robustness - delta validity, avg bound\n",
      "0.040236280590115484 >>> l1 cost\n",
      "0.68 >>> percentage of inliers dataset class 1 points\n",
      "1.708005003165761 >>> avg lof dataset class 1 points\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "milpr_ces = run_milpr(clf, d, nodes, utildataset, 0.03, test_xs_vals)\n",
    "milpr_scores = eval_ces(clf, m2s, milpr_ces, d, utildataset, test_xs_vals, 0.025)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+---------------+-------+-------------------+---------------+\n",
      "| name     |   validity |   delta validity |   M2 validity |    l1 |   %inlier class 1 |   lof class 1 |\n",
      "+==========+============+==================+===============+=======+===================+===============+\n",
      "| wach     |       0.58 |             0    |         0.566 | 0.17  |             0.448 |         1.809 |\n",
      "| rbr      |       1    |             0.62 |         0.997 | 0.043 |             0.78  |         1.339 |\n",
      "| robx     |       1    |             0.82 |         1     | 0.05  |             0.94  |         1.112 |\n",
      "| roar     |       0.98 |             1    |         0.98  | 0.219 |             0     |         2.838 |\n",
      "| proto-r  |       1    |             1    |         1     | 0.084 |             0.76  |         1.361 |\n",
      "| milp-r   |       1    |             1    |         0.998 | 0.04  |             0.68  |         1.708 |\n",
      "| proplace |       1    |             1    |         1     | 0.039 |             0.86  |         1.241 |\n",
      "+----------+------------+------------------+---------------+-------+-------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "# make table\n",
    "from tabulate import tabulate\n",
    "scores_names = [\"name\", \"validity\", \"delta validity\", \"M2 validity\", \"l1\", \"%inlier class 1\", \"lof class 1\"]\n",
    "scores_table = [scores_names,\n",
    "                np.concatenate((['wach'], np.round(wach_scores, 3))),\n",
    "                np.concatenate((['rbr'], np.round(rbr_scores, 3))),\n",
    "                np.concatenate((['robx'], np.round(robx_scores, 3))),\n",
    "                np.concatenate((['roar'], np.round(roar_scores, 3))),\n",
    "                np.concatenate((['proto-r'], np.round(protor_scores, 3))),\n",
    "                np.concatenate((['milp-r'], np.round(milpr_scores, 3))),\n",
    "                np.concatenate((['proplace'], np.round(proplace_scores, 3))), ]\n",
    "print(tabulate(scores_table, headers='firstrow', tablefmt='outline'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "-----+----------"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
